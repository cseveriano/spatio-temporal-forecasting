{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyFTS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d645661fd805>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyFTS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbenchmarks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMeasures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyFTS'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "from pyFTS.benchmarks import Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    mindf = df.min()\n",
    "    maxdf = df.max()\n",
    "    return (df-mindf)/(maxdf-mindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(norm, _min, _max):\n",
    "    return [(n * (_max-_min)) + _min for n in norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('results/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('results/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set target and input variables \n",
    "target_station = 'DHHL_3'\n",
    "\n",
    "#All neighbor stations with residual correlation greater than .90\n",
    "neighbor_stations_90 = ['DHHL_3',  'DHHL_4','DHHL_5','DHHL_10','DHHL_11','DHHL_9','DHHL_2', 'DHHL_6','DHHL_7','DHHL_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"df_oahu.pkl\")\n",
    "df_ssa_clean = pd.read_pickle(\"df_ssa_clean.pkl\")\n",
    "df_ssa_residual = pd.read_pickle(\"df_ssa_residual.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove columns with many corrupted or missing values\n",
    "df.drop(columns=['AP_1', 'AP_7'], inplace=True)\n",
    "df_ssa_clean.drop(columns=['AP_1', 'AP_7'], inplace=True)\n",
    "df_ssa_residual.drop(columns=['AP_1', 'AP_7'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data form the interval of interest\n",
    "interval = ((df.index >= '2010-06') & (df.index < '2010-08'))\n",
    "df = df.loc[interval]\n",
    "df_ssa_clean = df_ssa_clean.loc[interval]\n",
    "df_ssa_residual = df_ssa_residual.loc[interval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize Data\n",
    "\n",
    "# Save Min-Max for Denorm\n",
    "min_raw = df[target_station].min()\n",
    "min_clean = df_ssa_clean[target_station].min()\n",
    "min_residual = df_ssa_residual[target_station].min()\n",
    "\n",
    "max_raw = df[target_station].max()\n",
    "max_clean = df_ssa_clean[target_station].max()\n",
    "max_residual = df_ssa_residual[target_station].max()\n",
    "\n",
    "# Perform Normalization\n",
    "norm_df_ssa_clean = normalize(df_ssa_clean)\n",
    "norm_df_ssa_residual = normalize(df_ssa_residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Nested cross-validation Indexes\n",
    "Month Forward-chaining\n",
    "\n",
    "C. Bergmeir and J. M. Benítez. On the use of cross-validation for time series predictor evaluation. Inf. Sci., 191:192–213, May 2012. ISSN 0020–0255. doi: 10.1016/j.ins.2011.12.028. URL http://dx.doi.org/10.1016/j.ins.2011.12.028.\n",
    "\n",
    "L. J. Tashman. Out-of-sample tests of forecasting accuracy: an analysis and review. International Journal of Forecasting, 16(4):437–450, 2000. URLhttps://ideas.repec.org/a/eee/intfor/v16y2000i4p437-450.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling window\n",
    "\n",
    "Training: 4 weeks\n",
    "Validation: 1 week\n",
    "Test: 1 week\n",
    "\n",
    "Roliing daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRollingWindow(index):\n",
    "    pivot = index\n",
    "    train_start = pivot.strftime('%Y-%m-%d')\n",
    "    pivot = pivot + datetime.timedelta(days=27)\n",
    "    train_end = pivot.strftime('%Y-%m-%d')\n",
    "\n",
    "    pivot = pivot + datetime.timedelta(days=1)\n",
    "    validation_start = pivot.strftime('%Y-%m-%d')\n",
    "    pivot = pivot + datetime.timedelta(days=6)\n",
    "    validation_end = pivot.strftime('%Y-%m-%d')\n",
    "\n",
    "    pivot = pivot + datetime.timedelta(days=1)\n",
    "    test_start = pivot.strftime('%Y-%m-%d')\n",
    "    pivot = pivot + datetime.timedelta(days=6)\n",
    "    test_end = pivot.strftime('%Y-%m-%d')\n",
    "    \n",
    "    return train_start, train_end, validation_start, validation_end, test_start, test_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_error(cv_name, df, forecasts, order_list):\n",
    "    cv_results = pd.DataFrame(columns=['Split', 'RMSE', 'SMAPE', 'U'])\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    for i in np.arange(len(forecasts)):\n",
    "#        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        test = df[test_start : test_end]\n",
    "    \n",
    "        yhat = forecasts[i]\n",
    "        order = order_list[i]\n",
    "        \n",
    "        rmse = Measures.rmse(test[target_station].iloc[order:], yhat[:-1])\n",
    "#        print(\"RMSE: \",rmse)\n",
    "        \n",
    "        smape = Measures.smape(test[target_station].iloc[order:], yhat[:-1])\n",
    "#        print(\"SMAPE: \",smape)\n",
    "        \n",
    "        u = Measures.UStatistic(test[target_station].iloc[order:], yhat[:-1])\n",
    "#        print(\"U Statistic: \",u)\n",
    "       \n",
    "        res = {'Split' : index.strftime('%Y-%m-%d') ,'RMSE' : rmse, 'SMAPE' : smape, 'U' : u}\n",
    "        cv_results = cv_results.append(res, ignore_index=True)\n",
    "        cv_results.to_csv(cv_name+\".csv\")        \n",
    "\n",
    "        index = index + datetime.timedelta(days=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_ssa_series(clean, residual):\n",
    "    return [r + c for r, c in zip(residual,clean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual):\n",
    "    \n",
    "    forecasts_final = []\n",
    "    order_list = []\n",
    "    \n",
    "    for i in np.arange(len(forecasts_clean)):\n",
    "        f_clean = denormalize(forecasts_clean[i], min_clean, max_clean)\n",
    "        f_residual = denormalize(forecasts_residual[i], min_residual, max_residual)\n",
    "\n",
    "        o_clean = order_list_clean[i]\n",
    "        o_residual = order_list_residual[i]\n",
    "\n",
    "        max_order = max(o_clean, o_residual)\n",
    "\n",
    "        f_final = reconstruct_ssa_series(f_clean[max_order-o_clean:], f_residual[max_order-o_residual:])\n",
    "        \n",
    "        forecasts_final.append(f_final)\n",
    "        order_list.append(max_order)\n",
    "        \n",
    "    return forecasts_final, order_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(df.index[0]  + datetime.timedelta(weeks=19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[test_start : test_end].DHHL_3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_params = (2, 1, 2)\n",
    "sarima_params = (1, 1, 1, 61)\n",
    "fcst_clean = sarima_forecast(df_ssa_clean[train_start : train_end].DHHL_3, df_ssa_clean[test_start : test_end].DHHL_3, arima_params, sarima_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_ssa_clean[test_start : test_end].DHHL_3.values)\n",
    "plt.plot(fcst_clean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistence_forecast(train, test, step):\n",
    "    predictions = []\n",
    "    \n",
    "    for t in np.arange(0,len(test), step):\n",
    "        yhat = [test.iloc[t]]  * step\n",
    "        predictions.extend(yhat)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_persistence(df, step):\n",
    "\n",
    "    forecasts = []\n",
    "    lags_list = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "    \n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "        yhat = persistence_forecast(train[target_station], test[target_station], step)        \n",
    "        \n",
    "        lags_list.append(1)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_persistence(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_persistence(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_persistence\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from itertools import product\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_SARIMA_models(test_name, train, validation, parameters_list, period_length):\n",
    "\n",
    "    sarima_results = pd.DataFrame(columns=['Order','RMSE'])\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "\n",
    "    for param in parameters_list:\n",
    "        arima_order = (param[0],param[1],param[2])\n",
    "        sarima_order = (param[3],param[4],param[5],period_length)\n",
    "        print('Testing SARIMA%s %s ' % (str(arima_order),str(sarima_order)))\n",
    "        try:\n",
    "            fcst = sarima_forecast(train, validation, arima_order, sarima_order)\n",
    "            rmse = Measures.rmse(validation.values, fcst)\n",
    "            \n",
    "            if rmse < best_score:\n",
    "                best_score, best_cfg = rmse, (arima_order, sarima_order)\n",
    "\n",
    "            res = {'Parameters' : str(param) ,'RMSE' : rmse}\n",
    "            print('SARIMA%s %s RMSE=%.3f' % (str(arima_order),str(sarima_order),rmse))\n",
    "            sarima_results = sarima_results.append(res, ignore_index=True)\n",
    "            sarima_results.to_csv(test_name+\".csv\")\n",
    "        except:\n",
    "            print(sys.exc_info())\n",
    "            print('Invalid model%s %s ' % (str(arima_order),str(sarima_order)))\n",
    "            continue\n",
    "    \n",
    "    print('Best SARIMA(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLD_sarima_forecast(train, test, arima_order, sarima_order):\n",
    "    \n",
    "    whole_data = train.append(test)\n",
    "    test_data = test\n",
    "    \n",
    "    training_mod = SARIMAX(train.values, order=arima_order, seasonal_order=sarima_order, disp=True, trend='c')\n",
    "    training_res = training_mod.fit()\n",
    "    \n",
    "    mod = SARIMAX(whole_data.values, order=arima_order, seasonal_order=sarima_order, trend='c')\n",
    "    res = mod.filter(training_res.params)\n",
    "    \n",
    "    insample = res.predict()\n",
    "    wlen = len(whole_data)\n",
    "    tlen = len(test_data)\n",
    "\n",
    "    predictions = insample[wlen-tlen:]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarima_forecast(train, test, arima_order, sarima_order):\n",
    "\n",
    "    predictions = []\n",
    "    window_size = sarima_order[3] * 5\n",
    "    step = 5\n",
    "    \n",
    "    history = list(train.iloc[-window_size:])\n",
    "\n",
    "    print(\"Fitting model at:\", datetime.datetime.now())\n",
    "    model = SARIMAX(history, order=arima_order, seasonal_order=sarima_order,enforce_invertibility=False,enforce_stationarity=False)\n",
    "    model_fit = model.fit(disp=True,enforce_invertibility=False,enforce_stationarity=False, maxiter=100)\n",
    "\n",
    "    #save the state parameter\n",
    "    est_params = model_fit.params\n",
    "    est_state = model_fit.predicted_state[:, -1]\n",
    "    est_state_cov = model_fit.predicted_state_cov[:, :, -1]\n",
    "\n",
    "    st = 0\n",
    "        \n",
    "    print(\"Forecasting at:\", datetime.datetime.now())\n",
    "    for t in np.arange(1,len(test)+1,step):\n",
    "        obs = test.iloc[st:t].values\n",
    "        history.extend(obs)\n",
    "        history = history[-window_size:]\n",
    "        \n",
    "        mod_updated = SARIMAX(history, order=arima_order, seasonal_order=sarima_order,enforce_invertibility=False,enforce_stationarity=False)\n",
    "        mod_updated.initialize_known(est_state, est_state_cov)\n",
    "        mod_frcst = mod_updated.smooth(est_params)\n",
    "        \n",
    "        yhat = mod_frcst.forecast(step)   \n",
    "        predictions.extend(yhat)\n",
    "            \n",
    "        est_params = mod_frcst.params\n",
    "        est_state = mod_frcst.predicted_state[:, -1]\n",
    "        est_state_cov = mod_frcst.predicted_state_cov[:, :, -1]\n",
    "            \n",
    "        st = t\n",
    "    print(\"Forecasting complete at:\", datetime.datetime.now())\n",
    "                \n",
    "    return predictions[:len(test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_SARIMA(df, step):\n",
    "\n",
    "#    p_values = [0,1,2]\n",
    "#    d_values = [0,1]\n",
    "#    q_values = [0,1,2]\n",
    "#    P_values = [0,1]\n",
    "#    D_Values = [0,1]\n",
    "#    Q_Values = [0,1]\n",
    "\n",
    "    p_values = [2]\n",
    "    d_values = [1]\n",
    "    q_values = [2]\n",
    "    P_values = [1]\n",
    "    D_Values = [1]\n",
    "    Q_Values = [1]\n",
    "\n",
    "    parameters = product(p_values, d_values, q_values, P_values, D_Values, Q_Values)\n",
    "    parameters_list = list(parameters)\n",
    "    period_length = 61 #de 5:00 as 20:00\n",
    "\n",
    "    \n",
    "    forecasts = []\n",
    "    lags_list = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "    \n",
    "        # Perform grid search\n",
    "        #(arima_params, sarima_params) = evaluate_SARIMA_models(\"nested_test_sarima_oahu\", train[target_station], validation[target_station], parameters_list, period_length)\n",
    "        arima_params = (2, 1, 2)\n",
    "        sarima_params = (1, 1, 1, 61)\n",
    "        \n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "        yhat = sarima_forecast(train[target_station], test[target_station], arima_params, sarima_params)        \n",
    "        \n",
    "        lags_list.append(1)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_SARIMA(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_SARIMA(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_sarima\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Autoregressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR, DynamicVAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_VAR_models(test_name, train, validation,target, maxlags_list):\n",
    "    var_results = pd.DataFrame(columns=['Order','RMSE'])\n",
    "    best_score, best_cfg, best_model = float(\"inf\"), None, None\n",
    "    \n",
    "    for lgs in maxlags_list:\n",
    "        model = VAR(train)\n",
    "        results = model.fit(maxlags=lgs, ic='aic')\n",
    "        \n",
    "        order = results.k_ar\n",
    "        forecast = []\n",
    "\n",
    "        for i in range(len(validation)-order) :\n",
    "            forecast.extend(results.forecast(validation.values[i:i+order],1))\n",
    "\n",
    "        forecast_df = pd.DataFrame(columns=validation.columns, data=forecast)\n",
    "        rmse = Measures.rmse(validation[target].iloc[order:], forecast_df[target].values)\n",
    "\n",
    "        if rmse < best_score:\n",
    "            best_score, best_cfg, best_model = rmse, order, results\n",
    "\n",
    "        res = {'Order' : str(order) ,'RMSE' : rmse}\n",
    "        print('VAR (%s)  RMSE=%.3f' % (str(order),rmse))\n",
    "        var_results = var_results.append(res, ignore_index=True)\n",
    "        var_results.to_csv(test_name+\".csv\")\n",
    "        \n",
    "    print('Best VAR(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_forecast(train, test, target, order, step):\n",
    "    model = VAR(train.values)\n",
    "    results = model.fit(maxlags=order)\n",
    "    lag_order = results.k_ar\n",
    "    print(\"Lag order:\" + str(lag_order))\n",
    "    forecast = []\n",
    "\n",
    "    for i in np.arange(0,len(test)-lag_order+1,step) :\n",
    "        forecast.extend(results.forecast(test.values[i:i+lag_order],step))\n",
    "\n",
    "    forecast_df = pd.DataFrame(columns=test.columns, data=forecast)\n",
    "    return forecast_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_var(df, step):\n",
    "    maxlags_list = [1,2,4,6,8,10,20,40]\n",
    "    forecasts = []\n",
    "    order_list = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "    \n",
    "        # Perform grid search\n",
    "        best_model = evaluate_VAR_models(\"nested_test_var_oahu\", train[neighbor_stations_90], validation[neighbor_stations_90],target_station, maxlags_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "        order = best_model.k_ar\n",
    "        yhat = var_forecast(train[neighbor_stations_90], test[neighbor_stations_90], target_station, order, step)\n",
    "        \n",
    "        order_list.append(order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_var(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_var(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_var\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(df.index[0])\n",
    "y_pred_oahu = forecasts_final[0]\n",
    "y_obs_oahu = df[test_start : test_end].DHHL_3.values\n",
    "\n",
    "#x_date = pd.date_range(\"00:00\", \"24:00\", freq=\"10min\").strftime('%H:%M')\n",
    "\n",
    "#xn = np.arange(len(x_date))\n",
    "xn = np.arange(0,len(y_pred_oahu)-1)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(xn, y_obs_oahu[order_list[0]:], label='Observed')\n",
    "plt.plot(xn, y_pred_oahu[:-1], color ='orange', label='Predicted')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Irradiance [W/m2]')\n",
    "ax.legend(loc='best')\n",
    "ticks = [10,70,130,190,250,310,370]\n",
    "ax.set_xticks(ticks)\n",
    "\n",
    "ax.set_xticklabels(['06-Jul','07-Jul','08-Jul','09-Jul','10-Jul','11-Jul','12-Jul'])\n",
    "plt.show()\n",
    "fig.savefig(\"plot_oahu_ssa\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Order FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyFTS.partitioners import Grid, Entropy, Util as pUtil\n",
    "from pyFTS.models import hofts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hofts_models(test_name, train, validation, partitioners_list, order_list, partitions_list):\n",
    "    \n",
    "    hofts_results = pd.DataFrame(columns=['Partitioner','Partitions','Order','RMSE'])\n",
    "    best_score, best_cfg, best_model = float(\"inf\"), None, None\n",
    "\n",
    "    for _partitioner in partitioners_list:\n",
    "        for _order in order_list:\n",
    "            for npartitions in partitions_list:\n",
    "                print('HOFTS %s - %s - %s' % (str(_partitioner), npartitions, str(_order)))\n",
    "                fuzzy_sets = _partitioner(data=train.values, npart=npartitions)\n",
    "                model_simple_hofts = hofts.HighOrderFTS(order=_order)\n",
    "\n",
    "                model_simple_hofts.fit(train.values, order=_order, partitioner=fuzzy_sets)\n",
    "                \n",
    "                forecast = model_simple_hofts.predict(validation.values)\n",
    "                rmse = Measures.rmse(validation.iloc[_order:], forecast[:-1])\n",
    "\n",
    "                if rmse < best_score:\n",
    "                    best_score, best_cfg = rmse, (_order,npartitions,_partitioner)\n",
    "                    best_model = model_simple_hofts\n",
    "\n",
    "                res = {'Partitioner':str(_partitioner), 'Partitions':npartitions, 'Order' : str(_order) ,'RMSE' : rmse}\n",
    "                print('HOFTS %s - %s - %s  RMSE=%.3f' % (str(_partitioner), npartitions, str(_order),rmse))\n",
    "                hofts_results = hofts_results.append(res, ignore_index=True)\n",
    "                hofts_results.to_csv(test_name+\".csv\")\n",
    "\n",
    "    print('Best HOFTS(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    \n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hofts_forecast(train_df, test_df, _order, _partitioner, _npartitions):\n",
    "    \n",
    "    fuzzy_sets = _partitioner(data=train_df.values, npart=_npartitions)\n",
    "    model_simple_hofts = hofts.HighOrderFTS()\n",
    "    \n",
    "\n",
    "    model_simple_hofts.fit(train_df.values, order=_order, partitioner=fuzzy_sets)\n",
    "\n",
    "    \n",
    "    forecast = model_simple_hofts.predict(test_df.values)\n",
    "\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_hofts(df, step):\n",
    "    \n",
    "    partitioners_list = [Grid.GridPartitioner, Entropy.EntropyPartitioner]\n",
    "    eval_order_list = np.arange(1,3)\n",
    "    partitions_list = np.arange(10,100,10)\n",
    "    \n",
    "    forecasts = []\n",
    "    order_list = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=1)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (order,nparts,partitioner) = evaluate_hofts_models(\"nested_eval_hofts_oahu\", train[target_station], validation[target_station], partitioners_list, eval_order_list, partitions_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = hofts_forecast(train[target_station], test[target_station], order, partitioner, nparts)\n",
    "        \n",
    "        order_list.append(order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_hofts(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_hofts(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_hofts\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Variance FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyFTS.models.nonstationary import cvfts\n",
    "from pyFTS.models.nonstationary import partitioners as nspartitioners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cvfts_models(test_name, train, validation, partitions_list):\n",
    "    \n",
    "    cvfts_results = pd.DataFrame(columns=['Partitions','RMSE'])\n",
    "    best_score, best_cfg, best_model = float(\"inf\"), None, None\n",
    "\n",
    "    for npartitions in partitions_list:\n",
    "                \n",
    "        fuzzy_sets =  nspartitioners.PolynomialNonStationaryPartitioner(data=train.values, part=Grid.GridPartitioner(data=train.values, npart=npartitions), degree=2)\n",
    "                \n",
    "        model_cvfts = cvfts.ConditionalVarianceFTS()\n",
    "        model_cvfts.fit(train.values, parameters=1, partitioner=fuzzy_sets, num_batches=1000)\n",
    "                                \n",
    "        forecast = model_cvfts.predict(validation.values)\n",
    "        rmse = Measures.rmse(validation.iloc[1:], forecast[:-1])\n",
    "\n",
    "        if rmse < best_score:\n",
    "            best_score, best_cfg = rmse, npartitions\n",
    "            best_model = model_cvfts\n",
    "\n",
    "        res = {'Partitions':npartitions, 'RMSE' : rmse}\n",
    "        print('CVFTS %s -  RMSE=%.3f' % (npartitions, rmse))\n",
    "        cvfts_results = cvfts_results.append(res, ignore_index=True)\n",
    "        cvfts_results.to_csv(test_name+\".csv\")\n",
    "\n",
    "    print('Best CVFTS(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    \n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvfts_forecast(train, test, _partitions):\n",
    "    \n",
    "    fuzzy_sets =  nspartitioners.PolynomialNonStationaryPartitioner(data=train.values, part=Grid.GridPartitioner(data=train.values, npart=_partitions), degree=2)\n",
    "                    \n",
    "    model_cvfts = cvfts.ConditionalVarianceFTS()\n",
    "    model_cvfts.fit(train.values, parameters=1, partitioner=fuzzy_sets, num_batches=1000)\n",
    "\n",
    "    forecast = model_cvfts.predict(test.values)\n",
    "\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_cvfts(df, step):\n",
    "    \n",
    "    partitions_list = np.arange(80,100,10)\n",
    "    \n",
    "    forecasts = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        nparts = evaluate_cvfts_models(\"nested_eval_cvfts_oahu\", train[target_station], validation[target_station], partitions_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = cvfts_forecast(train[target_station], test[target_station],nparts)\n",
    "        \n",
    "        order_list.append(1)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_cvfts(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_cvfts(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_cvfts\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered Multivariate FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U git+https://github.com/cseveriano/spatio-temporal-forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.sthofts' from 'C:\\\\Users\\\\cseve\\\\Anaconda3\\\\lib\\\\site-packages\\\\models\\\\sthofts.py'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(sthofts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import KMeansPartitioner\n",
    "from models import sthofts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmvfts_forecast(train_df, test_df, target, _order, npartitions):\n",
    "    \n",
    "    print(\"KMean Partition at:\", datetime.datetime.now())\n",
    "\n",
    "    _partitioner = KMeansPartitioner.KMeansPartitioner(data=train_df.values, npart=npartitions, batch_size=1000, init_size=npartitions*3)\n",
    "\n",
    "    model_sthofts = sthofts.SpatioTemporalHighOrderFTS()\n",
    "    \n",
    "    print(\"CMVFTS fit at:\", datetime.datetime.now())\n",
    "    model_sthofts.fit(train_df.values, order=_order, partitioner=_partitioner)\n",
    "    \n",
    "    print(\"CMVFTS prediction at:\", datetime.datetime.now())\n",
    "    forecast = model_sthofts.predict(test_df.values)\n",
    "    forecast_df = pd.DataFrame(data=forecast, columns=test_df.columns)\n",
    "    return forecast_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cmvfts_models(test_name, train, validation, order_list, partitions_list):\n",
    "    \n",
    "    cmvfts_results = pd.DataFrame(columns=['Partitions','Order','RMSE'])\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "\n",
    "    for _order in order_list:\n",
    "        for npartitions in partitions_list:\n",
    "            \n",
    "            forecast = cmvfts_forecast(train, validation, target_station, _order, npartitions)\n",
    "            rmse = Measures.rmse(validation[target_station].iloc[_order:], forecast[:-1])\n",
    "\n",
    "            if rmse < best_score:\n",
    "                best_score, best_cfg = rmse, (_order,npartitions)\n",
    "\n",
    "            res = {'Partitions':npartitions, 'Order' : str(_order) ,'RMSE' : rmse}\n",
    "            print('CMVFTS %s - %s  RMSE=%.3f' % (npartitions, str(_order),rmse))\n",
    "            cmvfts_results = cmvfts_results.append(res, ignore_index=True)\n",
    "            cmvfts_results.to_csv(test_name+\".csv\")\n",
    "\n",
    "    print('Best CMVFTS(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    \n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_cmvfts(df, step):\n",
    "    \n",
    "    #eval_order_list = np.arange(1,3)\n",
    "    eval_order_list = [2]\n",
    "#    partitions_list = np.arange(80,110,10)\n",
    "#    partitions_list = [20,30]\n",
    "    partitions_list = [80]\n",
    "    \n",
    "    forecasts = []\n",
    "    order_list = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (order,nparts) = evaluate_cmvfts_models(\"nested_eval_cmvfts_oahu\", train[neighbor_stations_90], validation[neighbor_stations_90], eval_order_list, partitions_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = cmvfts_forecast(train[neighbor_stations_90], test[neighbor_stations_90],target_station, order, nparts)\n",
    "        \n",
    "        order_list.append(order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  2010-06-01\n",
      "KMean Partition at: 2018-08-17 10:27:38.086219\n",
      "CMVFTS fit at: 2018-08-17 10:27:38.335553\n",
      "CMVFTS prediction at: 2018-08-17 10:34:38.660364\n",
      "New forecast with membership\n",
      "CMVFTS 80 - 2  RMSE=0.018\n",
      "Best CMVFTS((2, 80)) RMSE=0.018\n",
      "KMean Partition at: 2018-08-17 10:36:22.035763\n",
      "CMVFTS fit at: 2018-08-17 10:36:22.158439\n",
      "CMVFTS prediction at: 2018-08-17 10:46:23.265118\n",
      "New forecast with membership\n",
      "Index:  2010-06-08\n",
      "KMean Partition at: 2018-08-17 10:47:31.254263\n",
      "CMVFTS fit at: 2018-08-17 10:47:31.361915\n",
      "CMVFTS prediction at: 2018-08-17 10:54:26.039490\n",
      "New forecast with membership\n",
      "CMVFTS 80 - 2  RMSE=0.017\n",
      "Best CMVFTS((2, 80)) RMSE=0.017\n",
      "KMean Partition at: 2018-08-17 10:55:34.475596\n",
      "CMVFTS fit at: 2018-08-17 10:55:34.626190\n",
      "CMVFTS prediction at: 2018-08-17 11:03:54.567306\n",
      "New forecast with membership\n",
      "Index:  2010-06-15\n",
      "KMean Partition at: 2018-08-17 11:05:01.862509\n",
      "CMVFTS fit at: 2018-08-17 11:05:02.001321\n"
     ]
    }
   ],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_cmvfts(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_cmvfts(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)\n",
    "calculate_rolling_error(\"rolling_cv_oahu_ssa_cmvfts-TEST-nomembership\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(df.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 1.79391160e-01, 2.12541840e+00,\n",
       "       6.94642440e+00, 2.02229386e+01, 2.48435741e+01, 5.18624013e+01,\n",
       "       1.24123383e+02, 1.17993704e+02, 1.41461509e+02, 2.60756724e+02,\n",
       "       4.20528198e+02, 4.68190023e+02, 4.82245572e+02, 5.97745613e+02,\n",
       "       6.52623933e+02, 6.66029313e+02, 5.55528980e+02, 7.41387598e+02,\n",
       "       4.46944939e+02, 6.11650233e+02, 4.28221159e+02, 3.77150701e+02,\n",
       "       6.42719262e+02, 5.68548517e+02, 4.95615063e+02, 5.92335313e+02,\n",
       "       2.46854434e+02, 3.17784821e+02, 7.10079530e+02, 4.11159989e+02,\n",
       "       9.15244491e+02, 6.23574118e+02, 5.96034033e+02, 3.99511503e+02,\n",
       "       9.26209347e+02, 9.05101853e+02, 8.82787867e+02, 8.55001248e+02,\n",
       "       1.00219136e+03, 9.21208182e+02, 8.14409070e+02, 7.47384484e+02,\n",
       "       6.77385716e+02, 6.15338756e+02, 5.55774688e+02, 5.16017944e+02,\n",
       "       3.67251260e+02, 3.74723090e+02, 3.29060077e+02, 1.73392541e+02,\n",
       "       9.06996940e+01, 3.23340144e+01, 8.07244078e+01, 4.73759467e+01,\n",
       "       1.30585637e+01, 2.48425982e+00, 1.96592594e-01, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.84822807e-01,\n",
       "       2.15531709e+00, 1.04781323e+01, 1.75404557e+01, 2.40064183e+01,\n",
       "       7.36793946e+01, 1.26990336e+02, 2.31167373e+02, 2.24310890e+02,\n",
       "       3.73430920e+02, 4.28859472e+02, 4.87178737e+02, 5.32255651e+02,\n",
       "       6.04885572e+02, 6.58526903e+02, 6.53956454e+02, 6.43455836e+02,\n",
       "       2.71536019e+02, 8.38352080e+02, 7.60630242e+02, 8.70177108e+02,\n",
       "       3.49615139e+02, 3.93445952e+02, 8.04912058e+02, 4.81859843e+02,\n",
       "       5.84545963e+02, 8.23053682e+02, 7.29687143e+02, 4.41028961e+02,\n",
       "       5.40449694e+02, 3.19925912e+02, 2.92799817e+02, 2.62889614e+02,\n",
       "       4.44566320e+02, 2.25007448e+02, 3.48751634e+02, 2.11904299e+02,\n",
       "       2.53410480e+02, 2.25418061e+02, 2.18773097e+02, 2.75101491e+02,\n",
       "       2.39955341e+02, 6.19352863e+02, 5.62233121e+02, 5.93316029e+02,\n",
       "       5.61321623e+02, 2.60988336e+02, 1.96998624e+02, 1.35130903e+02,\n",
       "       9.57527684e+01, 7.95072860e+01, 7.59896052e+01, 2.94365280e+01,\n",
       "       1.76865290e+01, 4.84145708e+00, 1.73240958e+00, 1.56782503e-01,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.67513133e-01, 2.62320309e+00, 7.49193927e+00, 1.41923569e+01,\n",
       "       3.65973602e+01, 4.86438403e+01, 9.89664586e+01, 1.19851872e+02,\n",
       "       3.32342312e+02, 3.75689721e+02, 2.03551095e+02, 3.97834851e+02,\n",
       "       2.96999068e+02, 5.02372794e+02, 6.66298326e+02, 6.61418367e+02,\n",
       "       7.56156950e+02, 3.49120326e+02, 3.34207932e+02, 3.61073821e+02,\n",
       "       7.76768289e+02, 4.06563906e+02, 3.07802644e+02, 3.72447094e+02,\n",
       "       2.74111390e+02, 3.13406300e+02, 5.68679539e+02, 3.52963150e+02,\n",
       "       4.51865867e+02, 5.58336477e+02, 4.85970083e+02, 4.78022084e+02,\n",
       "       6.12353483e+02, 9.37509724e+02, 1.01278705e+03, 9.87180397e+02,\n",
       "       8.79572642e+02, 9.13877156e+02, 8.76962063e+02, 8.23735433e+02,\n",
       "       7.89800362e+02, 7.09062692e+02, 3.88941815e+02, 4.75067980e+02,\n",
       "       5.16819368e+02, 5.42044863e+02, 4.67830444e+02, 3.77916838e+02,\n",
       "       3.13684677e+02, 1.27355739e+02, 2.14679646e+02, 8.32158538e+01,\n",
       "       9.33200728e+01, 4.37173327e+01, 1.11818222e+01, 2.39087065e+00,\n",
       "       1.82432722e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.72572198e-01, 1.26727902e+00, 4.77694464e+00,\n",
       "       1.38650433e+01, 3.93160259e+01, 7.38246812e+01, 9.23255611e+01,\n",
       "       9.97676702e+01, 1.85081651e+02, 2.88769591e+02, 2.27720812e+02,\n",
       "       5.04528614e+02, 2.17052812e+02, 5.49341947e+02, 4.30866508e+02,\n",
       "       5.53649314e+02, 7.05444423e+02, 8.74066768e+02, 9.64656807e+02,\n",
       "       7.89051547e+02, 1.04618226e+03, 4.64360546e+02, 4.70577339e+02,\n",
       "       1.03367951e+03, 9.32368070e+02, 8.44973288e+02, 1.01250286e+03,\n",
       "       1.03116352e+03, 9.62677358e+02, 1.04296860e+03, 1.09448933e+03,\n",
       "       6.10290154e+02, 5.23041007e+02, 1.12966140e+03, 1.00817678e+03,\n",
       "       8.96933041e+02, 5.03634872e+02, 8.94839334e+02, 8.08628597e+02,\n",
       "       7.00077879e+02, 7.31622470e+02, 7.16644067e+02, 6.73810989e+02,\n",
       "       6.37357330e+02, 5.64422212e+02, 4.50301150e+02, 3.89163457e+02,\n",
       "       3.77684667e+02, 3.15534250e+02, 2.63245522e+02, 1.92943702e+02,\n",
       "       1.78340281e+02, 6.35628033e+01, 3.22700043e+01, 1.54281114e+01,\n",
       "       1.93954915e+00, 1.28960279e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.74518827e-01, 3.61772146e+00,\n",
       "       1.13063654e+01, 2.29384518e+01, 3.70405933e+01, 8.76393712e+01,\n",
       "       8.87671228e+01, 1.14068462e+02, 1.61467498e+02, 3.19299844e+02,\n",
       "       4.69444223e+02, 4.81109746e+02, 5.29011447e+02, 5.98651362e+02,\n",
       "       3.16028907e+02, 2.49335139e+02, 4.57754790e+02, 6.68751497e+02,\n",
       "       7.03526373e+02, 5.13063754e+02, 8.69278373e+02, 8.79952183e+02,\n",
       "       7.09570877e+02, 8.96845928e+02, 1.04564423e+03, 4.88263276e+02,\n",
       "       6.30461120e+02, 7.54743358e+02, 1.02033622e+03, 4.45225284e+02,\n",
       "       4.41365466e+02, 6.83099146e+02, 1.04095255e+03, 1.01013225e+03,\n",
       "       1.11034770e+03, 7.42180290e+02, 6.69606087e+02, 9.00354436e+02,\n",
       "       7.82997613e+02, 3.36969158e+02, 3.62715139e+02, 8.02149501e+02,\n",
       "       4.01040629e+02, 4.45884574e+02, 3.61820174e+02, 3.95238597e+02,\n",
       "       3.49761744e+02, 1.34118840e+02, 2.79494292e+02, 1.14717851e+02,\n",
       "       1.26512952e+02, 1.10746734e+02, 5.64759107e+01, 4.60557356e+01,\n",
       "       1.44327087e+01, 1.95165669e+00, 2.03402640e-01, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.70474056e-01,\n",
       "       1.46555343e+00, 6.57557488e+00, 1.76070801e+01, 2.61690718e+01,\n",
       "       1.02573101e+02, 2.48736586e+02, 2.52659332e+02, 2.73598090e+02,\n",
       "       3.71563554e+02, 4.44021053e+02, 4.87698058e+02, 5.44350811e+02,\n",
       "       6.09473568e+02, 6.36314933e+02, 2.43344778e+02, 4.12181964e+02,\n",
       "       7.61761500e+02, 7.73267039e+02, 1.00195561e+03, 7.00481961e+02,\n",
       "       9.81279326e+02, 5.05983441e+02, 7.34242309e+02, 5.97944836e+02,\n",
       "       4.78308531e+02, 5.31185423e+02, 8.01882298e+02, 7.67723760e+02,\n",
       "       1.08345041e+03, 6.42226834e+02, 5.80363739e+02, 1.03018011e+03,\n",
       "       1.17706695e+03, 8.19515551e+02, 8.13764502e+02, 9.85171640e+02,\n",
       "       7.12497502e+02, 8.23787159e+02, 8.47463021e+02, 8.45931131e+02,\n",
       "       7.27749287e+02, 6.60125546e+02, 6.05217916e+02, 5.57839158e+02,\n",
       "       5.02505578e+02, 4.23036409e+02, 3.33220214e+02, 8.64819013e+01,\n",
       "       9.19107761e+01, 7.49849327e+01, 3.60634143e+01, 3.08907121e+01,\n",
       "       1.80407918e+01, 8.77723566e+00, 2.52364425e+00, 2.17056063e-01,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.01946189e-01, 1.88990172e+00, 8.54363372e+00, 2.00708167e+01,\n",
       "       2.74143166e+01, 3.93642898e+01, 5.23370727e+01, 9.91197817e+01,\n",
       "       2.80519640e+02, 3.76394249e+02, 4.35036866e+02, 5.01458001e+02,\n",
       "       5.58204002e+02, 6.02746832e+02, 6.81450804e+02, 5.77428849e+02,\n",
       "       5.51428482e+02, 6.94600536e+02, 8.60235271e+02, 7.30574090e+02,\n",
       "       6.08737336e+02, 8.56873136e+02, 5.74610776e+02, 1.87558446e+02,\n",
       "       2.20296516e+02, 1.86253370e+02, 2.03477369e+02, 2.56841814e+02,\n",
       "       3.34799048e+02, 4.76299133e+02, 9.95782064e+02, 1.02848999e+03,\n",
       "       1.07529308e+03, 9.06960023e+02, 6.44590353e+02, 6.74204051e+02,\n",
       "       3.68838201e+02, 6.07299900e+02, 5.82879931e+02, 5.66037292e+02,\n",
       "       6.99146933e+02, 3.65820799e+02, 2.00969283e+02, 1.43192074e+02,\n",
       "       1.27206570e+02, 8.76855339e+01, 1.41105514e+02, 1.27833211e+02,\n",
       "       8.68318518e+01, 3.75864853e+01, 2.84494572e+01, 2.26312156e+01,\n",
       "       1.43961089e+01, 8.29746009e+00, 5.01896002e+00, 1.73539834e+00,\n",
       "       1.44717486e-01, 0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[test_start : test_end].DHHL_3.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df[test_start : test_end].DHHL_3.values)\n",
    "plt.plot(forecasts_final[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_multi_forecast(train_df, test_df, _order, _steps, _neurons, _epochs):\n",
    "\n",
    "    \n",
    "    nfeat = len(train_df.columns)\n",
    "    nlags = _order\n",
    "    nsteps = _steps\n",
    "    nobs = nlags * nfeat\n",
    "    \n",
    "    train_reshaped_df = series_to_supervised(train_df, n_in=nlags, n_out=nsteps)\n",
    "    train_X, train_Y = train_reshaped_df.iloc[:,:nobs].values, train_reshaped_df.iloc[:,-nfeat].values\n",
    "    train_X = train_X.reshape((train_X.shape[0], nlags, nfeat))\n",
    "\n",
    "    test_reshaped_df = series_to_supervised(test_df, n_in=nlags, n_out=nsteps)\n",
    "    test_X, test_Y = test_reshaped_df.iloc[:,:nobs].values, test_reshaped_df.iloc[:,-nfeat].values\n",
    "    test_X = test_X.reshape((test_X.shape[0], nlags, nfeat))\n",
    "    \n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(_neurons, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "    # design network\n",
    "#    model = Sequential()\n",
    "#    model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "#    model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "#    model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "#    model.add(LSTM(_neurons, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "#    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "#    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "#    model = Sequential()\n",
    "#    _dropout = 0.1\n",
    "#    model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', dropout=_dropout, kernel_constraint=maxnorm(3)))\n",
    "#    model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', dropout=_dropout, kernel_constraint=maxnorm(3)))\n",
    "#    model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', dropout=_dropout, kernel_constraint=maxnorm(3)))\n",
    "#    model.add(LSTM(_neurons, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', dropout=_dropout, kernel_constraint=maxnorm(3)))\n",
    "#    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "#    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "    # fit network\n",
    "    model.fit(train_X, train_Y, epochs=_epochs, batch_size=1000, verbose=False, shuffle=False)\n",
    "    \n",
    "    forecast = model.predict(test_X)\n",
    "    \n",
    "    fcst = [f[0] for f in forecast]\n",
    "\n",
    "    return fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLD_evaluate_lstm_models(test_name, train, validation, order_list, neurons_list, epochs_list):\n",
    "    \n",
    "    lstm_results = pd.DataFrame(columns=['Order','Neurons','Epochs','RMSE'])\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "\n",
    "    for _order in order_list:\n",
    "        for _neurons in neurons_list:\n",
    "            for _epochs in epochs_list:\n",
    "                forecast = lstm_multi_forecast(train, validation, _order, 1, _neurons, _epochs)\n",
    "\n",
    "                obs = validation[target_station].values\n",
    "                rmse = Measures.rmse(obs[_order:], forecast)\n",
    "\n",
    "                if rmse < best_score:\n",
    "                    best_score, best_cfg = rmse, (_order,_neurons,_epochs)\n",
    "\n",
    "                res = {'Order' : str(_order) ,'Neurons' : str(_neurons) ,'Epochs' : str(_epochs) ,'RMSE' : rmse}\n",
    "                print('LSTM %s - %s - %s  RMSE=%.3f' % (str(_order), _neurons, str(_epochs),rmse))\n",
    "                lstm_results = lstm_results.append(res, ignore_index=True)\n",
    "                lstm_results.to_csv(test_name+\".csv\")\n",
    "\n",
    "    print('Best LSTM(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    \n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multivariate_lstm_models(test_name, train_df, validation_df, neurons_list, order_list, epochs_list):\n",
    "    \n",
    "    lstm_results = pd.DataFrame(columns=['Neurons','Order','Epochs','RMSE'])\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    \n",
    "    nfeat = len(train_df.columns)\n",
    "    nsteps = 1\n",
    "    \n",
    "    for _neurons in neurons_list:\n",
    "        for _order in order_list:\n",
    "            for epochs in epochs_list:\n",
    "                    \n",
    "                    nobs = nfeat * _order\n",
    "                    \n",
    "                    train_reshaped_df = series_to_supervised(train_df, n_in=_order, n_out=nsteps)\n",
    "                    train_X, train_Y = train_reshaped_df.iloc[:,:nobs].values, train_reshaped_df.iloc[:,-nfeat].values\n",
    "                    train_X = train_X.reshape((train_X.shape[0], _order, nfeat))                    \n",
    "                    \n",
    "                    val_reshaped_df = series_to_supervised(validation_df, n_in=_order, n_out=nsteps)\n",
    "                    validation_X, validation_Y = val_reshaped_df.iloc[:,:nobs].values, val_reshaped_df.iloc[:,-nfeat].values\n",
    "                    validation_X = validation_X.reshape((validation_X.shape[0], _order, nfeat))\n",
    "                    \n",
    "                    # design network\n",
    "                    model = Sequential()\n",
    "                    model.add(LSTM(_neurons, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "                    model.add(Dense(1))\n",
    "                    model.compile(loss='mae', optimizer='adam')\n",
    " \n",
    "                    # design network\n",
    "                    #model = Sequential()\n",
    "                    #model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "                    #model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "                    #model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "                    #model.add(LSTM(_neurons, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "                    #model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "                    #model.compile(loss='mae', optimizer='adam')\n",
    "                    \n",
    "\n",
    "                    # fit network\n",
    "                    history = model.fit(train_X, train_Y, epochs=epochs, batch_size=1000, verbose=False, shuffle=False)\n",
    "                    forecast = model.predict(validation_X)\n",
    "                    fcst = [f[0] for f in forecast]\n",
    "                    \n",
    "                    \n",
    "                    rmse = Measures.rmse(validation_Y, fcst)\n",
    "                    #rmse = math.sqrt(mean_squared_error(validation_Y, forecast))\n",
    "                    \n",
    "                    params = (_neurons, _order,epochs)\n",
    "                    if rmse < best_score:\n",
    "                        best_score, best_cfg = rmse, params\n",
    "\n",
    "                    res = {'Neurons':_neurons, 'Order':_order, 'Epochs' : epochs ,'RMSE' : rmse}\n",
    "                    print('LSTM %s  RMSE=%.3f' % (params,rmse))\n",
    "                    lstm_results = lstm_results.append(res, ignore_index=True)\n",
    "                    lstm_results.to_csv(test_name+\".csv\")\n",
    "\n",
    "    print('Best LSTM(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_lstm_multi(df, step):\n",
    "    \n",
    "    neurons_list = np.arange(50,110,50)\n",
    "    order_list = np.arange(2,4)\n",
    "    epochs_list = [100]\n",
    "\n",
    "    neurons_list = [5]\n",
    "    order_list = [2]\n",
    "    epochs_list = [1]\n",
    "\n",
    "    lags_list = []\n",
    "    forecasts = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (_neurons, _order,epochs) = evaluate_multivariate_lstm_models(\"nested_eval_lstm_multi_oahu\", train[neighbor_stations_90], validation[neighbor_stations_90], neurons_list, order_list, epochs_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = lstm_multi_forecast(train[neighbor_stations_90], test[neighbor_stations_90], _order, 1, _neurons,epochs)\n",
    "        \n",
    "        yhat.append(0) #para manter o formato do vetor de metricas\n",
    "        \n",
    "        lags_list.append(_order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_lstm_multi(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_lstm_multi(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_lstm_multi\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_lstm_uni(df, step):\n",
    "    \n",
    "    neurons_list = np.arange(50,110,50)\n",
    "    order_list = np.arange(2,4)\n",
    "    epochs_list = [100]\n",
    "\n",
    "    neurons_list = [5]\n",
    "    order_list = [2]\n",
    "    epochs_list = [1]\n",
    "\n",
    "    lags_list = []\n",
    "    forecasts = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (_neurons, _order,epochs) = evaluate_multivariate_lstm_models(\"nested_eval_lstm_multi_oahu\", train[[target_station]], validation[[target_station]], neurons_list, order_list, epochs_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = lstm_multi_forecast(train[[target_station]], test[[target_station]], _order, 1, _neurons,epochs)\n",
    "        \n",
    "        yhat.append(0) #para manter o formato do vetor de metricas\n",
    "        \n",
    "        lags_list.append(_order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_lstm_uni(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_lstm_uni(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_lstm_uni\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_multi_forecast(train_df, test_df, _order, _steps, _neurons, _epochs):\n",
    "\n",
    "    \n",
    "    nfeat = len(train_df.columns)\n",
    "    nlags = _order\n",
    "    nsteps = _steps\n",
    "    nobs = nlags * nfeat\n",
    "    \n",
    "    train_reshaped_df = series_to_supervised(train_df, n_in=nlags, n_out=nsteps)\n",
    "    train_X, train_Y = train_reshaped_df.iloc[:,:nobs].values, train_reshaped_df.iloc[:,-nfeat].values\n",
    "    \n",
    "    test_reshaped_df = series_to_supervised(test_df, n_in=nlags, n_out=nsteps)\n",
    "    test_X, test_Y = test_reshaped_df.iloc[:,:nobs].values, test_reshaped_df.iloc[:,-nfeat].values\n",
    "    \n",
    "    # design network\n",
    "    model = designMLPNetwork(_neurons,train_X.shape[1])\n",
    "    \n",
    "    # fit network\n",
    "    model.fit(train_X, train_Y, epochs=_epochs, batch_size=1000, verbose=False, shuffle=False)\n",
    "    \n",
    "    forecast = model.predict(test_X)\n",
    "    \n",
    "    fcst = [f[0] for f in forecast]\n",
    "\n",
    "    return fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multivariate_mlp_models(test_name, train_df, validation_df, neurons_list, order_list, epochs_list):\n",
    "    \n",
    "    lstm_results = pd.DataFrame(columns=['Neurons','Order','Epochs','RMSE'])\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    \n",
    "    nfeat = len(train_df.columns)\n",
    "    nsteps = 1\n",
    "    \n",
    "    for _neurons in neurons_list:\n",
    "        for _order in order_list:\n",
    "            for epochs in epochs_list:\n",
    "                    \n",
    "                    nobs = nfeat * _order\n",
    "                    \n",
    "                    train_reshaped_df = series_to_supervised(train_df, n_in=_order, n_out=nsteps)\n",
    "                    train_X, train_Y = train_reshaped_df.iloc[:,:nobs].values, train_reshaped_df.iloc[:,-nfeat].values\n",
    "                    \n",
    "                    val_reshaped_df = series_to_supervised(validation_df, n_in=_order, n_out=nsteps)\n",
    "                    validation_X, validation_Y = val_reshaped_df.iloc[:,:nobs].values, val_reshaped_df.iloc[:,-nfeat].values\n",
    "                   \n",
    "                    model = designMLPNetwork(_neurons,train_X.shape[1])\n",
    "                                        \n",
    "                    # fit network\n",
    "                    history = model.fit(train_X, train_Y, epochs=epochs, batch_size=1000, verbose=False, shuffle=False)\n",
    "                    forecast = model.predict(validation_X)\n",
    "                    fcst = [f[0] for f in forecast]\n",
    "                    \n",
    "                    \n",
    "                    rmse = Measures.rmse(validation_Y, fcst)\n",
    "                    #rmse = math.sqrt(mean_squared_error(validation_Y, forecast))\n",
    "                    \n",
    "                    params = (_neurons, _order,epochs)\n",
    "                    if rmse < best_score:\n",
    "                        best_score, best_cfg = rmse, params\n",
    "\n",
    "                    res = {'Neurons':_neurons, 'Order':_order, 'Epochs' : epochs ,'RMSE' : rmse}\n",
    "                    print('LSTM %s  RMSE=%.3f' % (params,rmse))\n",
    "                    lstm_results = lstm_results.append(res, ignore_index=True)\n",
    "                    lstm_results.to_csv(test_name+\".csv\")\n",
    "\n",
    "    print('Best MLP(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def designMLPNetwork(neurons, shape):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_dim=shape))\n",
    "    model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_mlp_multi(df, step):\n",
    "    \n",
    "    neurons_list = np.arange(50,110,50)\n",
    "    order_list = np.arange(2,4)\n",
    "    epochs_list = [100]\n",
    "\n",
    "    neurons_list = [50]\n",
    "    order_list = [4]\n",
    "    epochs_list = [500]\n",
    "\n",
    "    lags_list = []\n",
    "    forecasts = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (_neurons, _order,epochs) = evaluate_multivariate_mlp_models(\"nested_eval_mlp_multi_oahu\", train[neighbor_stations_90], validation[neighbor_stations_90], neurons_list, order_list, epochs_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = mlp_multi_forecast(train[neighbor_stations_90], test[neighbor_stations_90], _order, 1, _neurons,epochs)\n",
    "        \n",
    "        yhat.append(0) #para manter o formato do vetor de metricas\n",
    "        \n",
    "        lags_list.append(_order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_mlp_multi(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_mlp_multi(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)\n",
    "calculate_rolling_error(\"rolling_cv_oahu_mlp_multi\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_mlp_uni(df, step):\n",
    "    \n",
    "    neurons_list = [50]\n",
    "    order_list = [4]\n",
    "    epochs_list = [500]\n",
    "\n",
    "    lags_list = []\n",
    "    forecasts = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (_neurons, _order,epochs) = evaluate_multivariate_mlp_models(\"nested_eval_mlp_uni_oahu\", train[[target_station]], validation[[target_station]], neurons_list, order_list, epochs_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = mlp_multi_forecast(train[[target_station]], test[[target_station]], _order, 1, _neurons,epochs)\n",
    "        \n",
    "        yhat.append(0) #para manter o formato do vetor de metricas\n",
    "        \n",
    "        lags_list.append(_order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_mlp_uni(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_mlp_uni(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)\n",
    "calculate_rolling_error(\"rolling_cv_oahu_mlp_uni\", df, forecasts_final, order_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
