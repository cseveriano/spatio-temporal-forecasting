{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "from pyFTS.benchmarks import Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    mindf = df.min()\n",
    "    maxdf = df.max()\n",
    "    return (df-mindf)/(maxdf-mindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(norm, _min, _max):\n",
    "    return [(n * (_max-_min)) + _min for n in norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('results/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('results/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set target and input variables \n",
    "target_station = 'DHHL_3'\n",
    "\n",
    "#All neighbor stations with residual correlation greater than .90\n",
    "neighbor_stations_90 = ['DHHL_3',  'DHHL_4','DHHL_5','DHHL_10','DHHL_11','DHHL_9','DHHL_2', 'DHHL_6','DHHL_7','DHHL_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"df_oahu.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove columns with many corrupted or missing values\n",
    "df.drop(columns=['AP_1', 'AP_7'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data form the interval of interest\n",
    "interval = ((df.index >= '2010-06') & (df.index < '2010-08'))\n",
    "df = df.loc[interval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize Data\n",
    "\n",
    "# Save Min-Max for Denorm\n",
    "min_raw = df[target_station].min()\n",
    "\n",
    "max_raw = df[target_station].max()\n",
    "\n",
    "# Perform Normalization\n",
    "norm_df = normalize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_forecast(forecast_list):\n",
    "    forecast_final = []\n",
    "    \n",
    "    for fcst in forecast_list:\n",
    "        forecast_final.append(denormalize(fcst, min_raw, max_raw))\n",
    "        \n",
    "    return forecast_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Rolling window indexes\n",
    "\n",
    "- Training: 4 weeks\n",
    "- Validation: 1 week\n",
    "- Test: 1 week\n",
    "\n",
    "Roliing daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRollingWindow(index):\n",
    "    pivot = index\n",
    "    train_start = pivot.strftime('%Y-%m-%d')\n",
    "    pivot = pivot + datetime.timedelta(days=27)\n",
    "    train_end = pivot.strftime('%Y-%m-%d')\n",
    "\n",
    "    pivot = pivot + datetime.timedelta(days=1)\n",
    "    validation_start = pivot.strftime('%Y-%m-%d')\n",
    "    pivot = pivot + datetime.timedelta(days=6)\n",
    "    validation_end = pivot.strftime('%Y-%m-%d')\n",
    "\n",
    "    pivot = pivot + datetime.timedelta(days=1)\n",
    "    test_start = pivot.strftime('%Y-%m-%d')\n",
    "    pivot = pivot + datetime.timedelta(days=6)\n",
    "    test_end = pivot.strftime('%Y-%m-%d')\n",
    "    \n",
    "    return train_start, train_end, validation_start, validation_end, test_start, test_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_error(cv_name, df, forecasts, order_list):\n",
    "    cv_results = pd.DataFrame(columns=['Split', 'RMSE', 'SMAPE', 'U'])\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    for i in np.arange(len(forecasts)):\n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        test = df[test_start : test_end]\n",
    "    \n",
    "        yhat = forecasts[i]\n",
    "        order = order_list[i]\n",
    "        \n",
    "        rmse = Measures.rmse(test[target_station].iloc[order:], yhat[:-1])\n",
    "        \n",
    "        smape = Measures.smape(test[target_station].iloc[order:], yhat[:-1])\n",
    "        \n",
    "        u = Measures.UStatistic(test[target_station].iloc[order:], yhat[:-1])\n",
    "       \n",
    "        res = {'Split' : index.strftime('%Y-%m-%d') ,'RMSE' : rmse, 'SMAPE' : smape, 'U' : u}\n",
    "        cv_results = cv_results.append(res, ignore_index=True)\n",
    "        cv_results.to_csv(cv_name+\".csv\")        \n",
    "\n",
    "        index = index + datetime.timedelta(days=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistence_forecast(train, test, step):\n",
    "    predictions = []\n",
    "    \n",
    "    for t in np.arange(0,len(test), step):\n",
    "        yhat = [test.iloc[t]]  * step\n",
    "        predictions.extend(yhat)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_persistence(df, step):\n",
    "\n",
    "    forecasts = []\n",
    "    lags_list = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "    \n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "        yhat = persistence_forecast(train[target_station], test[target_station], step)        \n",
    "        \n",
    "        lags_list.append(1)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  2010-06-01\n",
      "Index:  2010-06-08\n",
      "Index:  2010-06-15\n",
      "Index:  2010-06-22\n"
     ]
    }
   ],
   "source": [
    "forecasts, order_list = rolling_cv_persistence(norm_df, 1)\n",
    "forecasts_final = get_final_forecast(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_raw_persistence\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cseveriano/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from itertools import product\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_SARIMA_models(test_name, train, validation, parameters_list, period_length):\n",
    "\n",
    "    sarima_results = pd.DataFrame(columns=['Order','RMSE'])\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "\n",
    "    for param in parameters_list:\n",
    "        arima_order = (param[0],param[1],param[2])\n",
    "        sarima_order = (param[3],param[4],param[5],period_length)\n",
    "        print('Testing SARIMA%s %s ' % (str(arima_order),str(sarima_order)))\n",
    "        try:\n",
    "            fcst = sarima_forecast(train, validation, arima_order, sarima_order)\n",
    "            rmse = Measures.rmse(validation.values, fcst)\n",
    "            \n",
    "            if rmse < best_score:\n",
    "                best_score, best_cfg = rmse, (arima_order, sarima_order)\n",
    "\n",
    "            res = {'Parameters' : str(param) ,'RMSE' : rmse}\n",
    "            print('SARIMA%s %s RMSE=%.3f' % (str(arima_order),str(sarima_order),rmse))\n",
    "            sarima_results = sarima_results.append(res, ignore_index=True)\n",
    "            sarima_results.to_csv(test_name+\".csv\")\n",
    "        except:\n",
    "            print(sys.exc_info())\n",
    "            print('Invalid model%s %s ' % (str(arima_order),str(sarima_order)))\n",
    "            continue\n",
    "    \n",
    "    print('Best SARIMA(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarima_forecast(train, test, arima_order, sarima_order):\n",
    "\n",
    "    predictions = []\n",
    "    window_size = sarima_order[3] * 5\n",
    "    \n",
    "    history = list(train.iloc[-window_size:])\n",
    "\n",
    "    model = SARIMAX(history, order=arima_order, seasonal_order=sarima_order,enforce_invertibility=False,enforce_stationarity=False)\n",
    "    model_fit = model.fit(disp=True,enforce_invertibility=False, maxiter=10)\n",
    "\n",
    "    \n",
    "    #save the state parameter\n",
    "    est_params = model_fit.params\n",
    "    est_state = model_fit.predicted_state[:, -1]\n",
    "    est_state_cov = model_fit.predicted_state_cov[:, :, -1]\n",
    "\n",
    "    st = 0\n",
    "        \n",
    "    for t in np.arange(1,len(test)+1,1):\n",
    "        obs = test.iloc[st:t].values\n",
    "        history.extend(obs)\n",
    "        history = history[-window_size:]\n",
    "        \n",
    "        mod_updated = SARIMAX(history, order=arima_order, seasonal_order=sarima_order,enforce_invertibility=False,enforce_stationarity=False)\n",
    "        mod_updated.initialize_known(est_state, est_state_cov)\n",
    "        mod_frcst = mod_updated.smooth(est_params)\n",
    "        \n",
    "        yhat = mod_frcst.forecast(1)   \n",
    "        predictions.extend(yhat)\n",
    "            \n",
    "        est_params = mod_frcst.params\n",
    "        est_state = mod_frcst.predicted_state[:, -1]\n",
    "        est_state_cov = mod_frcst.predicted_state_cov[:, :, -1]\n",
    "            \n",
    "        st = t\n",
    "                \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_SARIMA(df, step):\n",
    "\n",
    "#    p_values = [0,1,2]\n",
    "#    d_values = [0,1]\n",
    "#    q_values = [0,1,2]\n",
    "#    P_values = [0,1]\n",
    "#    D_Values = [0,1]\n",
    "#    Q_Values = [0,1]\n",
    "\n",
    "    p_values = [2]\n",
    "    d_values = [1]\n",
    "    q_values = [2]\n",
    "    P_values = [1]\n",
    "    D_Values = [1]\n",
    "    Q_Values = [1]\n",
    "\n",
    "    parameters = product(p_values, d_values, q_values, P_values, D_Values, Q_Values)\n",
    "    parameters_list = list(parameters)\n",
    "    period_length = 61 #de 5:00 as 20:00\n",
    "\n",
    "    \n",
    "    forecasts = []\n",
    "    lags_list = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "    \n",
    "        # Perform grid search\n",
    "        #(arima_params, sarima_params) = evaluate_SARIMA_models(\"nested_test_sarima_oahu\", train[target_station], validation[target_station], parameters_list, period_length)\n",
    "        arima_params = (2, 1, 2)\n",
    "        sarima_params = (1, 1, 1, 61)\n",
    "        \n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "        yhat = sarima_forecast(train[target_station], test[target_station], arima_params, sarima_params)        \n",
    "        \n",
    "        lags_list.append(1)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  2010-06-01\n",
      "Index:  2010-06-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cseveriano/anaconda3/lib/python3.6/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5718bb417f09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mforecasts_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder_list_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrolling_cv_SARIMA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_df_ssa_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mforecasts_residual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder_list_residual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrolling_cv_SARIMA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_df_ssa_residual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-f380ab5a788b>\u001b[0m in \u001b[0;36mrolling_cv_SARIMA\u001b[0;34m(df, step)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Concat train & validation for test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msarima_forecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_station\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_station\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marima_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msarima_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mlags_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-d2e5eff08b60>\u001b[0m in \u001b[0;36msarima_forecast\u001b[0;34m(train, test, arima_order, sarima_order)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmod_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSARIMAX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marima_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseasonal_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msarima_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menforce_invertibility\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menforce_stationarity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mmod_updated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest_state_cov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mmod_frcst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod_updated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod_frcst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/tsa/statespace/sarimax.py\u001b[0m in \u001b[0;36msmooth\u001b[0;34m(self, params, **kwargs)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results_class'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSARIMAXResults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results_wrapper_class'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSARIMAXResultsWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSARIMAX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/tsa/statespace/mlemodel.py\u001b[0m in \u001b[0;36msmooth\u001b[0;34m(self, params, transformed, complex_step, cov_type, cov_kwds, return_ssm, results_class, results_wrapper_class, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             result = results_wrapper_class(\n\u001b[0;32m--> 577\u001b[0;31m                 \u001b[0mresults_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mresult_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m             )\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/tsa/statespace/sarimax.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, params, filter_results, cov_type, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'opg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m         super(SARIMAXResults, self).__init__(model, params, filter_results,\n\u001b[0;32m-> 1716\u001b[0;31m                                              cov_type, **kwargs)\n\u001b[0m\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_resid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m  \u001b[0;31m# attribute required for wald tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/tsa/statespace/mlemodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, params, results, cov_type, cov_kwds, **kwargs)\u001b[0m\n\u001b[1;32m   1572\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m             self._get_robustcov_results(cov_type=cov_type, use_self=True,\n\u001b[0;32m-> 1574\u001b[0;31m                                         **cov_kwds)\n\u001b[0m\u001b[1;32m   1575\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinAlgError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/tsa/statespace/mlemodel.py\u001b[0m in \u001b[0;36m_get_robustcov_results\u001b[0;34m(self, cov_type, **kwargs)\u001b[0m\n\u001b[1;32m   1697\u001b[0m                 ' matrix (%s) described in Harvey (1989).' % approx_type_str)\n\u001b[1;32m   1698\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'opg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1699\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov_params_default\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov_params_opg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1700\u001b[0m             res.cov_kwds['description'] = (\n\u001b[1;32m   1701\u001b[0m                 \u001b[0;34m'Covariance matrix calculated using the outer product of'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/tools/decorators.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, type)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cachedval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Call the \"fget\" function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0m_cachedval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;31m# Set the attribute in obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# print(\"Setting %s in cache to %s\" % (name, _cachedval))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/tsa/statespace/mlemodel.py\u001b[0m in \u001b[0;36mcov_params_opg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1814\u001b[0m         \"\"\"\n\u001b[1;32m   1815\u001b[0m         return self._cov_params_opg(self._cov_approx_complex_step,\n\u001b[0;32m-> 1816\u001b[0;31m                                     self._cov_approx_centered)\n\u001b[0m\u001b[1;32m   1817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1818\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcache_readonly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/tsa/statespace/mlemodel.py\u001b[0m in \u001b[0;36m_cov_params_opg\u001b[0;34m(self, approx_complex_step, approx_centered)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 \u001b[0mapprox_complex_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapprox_complex_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1800\u001b[0;31m                 approx_centered=approx_centered)\n\u001b[0m\u001b[1;32m   1801\u001b[0m         )\n\u001b[1;32m   1802\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/tsa/statespace/mlemodel.py\u001b[0m in \u001b[0;36mopg_information_matrix\u001b[0;34m(self, params, transformed, approx_complex_step, **kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m         score_obs = self.score_obs(params, transformed=transformed,\n\u001b[1;32m    906\u001b[0m                                    \u001b[0mapprox_complex_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapprox_complex_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m                                    **kwargs).transpose()\n\u001b[0m\u001b[1;32m    908\u001b[0m         return (\n\u001b[1;32m    909\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_obs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/tsa/statespace/mlemodel.py\u001b[0m in \u001b[0;36mscore_obs\u001b[0;34m(self, params, method, transformed, approx_complex_step, approx_centered, **kwargs)\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transformed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             score = approx_fprime_cs(params, self.loglikeobs, epsilon=epsilon,\n\u001b[0;32m-> 1124\u001b[0;31m                                      kwargs=kwargs)\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'approx'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transformed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/tools/numdiff.py\u001b[0m in \u001b[0;36mapprox_fprime_cs\u001b[0;34m(x, f, epsilon, args, kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;31m# TODO: see if this can be vectorized, but usually dim is small\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     partials = [f(x+ih, *args, **kwargs).imag / epsilon[i]\n\u001b[0;32m--> 202\u001b[0;31m                 for i, ih in enumerate(increments)]\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/tools/numdiff.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;31m# TODO: see if this can be vectorized, but usually dim is small\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     partials = [f(x+ih, *args, **kwargs).imag / epsilon[i]\n\u001b[0;32m--> 202\u001b[0;31m                 for i, ih in enumerate(increments)]\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/tsa/statespace/mlemodel.py\u001b[0m in \u001b[0;36mloglikeobs\u001b[0;34m(self, params, transformed, complex_step, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplex_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomplex_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglikeobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomplex_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m     def _forecasts_error_partial_derivatives(self, params, transformed=True,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/tsa/statespace/kalman_filter.py\u001b[0m in \u001b[0;36mloglikeobs\u001b[0;34m(self, loglikelihood_burn, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0mloglikelihood_burn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglikelihood_burn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'loglikelihood'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m         \u001b[0mllf_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0;31m# Set any burned observations to have zero likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/tsa/statespace/kalman_filter.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, filter_method, inversion_method, stability_method, conserve_memory, tolerance, loglikelihood_burn, results, complex_step)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;31m# Run the filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mkfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;31m# We may just want the loglikelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "forecasts, order_list = rolling_cv_SARIMA(norm_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_raw_sarima\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Autoregressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cseveriano/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.api import VAR, DynamicVAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_VAR_models(test_name, train, validation,target, maxlags_list):\n",
    "    var_results = pd.DataFrame(columns=['Order','RMSE'])\n",
    "    best_score, best_cfg, best_model = float(\"inf\"), None, None\n",
    "    \n",
    "    for lgs in maxlags_list:\n",
    "        model = VAR(train)\n",
    "        results = model.fit(maxlags=lgs, ic='aic')\n",
    "        \n",
    "        order = results.k_ar\n",
    "        forecast = []\n",
    "\n",
    "        for i in range(len(validation)-order) :\n",
    "            forecast.extend(results.forecast(validation.values[i:i+order],1))\n",
    "\n",
    "        forecast_df = pd.DataFrame(columns=validation.columns, data=forecast)\n",
    "        rmse = Measures.rmse(validation[target].iloc[order:], forecast_df[target].values)\n",
    "\n",
    "        if rmse < best_score:\n",
    "            best_score, best_cfg, best_model = rmse, order, results\n",
    "\n",
    "        res = {'Order' : str(order) ,'RMSE' : rmse}\n",
    "        print('VAR (%s)  RMSE=%.3f' % (str(order),rmse))\n",
    "        var_results = var_results.append(res, ignore_index=True)\n",
    "        var_results.to_csv(test_name+\".csv\")\n",
    "        \n",
    "    print('Best VAR(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_forecast(train, test, target, order, step):\n",
    "    model = VAR(train.values)\n",
    "    results = model.fit(maxlags=order)\n",
    "    lag_order = results.k_ar\n",
    "    print(\"Lag order:\" + str(lag_order))\n",
    "    forecast = []\n",
    "\n",
    "    for i in np.arange(0,len(test)-lag_order+1,step) :\n",
    "        forecast.extend(results.forecast(test.values[i:i+lag_order],step))\n",
    "\n",
    "    forecast_df = pd.DataFrame(columns=test.columns, data=forecast)\n",
    "    return forecast_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv_var(df, step):\n",
    "    maxlags_list = [1,2,4,6,8,10,20,40]\n",
    "    forecasts = []\n",
    "    order_list = []\n",
    "    \n",
    "    for i in np.arange(n_folds):\n",
    "        train = getNestedData(df, train_inds[i])\n",
    "        validation = getNestedData(df, val_inds[i])\n",
    "        test = getNestedData(df, test_inds[i])\n",
    "    \n",
    "        # Perform grid search\n",
    "        best_model = evaluate_VAR_models(\"nested_test_var_oahu\", train[neighbor_stations_90], validation[neighbor_stations_90],target_station, maxlags_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "        order = best_model.k_ar\n",
    "        yhat = var_forecast(train[neighbor_stations_90], test[neighbor_stations_90], target_station, order, step)\n",
    "        \n",
    "        order_list.append(order)\n",
    "        forecasts.append(yhat)\n",
    "    \n",
    "    return forecasts, order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_var(df, step):\n",
    "    maxlags_list = [1,2,4,6,8,10,20,40]\n",
    "    forecasts = []\n",
    "    order_list = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "    \n",
    "        # Perform grid search\n",
    "        best_model = evaluate_VAR_models(\"nested_test_var_oahu\", train[neighbor_stations_90], validation[neighbor_stations_90],target_station, maxlags_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "        order = best_model.k_ar\n",
    "        yhat = var_forecast(train[neighbor_stations_90], test[neighbor_stations_90], target_station, order, step)\n",
    "        \n",
    "        order_list.append(order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  2010-06-01\n",
      "VAR (1)  RMSE=0.093\n",
      "VAR (2)  RMSE=0.092\n",
      "VAR (4)  RMSE=0.094\n",
      "VAR (6)  RMSE=0.095\n",
      "VAR (8)  RMSE=0.097\n",
      "VAR (10)  RMSE=0.097\n",
      "VAR (10)  RMSE=0.097\n",
      "VAR (10)  RMSE=0.097\n",
      "Best VAR(2) RMSE=0.092\n",
      "Lag order:2\n",
      "Index:  2010-06-08\n",
      "VAR (1)  RMSE=0.120\n",
      "VAR (2)  RMSE=0.118\n",
      "VAR (4)  RMSE=0.118\n",
      "VAR (6)  RMSE=0.119\n",
      "VAR (8)  RMSE=0.118\n",
      "VAR (8)  RMSE=0.118\n",
      "VAR (8)  RMSE=0.118\n",
      "VAR (8)  RMSE=0.118\n",
      "Best VAR(8) RMSE=0.118\n",
      "Lag order:8\n",
      "Index:  2010-06-15\n",
      "VAR (1)  RMSE=0.121\n",
      "VAR (2)  RMSE=0.123\n",
      "VAR (4)  RMSE=0.123\n",
      "VAR (6)  RMSE=0.123\n",
      "VAR (8)  RMSE=0.125\n",
      "VAR (8)  RMSE=0.125\n",
      "VAR (8)  RMSE=0.125\n",
      "VAR (8)  RMSE=0.125\n",
      "Best VAR(1) RMSE=0.121\n",
      "Lag order:1\n",
      "Index:  2010-06-22\n",
      "VAR (1)  RMSE=0.104\n",
      "VAR (2)  RMSE=0.103\n",
      "VAR (4)  RMSE=0.105\n",
      "VAR (6)  RMSE=0.106\n",
      "VAR (8)  RMSE=0.107\n",
      "VAR (8)  RMSE=0.107\n",
      "VAR (8)  RMSE=0.107\n",
      "VAR (8)  RMSE=0.107\n",
      "Best VAR(2) RMSE=0.103\n",
      "Lag order:2\n"
     ]
    }
   ],
   "source": [
    "forecasts, order_list = rolling_cv_var(norm_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final = get_final_forecast(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_raw_var\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Order FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyFTS.partitioners import Grid, Entropy, Util as pUtil\n",
    "from pyFTS.models import hofts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hofts_models(test_name, train, validation, partitioners_list, order_list, partitions_list):\n",
    "    \n",
    "    hofts_results = pd.DataFrame(columns=['Partitioner','Partitions','Order','RMSE'])\n",
    "    best_score, best_cfg, best_model = float(\"inf\"), None, None\n",
    "\n",
    "    for _partitioner in partitioners_list:\n",
    "        for _order in order_list:\n",
    "            for npartitions in partitions_list:\n",
    "                fuzzy_sets = _partitioner(data=train.values, npart=npartitions)\n",
    "                model_simple_hofts = hofts.HighOrderFTS(order=_order)\n",
    "\n",
    "                model_simple_hofts.fit(train.values, order=_order, partitioner=fuzzy_sets)\n",
    "                \n",
    "                forecast = model_simple_hofts.predict(validation.values)\n",
    "                rmse = Measures.rmse(validation.iloc[_order:], forecast[:-1])\n",
    "\n",
    "                if rmse < best_score:\n",
    "                    best_score, best_cfg = rmse, (_order,npartitions,_partitioner)\n",
    "                    best_model = model_simple_hofts\n",
    "\n",
    "                res = {'Partitioner':str(_partitioner), 'Partitions':npartitions, 'Order' : str(_order) ,'RMSE' : rmse}\n",
    "                print('HOFTS %s - %s - %s  RMSE=%.3f' % (str(_partitioner), npartitions, str(_order),rmse))\n",
    "                hofts_results = hofts_results.append(res, ignore_index=True)\n",
    "                hofts_results.to_csv(test_name+\".csv\")\n",
    "\n",
    "    print('Best HOFTS(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    \n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hofts_forecast(train_df, test_df, _order, _partitioner, _npartitions):\n",
    "    \n",
    "    fuzzy_sets = _partitioner(data=train_df.values, npart=_npartitions)\n",
    "    model_simple_hofts = hofts.HighOrderFTS()\n",
    "    \n",
    "\n",
    "    model_simple_hofts.fit(train_df.values, order=_order, partitioner=fuzzy_sets)\n",
    "\n",
    "    \n",
    "    forecast = model_simple_hofts.predict(test_df.values)\n",
    "\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_hofts(df, step):\n",
    "    \n",
    "    partitioners_list = [Grid.GridPartitioner, Entropy.EntropyPartitioner]\n",
    "    eval_order_list = np.arange(1,3)\n",
    "    partitions_list = np.arange(80,100,10)\n",
    "    \n",
    "    forecasts = []\n",
    "    order_list = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=1)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (order,nparts,partitioner) = evaluate_hofts_models(\"nested_eval_hofts_oahu\", train[target_station], validation[target_station], partitioners_list, eval_order_list, partitions_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = hofts_forecast(train[target_station], test[target_station], order, partitioner, nparts)\n",
    "        \n",
    "        order_list.append(order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  2010-06-01\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 1  RMSE=0.099\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 1  RMSE=0.097\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 2  RMSE=0.105\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 2  RMSE=0.102\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 1  RMSE=0.105\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 1  RMSE=0.105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cseveriano/anaconda3/lib/python3.6/site-packages/pyFTS/common/flrg.py:52: RuntimeWarning: Mean of empty slice\n",
      "  self.midpoint = np.nanmean(self.get_midpoints(sets))\n",
      "/Users/cseveriano/anaconda3/lib/python3.6/site-packages/pyFTS/models/hofts.py:136: RuntimeWarning: Mean of empty slice\n",
      "  ret.append(np.nanmean(tmp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 2  RMSE=0.102\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 2  RMSE=0.102\n",
      "Best HOFTS((1, 90, <class 'pyFTS.partitioners.Grid.GridPartitioner'>)) RMSE=0.097\n",
      "Index:  2010-06-02\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 1  RMSE=0.100\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 1  RMSE=0.099\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 2  RMSE=0.107\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 2  RMSE=0.102\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 1  RMSE=0.105\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 1  RMSE=0.105\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 2  RMSE=0.104\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 2  RMSE=0.104\n",
      "Best HOFTS((1, 90, <class 'pyFTS.partitioners.Grid.GridPartitioner'>)) RMSE=0.099\n",
      "Index:  2010-06-03\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 1  RMSE=0.104\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 1  RMSE=0.104\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 2  RMSE=0.111\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 2  RMSE=0.108\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 1  RMSE=0.109\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 1  RMSE=0.109\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 2  RMSE=0.111\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 2  RMSE=0.111\n",
      "Best HOFTS((1, 90, <class 'pyFTS.partitioners.Grid.GridPartitioner'>)) RMSE=0.104\n",
      "Index:  2010-06-04\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 1  RMSE=0.106\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 1  RMSE=0.106\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 2  RMSE=0.115\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 2  RMSE=0.113\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 1  RMSE=0.109\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 1  RMSE=0.109\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 2  RMSE=0.119\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 2  RMSE=0.119\n",
      "Best HOFTS((1, 80, <class 'pyFTS.partitioners.Grid.GridPartitioner'>)) RMSE=0.106\n",
      "Index:  2010-06-05\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 1  RMSE=0.112\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 1  RMSE=0.112\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 2  RMSE=0.120\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 2  RMSE=0.119\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 1  RMSE=0.115\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 1  RMSE=0.115\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 2  RMSE=0.119\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 2  RMSE=0.119\n",
      "Best HOFTS((1, 80, <class 'pyFTS.partitioners.Grid.GridPartitioner'>)) RMSE=0.112\n",
      "Index:  2010-06-06\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 1  RMSE=0.118\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 1  RMSE=0.118\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 2  RMSE=0.126\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 2  RMSE=0.126\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 1  RMSE=0.120\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 1  RMSE=0.120\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 2  RMSE=0.125\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 2  RMSE=0.125\n",
      "Best HOFTS((1, 90, <class 'pyFTS.partitioners.Grid.GridPartitioner'>)) RMSE=0.118\n",
      "Index:  2010-06-07\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 1  RMSE=0.122\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 1  RMSE=0.121\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 2  RMSE=0.132\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 2  RMSE=0.130\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 1  RMSE=0.121\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 1  RMSE=0.121\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 2  RMSE=0.131\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 2  RMSE=0.131\n",
      "Best HOFTS((1, 90, <class 'pyFTS.partitioners.Grid.GridPartitioner'>)) RMSE=0.121\n",
      "Index:  2010-06-08\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 1  RMSE=0.122\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 1  RMSE=0.122\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 2  RMSE=0.134\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 2  RMSE=0.132\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 1  RMSE=0.119\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 1  RMSE=0.119\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 2  RMSE=0.132\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 2  RMSE=0.132\n",
      "Best HOFTS((1, 80, <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'>)) RMSE=0.119\n",
      "Index:  2010-06-09\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 1  RMSE=0.122\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 1  RMSE=0.121\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 80 - 2  RMSE=0.132\n",
      "HOFTS <class 'pyFTS.partitioners.Grid.GridPartitioner'> - 90 - 2  RMSE=0.130\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 1  RMSE=0.119\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 1  RMSE=0.119\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 80 - 2  RMSE=0.130\n",
      "HOFTS <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'> - 90 - 2  RMSE=0.130\n",
      "Best HOFTS((1, 80, <class 'pyFTS.partitioners.Entropy.EntropyPartitioner'>)) RMSE=0.119\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-dcda07125fc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mforecasts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrolling_cv_hofts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-ff09831fab83>\u001b[0m in \u001b[0;36mrolling_cv_hofts\u001b[0;34m(df, step)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Perform forecast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhofts_forecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_station\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_station\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0morder_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-8507176a4d1f>\u001b[0m in \u001b[0;36mhofts_forecast\u001b[0;34m(train_df, test_df, _order, _partitioner, _npartitions)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhofts_forecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_partitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_npartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfuzzy_sets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_partitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_npartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_simple_hofts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhofts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHighOrderFTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyFTS/partitioners/Entropy.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;34m\"\"\"Huarng Entropy Partitioner\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEntropyPartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Entropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyFTS/partitioners/partitioner.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_max\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mordered_sets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyFTS/partitioners/Entropy.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0msets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mpartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mpartitions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mpartitions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyFTS/partitioners/Entropy.py\u001b[0m in \u001b[0;36mbestSplit\u001b[0;34m(data, npart)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyFTS/partitioners/Entropy.py\u001b[0m in \u001b[0;36mbestSplit\u001b[0;34m(data, npart)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyFTS/partitioners/Entropy.py\u001b[0m in \u001b[0;36mbestSplit\u001b[0;34m(data, npart)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyFTS/partitioners/Entropy.py\u001b[0m in \u001b[0;36mbestSplit\u001b[0;34m(data, npart)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyFTS/partitioners/Entropy.py\u001b[0m in \u001b[0;36mbestSplit\u001b[0;34m(data, npart)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "forecasts, order_list = rolling_cv_hofts(norm_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final = get_final_forecast(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_raw_hofts\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Variance FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyFTS.models.nonstationary import cvfts\n",
    "from pyFTS.models.nonstationary import partitioners as nspartitioners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cvfts_models(test_name, train, validation, partitions_list):\n",
    "    \n",
    "    cvfts_results = pd.DataFrame(columns=['Partitions','RMSE'])\n",
    "    best_score, best_cfg, best_model = float(\"inf\"), None, None\n",
    "\n",
    "    for npartitions in partitions_list:\n",
    "                \n",
    "        fuzzy_sets =  nspartitioners.PolynomialNonStationaryPartitioner(data=train.values, part=Grid.GridPartitioner(data=train.values, npart=npartitions), degree=2)\n",
    "                \n",
    "        model_cvfts = cvfts.ConditionalVarianceFTS()\n",
    "        model_cvfts.fit(train.values, parameters=1, partitioner=fuzzy_sets, num_batches=1000)\n",
    "                                \n",
    "        forecast = model_cvfts.predict(validation.values)\n",
    "        rmse = Measures.rmse(validation.iloc[1:], forecast[:-1])\n",
    "\n",
    "        if rmse < best_score:\n",
    "            best_score, best_cfg = rmse, npartitions\n",
    "            best_model = model_cvfts\n",
    "\n",
    "        res = {'Partitions':npartitions, 'RMSE' : rmse}\n",
    "        print('CVFTS %s -  RMSE=%.3f' % (npartitions, rmse))\n",
    "        cvfts_results = cvfts_results.append(res, ignore_index=True)\n",
    "        cvfts_results.to_csv(test_name+\".csv\")\n",
    "\n",
    "    print('Best CVFTS(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    \n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvfts_forecast(train, test, _partitions):\n",
    "    \n",
    "    fuzzy_sets =  nspartitioners.PolynomialNonStationaryPartitioner(data=train.values, part=Grid.GridPartitioner(data=train.values, npart=_partitions), degree=2)\n",
    "                    \n",
    "    model_cvfts = cvfts.ConditionalVarianceFTS()\n",
    "    model_cvfts.fit(train.values, parameters=1, partitioner=fuzzy_sets, num_batches=1000)\n",
    "\n",
    "    forecast = model_cvfts.predict(test.values)\n",
    "\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_cvfts(df, step):\n",
    "    \n",
    "    partitions_list = np.arange(80,100,10)\n",
    "    \n",
    "    forecasts = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        nparts = evaluate_cvfts_models(\"nested_eval_cvfts_oahu\", train[target_station], validation[target_station], partitions_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = cvfts_forecast(train[target_station], test[target_station],nparts)\n",
    "        \n",
    "        order_list.append(1)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  2010-06-01\n",
      "CVFTS 10 -  RMSE=0.049\n",
      "CVFTS 20 -  RMSE=0.037\n",
      "CVFTS 30 -  RMSE=0.032\n",
      "CVFTS 40 -  RMSE=0.032\n",
      "CVFTS 50 -  RMSE=0.032\n",
      "CVFTS 60 -  RMSE=0.031\n",
      "CVFTS 70 -  RMSE=0.031\n",
      "CVFTS 80 -  RMSE=0.031\n",
      "CVFTS 90 -  RMSE=0.031\n",
      "Best CVFTS(90) RMSE=0.031\n",
      "Index:  2010-06-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cseveriano/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVFTS 10 -  RMSE=0.052\n",
      "CVFTS 20 -  RMSE=0.036\n",
      "CVFTS 30 -  RMSE=0.032\n",
      "CVFTS 40 -  RMSE=0.032\n",
      "CVFTS 50 -  RMSE=0.032\n",
      "CVFTS 60 -  RMSE=0.031\n",
      "CVFTS 70 -  RMSE=0.031\n",
      "CVFTS 80 -  RMSE=0.031\n",
      "CVFTS 90 -  RMSE=0.031\n",
      "Best CVFTS(90) RMSE=0.031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cseveriano/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  2010-06-03\n",
      "CVFTS 10 -  RMSE=0.051\n",
      "CVFTS 20 -  RMSE=0.037\n",
      "CVFTS 30 -  RMSE=0.032\n",
      "CVFTS 40 -  RMSE=0.032\n",
      "CVFTS 50 -  RMSE=0.031\n",
      "CVFTS 60 -  RMSE=0.031\n",
      "CVFTS 70 -  RMSE=0.031\n",
      "CVFTS 80 -  RMSE=0.031\n",
      "CVFTS 90 -  RMSE=0.031\n",
      "Best CVFTS(70) RMSE=0.031\n",
      "Index:  2010-06-04\n",
      "CVFTS 10 -  RMSE=0.049\n",
      "CVFTS 20 -  RMSE=0.035\n",
      "CVFTS 30 -  RMSE=0.032\n",
      "CVFTS 40 -  RMSE=0.032\n",
      "CVFTS 50 -  RMSE=0.031\n",
      "CVFTS 60 -  RMSE=0.031\n",
      "CVFTS 70 -  RMSE=0.031\n",
      "CVFTS 80 -  RMSE=0.031\n",
      "CVFTS 90 -  RMSE=0.031\n",
      "Best CVFTS(90) RMSE=0.031\n",
      "Index:  2010-06-05\n",
      "CVFTS 10 -  RMSE=0.050\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-bd4738b0a6e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mforecasts_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder_list_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrolling_cv_cvfts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_df_ssa_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mforecasts_residual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder_list_residual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrolling_cv_cvfts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_df_ssa_residual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-54eb9831e744>\u001b[0m in \u001b[0;36mrolling_cv_cvfts\u001b[0;34m(df, step)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Perform grid search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mnparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_cvfts_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nested_eval_cvfts_oahu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_station\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_station\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Concat train & validation for test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-ffd602a6ccd0>\u001b[0m in \u001b[0;36mevaluate_cvfts_models\u001b[0;34m(test_name, train, validation, partitions_list)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmodel_cvfts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvfts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConditionalVarianceFTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mmodel_cvfts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfuzzy_sets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mforecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cvfts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyFTS/common/fts.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, ndata, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                         \u001b[0mmdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mct\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mct\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyFTS/models/nonstationary/cvfts.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, ndata, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtmpdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuzzySeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mordered_sets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fuzzy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconst_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mflrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_non_recurrent_flrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_flrg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyFTS/common/FLR.py\u001b[0m in \u001b[0;36mgenerate_non_recurrent_flrs\u001b[0;34m(fuzzyData)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mflrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_recurrent_flrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuzzyData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mflr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflrs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyFTS/common/FLR.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLHS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" -> \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyFTS/models/nonstationary/common.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mtmp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"(\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"{0:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\") \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"(\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"{0:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\") \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\": \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "forecasts, order_list = rolling_cv_cvfts(norm_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final = get_final_forecast(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_raw_cvfts\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered Multivariate FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/cseveriano/spatio-temporal-forecasting\n",
      "  Cloning https://github.com/cseveriano/spatio-temporal-forecasting to /private/var/folders/13/t7d8w0nd0hv6w9_p2rntvym00000gr/T/pip-req-build-dtzphc34\n",
      "Building wheels for collected packages: spatio-temporal-forecasting\n",
      "  Running setup.py bdist_wheel for spatio-temporal-forecasting ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /private/var/folders/13/t7d8w0nd0hv6w9_p2rntvym00000gr/T/pip-ephem-wheel-cache-i5nysv4z/wheels/d2/1f/6f/439795864246039ef36c6a3c88edf7935c803c2cf97133066a\n",
      "Successfully built spatio-temporal-forecasting\n",
      "Installing collected packages: spatio-temporal-forecasting\n",
      "  Found existing installation: spatio-temporal-forecasting 0.1\n",
      "    Uninstalling spatio-temporal-forecasting-0.1:\n",
      "      Successfully uninstalled spatio-temporal-forecasting-0.1\n",
      "Successfully installed spatio-temporal-forecasting-0.1\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U git+https://github.com/cseveriano/spatio-temporal-forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import KMeansPartitioner\n",
    "from models import sthofts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmvfts_forecast(train_df, test_df, target, _order, npartitions):\n",
    "    \n",
    "    _partitioner = KMeansPartitioner.KMeansPartitioner(data=train_df.values, npart=npartitions, batch_size=1000, init_size=npartitions*3)\n",
    "    model_sthofts = sthofts.SpatioTemporalHighOrderFTS(membership_threshold=0.6)\n",
    "    \n",
    "    model_sthofts.fit(train_df.values, num_batches=100, order=_order, partitioner=_partitioner)\n",
    "    forecast = model_sthofts.predict(test_df.values)\n",
    "    forecast_df = pd.DataFrame(data=forecast, columns=test_df.columns)\n",
    "    return forecast_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cmvfts_models(test_name, train, validation, order_list, partitions_list):\n",
    "    \n",
    "    cmvfts_results = pd.DataFrame(columns=['Partitions','Order','RMSE'])\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "\n",
    "    for _order in order_list:\n",
    "        for npartitions in partitions_list:\n",
    "            \n",
    "            forecast = cmvfts_forecast(train, validation, target_station, _order, npartitions)\n",
    "            rmse = Measures.rmse(validation[target_station].iloc[_order:], forecast[:-1])\n",
    "\n",
    "            if rmse < best_score:\n",
    "                best_score, best_cfg = rmse, (_order,npartitions)\n",
    "\n",
    "            res = {'Partitions':npartitions, 'Order' : str(_order) ,'RMSE' : rmse}\n",
    "            print('CMVFTS %s - %s  RMSE=%.3f' % (npartitions, str(_order),rmse))\n",
    "            cmvfts_results = cmvfts_results.append(res, ignore_index=True)\n",
    "            cmvfts_results.to_csv(test_name+\".csv\")\n",
    "\n",
    "    print('Best CMVFTS(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    \n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_cmvfts(df, step):\n",
    "    \n",
    "    eval_order_list = np.arange(1,3)\n",
    "    partitions_list = np.arange(10,100,10)\n",
    "    \n",
    "    forecasts = []\n",
    "    order_list = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (order,nparts) = evaluate_cmvfts_models(\"nested_eval_cmvfts_oahu\", train[neighbor_stations_90], validation[neighbor_stations_90], eval_order_list, partitions_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = cmvfts_forecast(train[neighbor_stations_90], test[neighbor_stations_90], order, nparts)\n",
    "        \n",
    "        order_list.append(order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  2010-06-01\n",
      "CMVFTS 10 - 1  RMSE=0.046\n",
      "CMVFTS 20 - 1  RMSE=0.036\n",
      "CMVFTS 30 - 1  RMSE=0.034\n",
      "CMVFTS 40 - 1  RMSE=0.034\n",
      "CMVFTS 50 - 1  RMSE=0.034\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-649747daa833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mforecasts_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder_list_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrolling_cv_cmvfts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_df_ssa_clean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mforecasts_residual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder_list_residual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrolling_cv_cmvfts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_df_ssa_residual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-d2db3731a898>\u001b[0m in \u001b[0;36mrolling_cv_cmvfts\u001b[0;34m(df, step)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Perform grid search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_cmvfts_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nested_eval_cmvfts_oahu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneighbor_stations_90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneighbor_stations_90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_order_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Concat train & validation for test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-eb915cad7cc8>\u001b[0m in \u001b[0;36mevaluate_cmvfts_models\u001b[0;34m(test_name, train, validation, order_list, partitions_list)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnpartitions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpartitions_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mforecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmvfts_forecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_station\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeasures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_station\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecast\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-444e3fd52c71>\u001b[0m in \u001b[0;36mcmvfts_forecast\u001b[0;34m(train_df, test_df, target, _order, npartitions)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_sthofts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msthofts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpatioTemporalHighOrderFTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmembership_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel_sthofts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_partitioner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mforecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_sthofts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mforecast_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyFTS/common/fts.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, ndata, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                         \u001b[0mmdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mct\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mct\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/models/sthofts.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_flrg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/models/sthofts.py\u001b[0m in \u001b[0;36mgenerate_flrg\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mrhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuzzyfication\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mflrgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_lhs_flrg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mflrg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflrgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/models/sthofts.py\u001b[0m in \u001b[0;36mgenerate_lhs_flrg\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mlhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuzzyfication\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mlags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlhs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/models/sthofts.py\u001b[0m in \u001b[0;36mfuzzyfication\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mfuzzy_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mordered_sets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mmemberships\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmembership\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# sorting memberships\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyFTS/common/FuzzySet.py\u001b[0m in \u001b[0;36mmembership\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmembership\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mx\u001b[0m \u001b[0mat\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfuzzy\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartition_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/models/ClusterMembership.py\u001b[0m in \u001b[0;36mweighted_distance\u001b[0;34m(x, parameters)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Distance between instances and centroids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcentroid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mdists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentroid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/models/ClusterMembership.py\u001b[0m in \u001b[0;36mdistance\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_membership\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_inst_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "forecasts, order_list = rolling_cv_cmvfts(norm_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final = get_final_forecast(forecasts)\n",
    "calculate_rolling_error(\"rolling_cv_oahu_raw_cmvfts\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (  t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_multi_forecast(train_df, test_df, _order, _steps, _neurons, _epochs):\n",
    "\n",
    "    \n",
    "    nfeat = len(train_df.columns)\n",
    "    nlags = _order\n",
    "    nsteps = _steps\n",
    "    nobs = nlags * nfeat\n",
    "    \n",
    "    train_reshaped_df = series_to_supervised(train_df, n_in=nlags, n_out=nsteps)\n",
    "    train_X, train_Y = train_reshaped_df.iloc[:,:nobs].values, train_reshaped_df.iloc[:,-nfeat].values\n",
    "    train_X = train_X.reshape((train_X.shape[0], nlags, nfeat))\n",
    "\n",
    "    test_reshaped_df = series_to_supervised(test_df, n_in=nlags, n_out=nsteps)\n",
    "    test_X, test_Y = test_reshaped_df.iloc[:,:nobs].values, test_reshaped_df.iloc[:,-nfeat].values\n",
    "    test_X = test_X.reshape((test_X.shape[0], nlags, nfeat))\n",
    "    \n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(_neurons, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "    # fit network\n",
    "    model.fit(train_X, train_Y, epochs=_epochs, batch_size=1000, verbose=False, shuffle=False)\n",
    "    \n",
    "    forecast = model.predict(test_X)\n",
    "    \n",
    "    fcst = [f[0] for f in forecast]\n",
    "\n",
    "    return fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multivariate_lstm_models(test_name, train_df, validation_df, neurons_list, order_list, epochs_list):\n",
    "    \n",
    "    lstm_results = pd.DataFrame(columns=['Neurons','Order','Epochs','RMSE'])\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    \n",
    "    nfeat = len(train_df.columns)\n",
    "    nsteps = 1\n",
    "    \n",
    "    for _neurons in neurons_list:\n",
    "        for _order in order_list:\n",
    "            for epochs in epochs_list:\n",
    "                    \n",
    "                    nobs = nfeat * _order\n",
    "                    \n",
    "                    train_reshaped_df = series_to_supervised(train_df, n_in=_order, n_out=nsteps)\n",
    "                    train_X, train_Y = train_reshaped_df.iloc[:,:nobs].values, train_reshaped_df.iloc[:,-nfeat].values\n",
    "                    train_X = train_X.reshape((train_X.shape[0], _order, nfeat))                    \n",
    "                    \n",
    "                    val_reshaped_df = series_to_supervised(validation_df, n_in=_order, n_out=nsteps)\n",
    "                    validation_X, validation_Y = val_reshaped_df.iloc[:,:nobs].values, val_reshaped_df.iloc[:,-nfeat].values\n",
    "                    validation_X = validation_X.reshape((validation_X.shape[0], _order, nfeat))\n",
    "                    \n",
    "                    # design network\n",
    "                    model = Sequential()\n",
    "                    model.add(LSTM(_neurons, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "                    model.add(Dense(1))\n",
    "                    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "                    # fit network\n",
    "                    history = model.fit(train_X, train_Y, epochs=epochs, batch_size=1000, verbose=False, shuffle=False)\n",
    "                    forecast = model.predict(validation_X)\n",
    "                    fcst = [f[0] for f in forecast]\n",
    "                    \n",
    "                    \n",
    "                    rmse = Measures.rmse(validation_Y, fcst)\n",
    "                    #rmse = math.sqrt(mean_squared_error(validation_Y, forecast))\n",
    "                    \n",
    "                    params = (_neurons, _order,epochs)\n",
    "                    if rmse < best_score:\n",
    "                        best_score, best_cfg = rmse, params\n",
    "\n",
    "                    res = {'Neurons':_neurons, 'Order':_order, 'Epochs' : epochs ,'RMSE' : rmse}\n",
    "                    print('LSTM %s  RMSE=%.3f' % (params,rmse))\n",
    "                    lstm_results = lstm_results.append(res, ignore_index=True)\n",
    "                    lstm_results.to_csv(test_name+\".csv\")\n",
    "\n",
    "    print('Best LSTM(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_lstm_multi(df, step):\n",
    "    \n",
    "    neurons_list = np.arange(50,110,50)\n",
    "    order_list = np.arange(2,4)\n",
    "    epochs_list = [100]\n",
    "\n",
    "    neurons_list = [5]\n",
    "    order_list = [2]\n",
    "    epochs_list = [1]\n",
    "\n",
    "    lags_list = []\n",
    "    forecasts = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (_neurons, _order,epochs) = evaluate_multivariate_lstm_models(\"nested_eval_lstm_multi_oahu\", train[neighbor_stations_90], validation[neighbor_stations_90], neurons_list, order_list, epochs_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = lstm_multi_forecast(train[neighbor_stations_90], test[neighbor_stations_90], _order, 1, _neurons,epochs)\n",
    "        \n",
    "        yhat.append(0) #para manter o formato do vetor de metricas\n",
    "        \n",
    "        lags_list.append(_order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  2010-06-01\n",
      "LSTM (5, 2, 1)  RMSE=0.424\n",
      "Best LSTM((5, 2, 1)) RMSE=0.424\n",
      "Index:  2010-06-08\n",
      "LSTM (5, 2, 1)  RMSE=0.411\n",
      "Best LSTM((5, 2, 1)) RMSE=0.411\n",
      "Index:  2010-06-15\n",
      "LSTM (5, 2, 1)  RMSE=0.276\n",
      "Best LSTM((5, 2, 1)) RMSE=0.276\n",
      "Index:  2010-06-22\n",
      "LSTM (5, 2, 1)  RMSE=0.311\n",
      "Best LSTM((5, 2, 1)) RMSE=0.311\n",
      "Index:  2010-06-01\n",
      "LSTM (5, 2, 1)  RMSE=0.476\n",
      "Best LSTM((5, 2, 1)) RMSE=0.476\n",
      "Index:  2010-06-08\n",
      "LSTM (5, 2, 1)  RMSE=0.584\n",
      "Best LSTM((5, 2, 1)) RMSE=0.584\n",
      "Index:  2010-06-15\n",
      "LSTM (5, 2, 1)  RMSE=0.464\n",
      "Best LSTM((5, 2, 1)) RMSE=0.464\n",
      "Index:  2010-06-22\n",
      "LSTM (5, 2, 1)  RMSE=0.437\n",
      "Best LSTM((5, 2, 1)) RMSE=0.437\n"
     ]
    }
   ],
   "source": [
    "forecasts, order_list = rolling_cv_lstm_multi(norm_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final = get_final_forecast(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_raw_lstm_multi\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_lstm_uni(df, step):\n",
    "    \n",
    "    neurons_list = np.arange(50,110,50)\n",
    "    order_list = np.arange(2,4)\n",
    "    epochs_list = [100]\n",
    "\n",
    "    neurons_list = [5]\n",
    "    order_list = [2]\n",
    "    epochs_list = [1]\n",
    "\n",
    "    lags_list = []\n",
    "    forecasts = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (_neurons, _order,epochs) = evaluate_multivariate_lstm_models(\"nested_eval_lstm_multi_oahu\", train[[target_station]], validation[[target_station]], neurons_list, order_list, epochs_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = lstm_multi_forecast(train[[target_station]], test[[target_station]], _order, 1, _neurons,epochs)\n",
    "        \n",
    "        yhat.append(0) #para manter o formato do vetor de metricas\n",
    "        \n",
    "        lags_list.append(_order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  2010-06-01\n",
      "LSTM (5, 2, 1)  RMSE=0.646\n",
      "Best LSTM((5, 2, 1)) RMSE=0.646\n",
      "Index:  2010-06-08\n",
      "LSTM (5, 2, 1)  RMSE=0.459\n",
      "Best LSTM((5, 2, 1)) RMSE=0.459\n",
      "Index:  2010-06-15\n",
      "LSTM (5, 2, 1)  RMSE=0.412\n",
      "Best LSTM((5, 2, 1)) RMSE=0.412\n",
      "Index:  2010-06-22\n",
      "LSTM (5, 2, 1)  RMSE=0.581\n",
      "Best LSTM((5, 2, 1)) RMSE=0.581\n",
      "Index:  2010-06-01\n",
      "LSTM (5, 2, 1)  RMSE=0.412\n",
      "Best LSTM((5, 2, 1)) RMSE=0.412\n",
      "Index:  2010-06-08\n",
      "LSTM (5, 2, 1)  RMSE=0.568\n",
      "Best LSTM((5, 2, 1)) RMSE=0.568\n",
      "Index:  2010-06-15\n",
      "LSTM (5, 2, 1)  RMSE=0.423\n",
      "Best LSTM((5, 2, 1)) RMSE=0.423\n",
      "Index:  2010-06-22\n",
      "LSTM (5, 2, 1)  RMSE=0.521\n",
      "Best LSTM((5, 2, 1)) RMSE=0.521\n"
     ]
    }
   ],
   "source": [
    "forecasts, order_list = rolling_cv_lstm_uni(norm_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final = get_final_forecast(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_raw_lstm_uni\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_multi_forecast(train_df, test_df, _order, _steps, _neurons, _epochs):\n",
    "\n",
    "    \n",
    "    nfeat = len(train_df.columns)\n",
    "    nlags = _order\n",
    "    nsteps = _steps\n",
    "    nobs = nlags * nfeat\n",
    "    \n",
    "    train_reshaped_df = series_to_supervised(train_df, n_in=nlags, n_out=nsteps)\n",
    "    train_X, train_Y = train_reshaped_df.iloc[:,:nobs].values, train_reshaped_df.iloc[:,-nfeat].values\n",
    "    \n",
    "    test_reshaped_df = series_to_supervised(test_df, n_in=nlags, n_out=nsteps)\n",
    "    test_X, test_Y = test_reshaped_df.iloc[:,:nobs].values, test_reshaped_df.iloc[:,-nfeat].values\n",
    "    \n",
    "    # design network\n",
    "    model = designMLPNetwork(_neurons,train_X.shape[1])\n",
    "    \n",
    "    # fit network\n",
    "    model.fit(train_X, train_Y, epochs=_epochs, batch_size=1000, verbose=False, shuffle=False)\n",
    "    \n",
    "    forecast = model.predict(test_X)\n",
    "    \n",
    "    fcst = [f[0] for f in forecast]\n",
    "\n",
    "    return fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multivariate_mlp_models(test_name, train_df, validation_df, neurons_list, order_list, epochs_list):\n",
    "    \n",
    "    lstm_results = pd.DataFrame(columns=['Neurons','Order','Epochs','RMSE'])\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    \n",
    "    nfeat = len(train_df.columns)\n",
    "    nsteps = 1\n",
    "    \n",
    "    for _neurons in neurons_list:\n",
    "        for _order in order_list:\n",
    "            for epochs in epochs_list:\n",
    "                    \n",
    "                    nobs = nfeat * _order\n",
    "                    \n",
    "                    train_reshaped_df = series_to_supervised(train_df, n_in=_order, n_out=nsteps)\n",
    "                    train_X, train_Y = train_reshaped_df.iloc[:,:nobs].values, train_reshaped_df.iloc[:,-nfeat].values\n",
    "                    \n",
    "                    val_reshaped_df = series_to_supervised(validation_df, n_in=_order, n_out=nsteps)\n",
    "                    validation_X, validation_Y = val_reshaped_df.iloc[:,:nobs].values, val_reshaped_df.iloc[:,-nfeat].values\n",
    "                   \n",
    "                    model = designMLPNetwork(_neurons,train_X.shape[1])\n",
    "                                        \n",
    "                    # fit network\n",
    "                    history = model.fit(train_X, train_Y, epochs=epochs, batch_size=1000, verbose=False, shuffle=False)\n",
    "                    forecast = model.predict(validation_X)\n",
    "                    fcst = [f[0] for f in forecast]\n",
    "                    \n",
    "                    \n",
    "                    rmse = Measures.rmse(validation_Y, fcst)\n",
    "                    #rmse = math.sqrt(mean_squared_error(validation_Y, forecast))\n",
    "                    \n",
    "                    params = (_neurons, _order,epochs)\n",
    "                    if rmse < best_score:\n",
    "                        best_score, best_cfg = rmse, params\n",
    "\n",
    "                    res = {'Neurons':_neurons, 'Order':_order, 'Epochs' : epochs ,'RMSE' : rmse}\n",
    "                    print('LSTM %s  RMSE=%.3f' % (params,rmse))\n",
    "                    lstm_results = lstm_results.append(res, ignore_index=True)\n",
    "                    lstm_results.to_csv(test_name+\".csv\")\n",
    "\n",
    "    print('Best MLP(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def designMLPNetwork(neurons, shape):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_dim=shape))\n",
    "    model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_mlp_multi(df, step):\n",
    "    \n",
    "    neurons_list = np.arange(50,110,50)\n",
    "    order_list = np.arange(2,4)\n",
    "    epochs_list = [100]\n",
    "\n",
    "    neurons_list = [50]\n",
    "    order_list = [4]\n",
    "    epochs_list = [500]\n",
    "\n",
    "    lags_list = []\n",
    "    forecasts = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (_neurons, _order,epochs) = evaluate_multivariate_mlp_models(\"nested_eval_mlp_multi_oahu\", train[neighbor_stations_90], validation[neighbor_stations_90], neurons_list, order_list, epochs_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = mlp_multi_forecast(train[neighbor_stations_90], test[neighbor_stations_90], _order, 1, _neurons,epochs)\n",
    "        \n",
    "        yhat.append(0) #para manter o formato do vetor de metricas\n",
    "        \n",
    "        lags_list.append(_order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  2010-06-01\n",
      "LSTM (50, 4, 500)  RMSE=0.009\n",
      "Best MLP((50, 4, 500)) RMSE=0.009\n",
      "Index:  2010-06-08\n",
      "LSTM (50, 4, 500)  RMSE=0.008\n",
      "Best MLP((50, 4, 500)) RMSE=0.008\n",
      "Index:  2010-06-15\n",
      "LSTM (50, 4, 500)  RMSE=0.011\n",
      "Best MLP((50, 4, 500)) RMSE=0.011\n",
      "Index:  2010-06-22\n",
      "LSTM (50, 4, 500)  RMSE=0.007\n",
      "Best MLP((50, 4, 500)) RMSE=0.007\n",
      "Index:  2010-06-01\n",
      "LSTM (50, 4, 500)  RMSE=0.110\n",
      "Best MLP((50, 4, 500)) RMSE=0.110\n",
      "Index:  2010-06-08\n",
      "LSTM (50, 4, 500)  RMSE=0.137\n",
      "Best MLP((50, 4, 500)) RMSE=0.137\n",
      "Index:  2010-06-15\n",
      "LSTM (50, 4, 500)  RMSE=0.129\n",
      "Best MLP((50, 4, 500)) RMSE=0.129\n",
      "Index:  2010-06-22\n",
      "LSTM (50, 4, 500)  RMSE=0.115\n",
      "Best MLP((50, 4, 500)) RMSE=0.115\n"
     ]
    }
   ],
   "source": [
    "forecasts, order_list = rolling_cv_mlp_multi(norm_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final = get_final_forecast(forecasts)\n",
    "calculate_rolling_error(\"rolling_cv_oahu_raw_mlp_multi\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_mlp_uni(df, step):\n",
    "    \n",
    "    neurons_list = [50]\n",
    "    order_list = [4]\n",
    "    epochs_list = [500]\n",
    "\n",
    "    lags_list = []\n",
    "    forecasts = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (_neurons, _order,epochs) = evaluate_multivariate_mlp_models(\"nested_eval_mlp_uni_oahu\", train[[target_station]], validation[[target_station]], neurons_list, order_list, epochs_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = mlp_multi_forecast(train[[target_station]], test[[target_station]], _order, 1, _neurons,epochs)\n",
    "        \n",
    "        yhat.append(0) #para manter o formato do vetor de metricas\n",
    "        \n",
    "        lags_list.append(_order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  2010-06-01\n",
      "LSTM (50, 4, 500)  RMSE=0.005\n",
      "Best MLP((50, 4, 500)) RMSE=0.005\n",
      "Index:  2010-06-08\n",
      "LSTM (50, 4, 500)  RMSE=0.010\n",
      "Best MLP((50, 4, 500)) RMSE=0.010\n",
      "Index:  2010-06-15\n",
      "LSTM (50, 4, 500)  RMSE=0.006\n",
      "Best MLP((50, 4, 500)) RMSE=0.006\n",
      "Index:  2010-06-22\n",
      "LSTM (50, 4, 500)  RMSE=0.006\n",
      "Best MLP((50, 4, 500)) RMSE=0.006\n",
      "Index:  2010-06-01\n",
      "LSTM (50, 4, 500)  RMSE=0.103\n",
      "Best MLP((50, 4, 500)) RMSE=0.103\n",
      "Index:  2010-06-08\n",
      "LSTM (50, 4, 500)  RMSE=0.128\n",
      "Best MLP((50, 4, 500)) RMSE=0.128\n",
      "Index:  2010-06-15\n",
      "LSTM (50, 4, 500)  RMSE=0.131\n",
      "Best MLP((50, 4, 500)) RMSE=0.131\n",
      "Index:  2010-06-22\n",
      "LSTM (50, 4, 500)  RMSE=0.110\n",
      "Best MLP((50, 4, 500)) RMSE=0.110\n"
     ]
    }
   ],
   "source": [
    "forecasts, order_list = rolling_cv_mlp_uni(norm_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final = get_final_forecast(forecasts)\n",
    "calculate_rolling_error(\"rolling_cv_oahu_raw_mlp_uni\", df, forecasts_final, order_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
