{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    mindf = df.min()\n",
    "    maxdf = df.max()\n",
    "    return (df-mindf)/(maxdf-mindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(norm, _min, _max):\n",
    "    return [(n * (_max-_min)) + _min for n in norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, df_clean, df_residual, interval):\n",
    "    sample_df = df.loc[interval]\n",
    "    residual_sample_df = df_residual.loc[interval]\n",
    "    clean_sample_df = df_clean.loc[interval]\n",
    "\n",
    "    week = (sample_df.index.day - 1) // 7 + 1\n",
    "\n",
    "    # PARA OS TESTES:\n",
    "    # 2 SEMANAS PARA TREINAMENTO\n",
    "    train_df = sample_df.loc[week <= 2]\n",
    "    train_residual_df = residual_sample_df.loc[week <= 2]\n",
    "    train_clean_df = clean_sample_df.loc[week <= 2]\n",
    "\n",
    "    # 1 SEMANA PARA VALIDACAO\n",
    "    validation_df = sample_df.loc[week == 3]\n",
    "    validation_residual_df = residual_sample_df.loc[week == 3]\n",
    "    validation_clean_df = clean_sample_df.loc[week == 3]\n",
    "\n",
    "    # 1 SEMANA PARA TESTES\n",
    "    test_df = sample_df.loc[week > 3]\n",
    "    test_residual_df = residual_sample_df.loc[week > 3]\n",
    "    test_clean_df = clean_sample_df.loc[week > 3]\n",
    "    \n",
    "    return (train_df, train_clean_df, train_residual_df, validation_df, validation_clean_df, validation_residual_df, test_df, test_clean_df, test_residual_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(test, forecast, order, step):\n",
    "    rmse = math.sqrt(mean_squared_error(test.iloc[(order):], forecast[:-step]))\n",
    "    print(\"RMSE : \"+str(rmse))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_ssa_series(clean, residual):\n",
    "    return [r + c for r, c in zip(residual,clean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('results/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('results/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(raw_df, interval=1):\n",
    "    df_diff = pd.DataFrame(columns=raw_df.columns, index=raw_df.index[1:])\n",
    "    \n",
    "    for col in raw_df.columns:\n",
    "        raw_array = raw_df[col]\n",
    "        diff = []\n",
    "        for i in range(interval, len(raw_array)):\n",
    "            value = raw_array[i] - raw_array[i - interval]\n",
    "            diff.append(value)\n",
    "        \n",
    "        df_diff[col] = diff\n",
    "    return df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_difference(raw_series, diff_series):\n",
    "    inverted = []\n",
    "    for i in range(len(diff_series)):\n",
    "        interval = len(raw_series)-i\n",
    "        value = diff_series[i] + raw_series[-interval]\n",
    "        inverted.append(value)\n",
    "        \n",
    "    return inverted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "Split the data into train, validation and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set target and input variables \n",
    "target_station = 'WTG01'\n",
    "\n",
    "#All neighbor stations with residual correlation greater than .90\n",
    "neighbor_stations_90 = ['WTG01','WTG02','WTG03','WTG05','WTG06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"df_wind_speed.pkl\")\n",
    "df_ssa_clean = pd.read_pickle(\"df_wind_speed_ssa_clean.pkl\")\n",
    "df_ssa_residual = pd.read_pickle(\"df_wind_speed_ssa_residual.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize Data\n",
    "\n",
    "# Save Min-Max for Denorm\n",
    "min_raw = df[target_station].min()\n",
    "min_clean = df_ssa_clean[target_station].min()\n",
    "min_residual = df_ssa_residual[target_station].min()\n",
    "\n",
    "max_raw = df[target_station].max()\n",
    "max_clean = df_ssa_clean[target_station].max()\n",
    "max_residual = df_ssa_residual[target_station].max()\n",
    "\n",
    "# Perform Normalization\n",
    "norm_df_ssa_clean = normalize(df_ssa_clean)\n",
    "norm_df_ssa_residual = normalize(df_ssa_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "interval = ((df.index >= '2017-05') & (df.index <= '2018-05'))\n",
    "#interval = ((df.index >= '2010-11') & (df.index <= '2010-12'))\n",
    "\n",
    "(train_df, train_clean_df, train_residual_df, \n",
    " validation_df, validation_clean_df, validation_residual_df, \n",
    " test_df, test_clean_df, test_residual_df) = split_data(df, norm_df_ssa_clean, norm_df_ssa_residual, interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['2017-06-01'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting with SSA Decomposition\n",
    "\n",
    "For each dataset, all the time series were decomposed in 2 components (trend plus harmonic and residual) and the 2 resulting datasets were used for different configurations of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistence_forecast(train, test, step):\n",
    "    predictions = []\n",
    "    \n",
    "    for t in np.arange(0,len(test), step):\n",
    "        yhat = [test.iloc[t]]  * step\n",
    "        predictions.extend(yhat)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "persistence_order = 1\n",
    "\n",
    "forecast_clean = persistence_forecast(train_clean_df[target_station], test_clean_df[target_station],step)\n",
    "forecast_clean = denormalize(forecast_clean, min_clean, max_clean)\n",
    "\n",
    "forecast_residual = persistence_forecast(train_residual_df[target_station], test_residual_df[target_station],step)\n",
    "forecast_residual = denormalize(forecast_residual, min_residual, max_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 0.6324497271451677\n"
     ]
    }
   ],
   "source": [
    "final_forecast = reconstruct_ssa_series(forecast_clean, forecast_residual)\n",
    "rmse = calculate_rmse(test_df[target_station], final_forecast, persistence_order, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'rmse': rmse, 'final': final_forecast, 'clean': forecast_clean, 'residual': forecast_residual}\n",
    "save_obj(result, name=\"wind_persistence_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cseveriano/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarima_forecast(train, test, arima_order, sarima_order, step):\n",
    "\n",
    "    predictions = []\n",
    "    window_size = sarima_order[3] * 5\n",
    "    \n",
    "    for date in train.index.to_period('M').unique():\n",
    "        \n",
    "        history = list(train[str(date)].iloc[-window_size:])\n",
    "        \n",
    "        model = SARIMAX(history, order=arima_order, seasonal_order=sarima_order,enforce_invertibility=False,enforce_stationarity=False)\n",
    "        model_fit = model.fit(disp=True,enforce_invertibility=False,  method='powell', maxiter=200)\n",
    "        \n",
    "        #save the state parameter\n",
    "        est_params = model_fit.params\n",
    "        est_state = model_fit.predicted_state[:, -1]\n",
    "        est_state_cov = model_fit.predicted_state_cov[:, :, -1]\n",
    "\n",
    "        print(\"Predicting : \"+str(date))\n",
    "        \n",
    "        st = 0\n",
    "        test_date = test[str(date)]\n",
    "        \n",
    "        for t in np.arange(1,len(test_date)+1,step):\n",
    "            obs = test_date.iloc[st:t].values\n",
    "            history.extend(obs)\n",
    "            history = history[-window_size:]\n",
    "            \n",
    "            mod_updated = SARIMAX(history, order=arima_order, seasonal_order=sarima_order,enforce_invertibility=False,enforce_stationarity=False)\n",
    "            mod_updated.initialize_known(est_state, est_state_cov)\n",
    "            mod_frcst = mod_updated.smooth(est_params)\n",
    "\n",
    "        \n",
    "            yhat = mod_frcst.forecast(step)   \n",
    "            predictions.extend(yhat)\n",
    "            \n",
    "            est_params = mod_frcst.params\n",
    "            est_state = mod_frcst.predicted_state[:, -1]\n",
    "            est_state_cov = mod_frcst.predicted_state_cov[:, :, -1]\n",
    "            \n",
    "            st = t\n",
    "                \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarima_forecast(train, test, arima_order, sarima_order, step):\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    for date in train.index.to_period('M').unique():\n",
    "        print(\"Predicting : \"+str(date))\n",
    "        history = list(train[str(date)])\n",
    "        test_steps = len(test[str(date)])\n",
    "        print(\"Number of steps : \"+str(test_steps))\n",
    "        \n",
    "        model = SARIMAX(history, order=arima_order, seasonal_order=sarima_order,enforce_invertibility=False,enforce_stationarity=False)\n",
    "        model_fit = model.fit(disp=True,enforce_invertibility=False, method='powell', maxiter=200)\n",
    "        yhat = model_fit.forecast(test_steps)      \n",
    "        \n",
    "        predictions.extend(yhat)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting : 2010-06\n",
      "Number of steps : 549\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -3.657188\n",
      "         Iterations: 3\n",
      "         Function evaluations: 331\n",
      "Predicting : 2010-07\n",
      "Number of steps : 610\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -3.466340\n",
      "         Iterations: 5\n",
      "         Function evaluations: 539\n",
      "Predicting : 2010-08\n",
      "Number of steps : 610\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -3.581201\n",
      "         Iterations: 4\n",
      "         Function evaluations: 464\n",
      "Predicting : 2010-09\n",
      "Number of steps : 549\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -3.512425\n",
      "         Iterations: 5\n",
      "         Function evaluations: 553\n",
      "Predicting : 2010-10\n",
      "Number of steps : 610\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -3.685367\n",
      "         Iterations: 3\n",
      "         Function evaluations: 323\n",
      "Predicting : 2010-11\n",
      "Number of steps : 549\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -3.839113\n",
      "         Iterations: 4\n",
      "         Function evaluations: 486\n",
      "Predicting : 2010-12\n",
      "Number of steps : 610\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -4.234858\n",
      "         Iterations: 4\n",
      "         Function evaluations: 462\n",
      "Predicting : 2011-01\n",
      "Number of steps : 610\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -4.033419\n",
      "         Iterations: 4\n",
      "         Function evaluations: 459\n",
      "Predicting : 2011-02\n",
      "Number of steps : 427\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -3.678135\n",
      "         Iterations: 5\n",
      "         Function evaluations: 559\n",
      "Predicting : 2011-03\n",
      "Number of steps : 610\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -3.702938\n",
      "         Iterations: 5\n",
      "         Function evaluations: 555\n",
      "Predicting : 2011-04\n",
      "Number of steps : 549\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -3.667302\n",
      "         Iterations: 4\n",
      "         Function evaluations: 452\n",
      "Predicting : 2011-05\n",
      "Number of steps : 610\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -3.683944\n",
      "         Iterations: 5\n",
      "         Function evaluations: 528\n"
     ]
    }
   ],
   "source": [
    "#Clean - SARIMA(2, 1, 2, 1, 1, 1)\n",
    "#Residual - SARIMA(2, 0, 1, 1, 1, 1)\n",
    "order = 1\n",
    "step = 1\n",
    "arima_order_clean = (2, 1, 2)\n",
    "sarima_order_clean = (1, 1, 1, 61)\n",
    "forecast_clean = sarima_forecast(train_clean_df[target_station], test_clean_df[target_station], arima_order_clean, sarima_order_clean, step)\n",
    "forecast_clean = denormalize(forecast_clean, min_clean, max_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting : 2010-06\n",
      "Number of steps : 549\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -1.139909\n",
      "         Iterations: 4\n",
      "         Function evaluations: 303\n",
      "Predicting : 2010-07\n",
      "Number of steps : 610\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.954949\n",
      "         Iterations: 6\n",
      "         Function evaluations: 470\n",
      "Predicting : 2010-08\n",
      "Number of steps : 610\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -1.135208\n",
      "         Iterations: 5\n",
      "         Function evaluations: 369\n",
      "Predicting : 2010-09\n",
      "Number of steps : 549\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -1.059952\n",
      "         Iterations: 4\n",
      "         Function evaluations: 298\n",
      "Predicting : 2010-10\n",
      "Number of steps : 610\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -1.192249\n",
      "         Iterations: 4\n",
      "         Function evaluations: 346\n",
      "Predicting : 2010-11\n",
      "Number of steps : 549\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -1.399083\n",
      "         Iterations: 5\n",
      "         Function evaluations: 438\n",
      "Predicting : 2010-12\n",
      "Number of steps : 610\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -1.833879\n",
      "         Iterations: 4\n",
      "         Function evaluations: 314\n",
      "Predicting : 2011-01\n",
      "Number of steps : 610\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -1.572810\n",
      "         Iterations: 4\n",
      "         Function evaluations: 314\n",
      "Predicting : 2011-02\n",
      "Number of steps : 427\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -1.389136\n",
      "         Iterations: 4\n",
      "         Function evaluations: 311\n",
      "Predicting : 2011-03\n",
      "Number of steps : 610\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -1.235220\n",
      "         Iterations: 5\n",
      "         Function evaluations: 397\n",
      "Predicting : 2011-04\n",
      "Number of steps : 549\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -1.158881\n",
      "         Iterations: 6\n",
      "         Function evaluations: 464\n",
      "Predicting : 2011-05\n",
      "Number of steps : 610\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -1.258736\n",
      "         Iterations: 4\n",
      "         Function evaluations: 312\n"
     ]
    }
   ],
   "source": [
    "arima_order_residual = (2, 0, 1)\n",
    "sarima_order_residual = (1, 1, 1, 61)\n",
    "forecast_residual = sarima_forecast(train_residual_df[target_station], test_residual_df[target_station], arima_order_residual, sarima_order_residual,step)\n",
    "forecast_residual = denormalize(forecast_residual, min_residual, max_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 178.31337062654006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "178.31337062654006"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forecast = reconstruct_ssa_series(forecast_clean, forecast_residual)\n",
    "rmse = calculate_rmse(test_df[target_station], final_forecast, order, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 178.31337062654006\n"
     ]
    }
   ],
   "source": [
    "result = {'rmse': rmse, 'final': final_forecast, 'clean': forecast_clean, 'residual': forecast_residual}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(result, name=\"wind_sarima_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Autoregressive - VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR, DynamicVAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_forecast(train, test, target, order, step):\n",
    "    model = VAR(train.values)\n",
    "    results = model.fit(maxlags=order)\n",
    "    lag_order = results.k_ar\n",
    "    print(\"Lag order:\" + str(lag_order))\n",
    "    forecast = []\n",
    "\n",
    "    for i in np.arange(0,len(test)-lag_order+1,step) :\n",
    "        forecast.extend(results.forecast(test.values[i:i+lag_order],step))\n",
    "\n",
    "    forecast_df = pd.DataFrame(columns=test.columns, data=forecast)\n",
    "    return forecast_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag order:4\n"
     ]
    }
   ],
   "source": [
    "# Clean = VAR(2)\n",
    "# Residual = VAR(4)\n",
    "\n",
    "var_order = 4\n",
    "step = 1\n",
    "\n",
    "forecast_clean = var_forecast(train_clean_df[neighbor_stations_90], test_clean_df[neighbor_stations_90], target_station, var_order, step)\n",
    "forecast_clean = denormalize(forecast_clean, min_clean, max_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag order:4\n"
     ]
    }
   ],
   "source": [
    "forecast_residual = var_forecast(train_residual_df[neighbor_stations_90], test_residual_df[neighbor_stations_90], target_station, var_order, step)\n",
    "forecast_residual = denormalize(forecast_residual, min_residual, max_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 0.42108261396600716\n"
     ]
    }
   ],
   "source": [
    "final_forecast = reconstruct_ssa_series(forecast_clean, forecast_residual)\n",
    "rmse = calculate_rmse(test_df[target_station], final_forecast, var_order, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'rmse': rmse, 'final': final_forecast, 'clean': forecast_clean, 'residual': forecast_residual}\n",
    "save_obj(result, name=\"wind_var_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_multi_forecast(train_df, test_df, _order, _steps, _neurons, _epochs):\n",
    "\n",
    "    \n",
    "    nfeat = len(train_df.columns)\n",
    "    nlags = _order\n",
    "    nsteps = _steps\n",
    "    nobs = nlags * nfeat\n",
    "    \n",
    "    train_reshaped_df = series_to_supervised(train_df, n_in=nlags, n_out=nsteps)\n",
    "    train_X, train_Y = train_reshaped_df.iloc[:,:nobs].values, train_reshaped_df.iloc[:,-nfeat].values\n",
    "    train_X = train_X.reshape((train_X.shape[0], nlags, nfeat))\n",
    "    \n",
    "    test_reshaped_df = series_to_supervised(test_df, n_in=nlags, n_out=nsteps)\n",
    "    test_X, test_Y = test_reshaped_df.iloc[:,:nobs].values, test_reshaped_df.iloc[:,-nfeat].values\n",
    "    test_X = test_X.reshape((test_X.shape[0], nlags, nfeat))\n",
    "    \n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(_neurons, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    \n",
    "    # fit network\n",
    "    model.fit(train_X, train_Y, epochs=_epochs, batch_size=72, verbose=False, shuffle=False)\n",
    "    \n",
    "    forecast = model.predict(test_X)\n",
    "        \n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 100\n",
    "lstm_order = 4\n",
    "epochs = 100\n",
    "steps = 1\n",
    "\n",
    "forecast_clean = lstm_multi_forecast(train_clean_df[neighbor_stations_90], test_clean_df[neighbor_stations_90], lstm_order, steps, neurons, epochs)\n",
    "forecast_clean = denormalize(forecast_clean, min_clean, max_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_residual = lstm_multi_forecast(train_residual_df[neighbor_stations_90], test_residual_df[neighbor_stations_90], lstm_order, steps, neurons, epochs)\n",
    "forecast_residual = denormalize(forecast_residual, min_residual, max_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_forecast = reconstruct_ssa_series(forecast_clean, forecast_residual)\n",
    "final_forecast.append(0) ## para manter o mesmo tamanho dos demais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 0.4330875880143308\n"
     ]
    }
   ],
   "source": [
    "rmse = calculate_rmse(test_df[target_station], final_forecast, lstm_order, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'rmse': rmse, 'final': final_forecast, 'clean': forecast_clean, 'residual': forecast_residual}\n",
    "save_obj(result, name=\"wind_lstm_multi_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 0.4381514919938163\n"
     ]
    }
   ],
   "source": [
    "neurons = 50\n",
    "lstm_order = 4\n",
    "epochs = 100\n",
    "steps = 1\n",
    "\n",
    "forecast_clean = lstm_multi_forecast(train_clean_df[[target_station]], test_clean_df[[target_station]], lstm_order, steps, neurons, epochs)\n",
    "forecast_clean = denormalize(forecast_clean, min_clean, max_clean)\n",
    "\n",
    "forecast_residual = lstm_multi_forecast(train_residual_df[[target_station]], test_residual_df[[target_station]], lstm_order, steps, neurons, epochs)\n",
    "forecast_residual = denormalize(forecast_residual, min_residual, max_residual)\n",
    "\n",
    "final_forecast = reconstruct_ssa_series(forecast_clean, forecast_residual)\n",
    "final_forecast.append(0) ## para manter o mesmo tamanho dos demais\n",
    "\n",
    "rmse = calculate_rmse(test_df[target_station], final_forecast, lstm_order, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'rmse': rmse, 'final': final_forecast, 'clean': forecast_clean, 'residual': forecast_residual}\n",
    "save_obj(result, name=\"wind_lstm_uni_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_forecast(train_df, test_df, _order, _steps, _neurons, _epochs):\n",
    "\n",
    "    \n",
    "    nfeat = len(train_df.columns)\n",
    "    nlags = _order\n",
    "    nsteps = _steps\n",
    "    nobs = nlags * nfeat\n",
    "    \n",
    "    train_reshaped_df = series_to_supervised(train_df, n_in=nlags, n_out=nsteps)\n",
    "    train_X, train_Y = train_reshaped_df.iloc[:,:nobs].values, train_reshaped_df.iloc[:,-nfeat].values\n",
    "    \n",
    "    test_reshaped_df = series_to_supervised(test_df, n_in=nlags, n_out=nsteps)\n",
    "    test_X, test_Y = test_reshaped_df.iloc[:,:nobs].values, test_reshaped_df.iloc[:,-nfeat].values\n",
    "    \n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_dim=train_X.shape[1]))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit(train_X, train_Y, epochs=_epochs, batch_size=72, verbose=False, shuffle=False)   \n",
    "\n",
    "    forecast = model.predict(test_X)\n",
    "        \n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 90\n",
    "mlp_order = 3\n",
    "epochs = 100\n",
    "steps = 1\n",
    "\n",
    "forecast_clean = mlp_forecast(train_clean_df[neighbor_stations_90], test_clean_df[neighbor_stations_90], mlp_order, steps, neurons, epochs)\n",
    "forecast_clean = denormalize(forecast_clean, min_clean, max_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_residual = mlp_forecast(train_residual_df[neighbor_stations_90], test_residual_df[neighbor_stations_90], mlp_order, steps, neurons, epochs)\n",
    "forecast_residual = denormalize(forecast_residual, min_residual, max_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_forecast = reconstruct_ssa_series(forecast_clean, forecast_residual)\n",
    "final_forecast.append(0) ## para manter o mesmo tamanho dos demais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 0.43929870532978144\n"
     ]
    }
   ],
   "source": [
    "rmse = calculate_rmse(test_df[target_station], final_forecast, mlp_order, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'rmse': rmse, 'final': final_forecast, 'clean': forecast_clean, 'residual': forecast_residual}\n",
    "save_obj(result, name=\"wind_mlp_multi_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 0.4424553425781263\n"
     ]
    }
   ],
   "source": [
    "neurons = 50\n",
    "mlp_order = 4\n",
    "epochs = 100\n",
    "steps = 1\n",
    "\n",
    "forecast_clean = mlp_forecast(train_clean_df[[target_station]], test_clean_df[[target_station]], mlp_order, steps, neurons, epochs)\n",
    "forecast_clean = denormalize(forecast_clean, min_clean, max_clean)\n",
    "\n",
    "forecast_residual = mlp_forecast(train_residual_df[[target_station]], test_residual_df[[target_station]], mlp_order, steps, neurons, epochs)\n",
    "forecast_residual = denormalize(forecast_residual, min_residual, max_residual)\n",
    "\n",
    "final_forecast = reconstruct_ssa_series(forecast_clean, forecast_residual)\n",
    "final_forecast.append(0) ## para manter o mesmo tamanho dos demais\n",
    "\n",
    "rmse = calculate_rmse(test_df[target_station], final_forecast, mlp_order, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'rmse': rmse, 'final': final_forecast, 'clean': forecast_clean, 'residual': forecast_residual}\n",
    "save_obj(result, name=\"wind_mlp_uni_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Order FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyFTS.partitioners import Grid, Entropy, Util as pUtil\n",
    "from pyFTS.models import hofts\n",
    "from pyFTS.common import Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hofts_forecast(train_df, test_df, _order, _partitioner, _npartitions):\n",
    "    \n",
    "    fuzzy_sets = _partitioner(data=train_df.values, npart=_npartitions)\n",
    "    model_simple_hofts = hofts.HighOrderFTS()\n",
    "    \n",
    "\n",
    "    model_simple_hofts.fit(train_df.values, order=_order, partitioner=fuzzy_sets)\n",
    "\n",
    "    \n",
    "    forecast = model_simple_hofts.predict(test_df.values)\n",
    "\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hofts_order = 3\n",
    "partitioner = Grid.GridPartitioner\n",
    "nparts = 80\n",
    "\n",
    "\n",
    "forecast_clean = hofts_forecast(train_clean_df[target_station], test_clean_df[target_station], hofts_order, partitioner, nparts)\n",
    "forecast_clean = denormalize(forecast_clean, min_clean, max_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_residual = hofts_forecast(train_residual_df[target_station], test_residual_df[target_station], hofts_order, partitioner, nparts)\n",
    "forecast_residual = denormalize(forecast_residual, min_residual, max_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 0.5398536727385147\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "final_forecast = reconstruct_ssa_series(forecast_clean, forecast_residual)\n",
    "rmse = calculate_rmse(test_df[target_station], final_forecast, hofts_order, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'rmse': rmse, 'final': final_forecast, 'clean': forecast_clean, 'residual': forecast_residual}\n",
    "save_obj(result, name=\"wind_hofts_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import KMeansPartitioner\n",
    "from models import sthofts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.sthofts' from '/Users/cseveriano/Google Drive/Doutorado/Codes/spatio-temporal-forecasting/src/models/sthofts.py'>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(sthofts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sthofts_forecast(train_df, test_df, target, _order, npartitions):\n",
    "    \n",
    "    _partitioner = KMeansPartitioner.KMeansPartitioner(data=train_df.values, npart=npartitions, batch_size=1000, init_size=npartitions*3)\n",
    "    model_sthofts = sthofts.SpatioTemporalHighOrderFTS()\n",
    "    \n",
    "    model_sthofts.fit(train_df.values, dump = 'time', num_batches=100, order=_order, partitioner=_partitioner)\n",
    "    forecast = model_sthofts.predict(test_df.values)\n",
    "    forecast_df = pd.DataFrame(data=forecast, columns=test_df.columns)\n",
    "    return forecast_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 16:05:30] Start training\n",
      "[ 16:05:30] Starting batch 1\n",
      "[ 16:05:32] Finish batch 1\n",
      "[ 16:05:32] Starting batch 2\n",
      "[ 16:05:35] Finish batch 2\n",
      "[ 16:05:35] Starting batch 3\n",
      "[ 16:05:38] Finish batch 3\n",
      "[ 16:05:38] Starting batch 4\n",
      "[ 16:05:41] Finish batch 4\n",
      "[ 16:05:41] Starting batch 5\n",
      "[ 16:05:44] Finish batch 5\n",
      "[ 16:05:44] Starting batch 6\n",
      "[ 16:05:47] Finish batch 6\n",
      "[ 16:05:47] Starting batch 7\n",
      "[ 16:05:50] Finish batch 7\n",
      "[ 16:05:50] Starting batch 8\n",
      "[ 16:05:53] Finish batch 8\n",
      "[ 16:05:53] Starting batch 9\n",
      "[ 16:05:56] Finish batch 9\n",
      "[ 16:05:56] Starting batch 10\n",
      "[ 16:05:59] Finish batch 10\n",
      "[ 16:05:59] Starting batch 11\n",
      "[ 16:06:02] Finish batch 11\n",
      "[ 16:06:02] Starting batch 12\n",
      "[ 16:06:04] Finish batch 12\n",
      "[ 16:06:04] Starting batch 13\n",
      "[ 16:06:07] Finish batch 13\n",
      "[ 16:06:07] Starting batch 14\n",
      "[ 16:06:10] Finish batch 14\n",
      "[ 16:06:10] Starting batch 15\n",
      "[ 16:06:13] Finish batch 15\n",
      "[ 16:06:13] Starting batch 16\n",
      "[ 16:06:16] Finish batch 16\n",
      "[ 16:06:16] Starting batch 17\n",
      "[ 16:06:19] Finish batch 17\n",
      "[ 16:06:19] Starting batch 18\n",
      "[ 16:06:22] Finish batch 18\n",
      "[ 16:06:22] Starting batch 19\n",
      "[ 16:06:25] Finish batch 19\n",
      "[ 16:06:25] Starting batch 20\n",
      "[ 16:06:28] Finish batch 20\n",
      "[ 16:06:28] Starting batch 21\n",
      "[ 16:06:31] Finish batch 21\n",
      "[ 16:06:31] Starting batch 22\n",
      "[ 16:06:34] Finish batch 22\n",
      "[ 16:06:34] Starting batch 23\n",
      "[ 16:06:37] Finish batch 23\n",
      "[ 16:06:37] Starting batch 24\n",
      "[ 16:06:40] Finish batch 24\n",
      "[ 16:06:40] Starting batch 25\n",
      "[ 16:06:43] Finish batch 25\n",
      "[ 16:06:43] Starting batch 26\n",
      "[ 16:06:46] Finish batch 26\n",
      "[ 16:06:46] Starting batch 27\n",
      "[ 16:06:49] Finish batch 27\n",
      "[ 16:06:49] Starting batch 28\n",
      "[ 16:06:51] Finish batch 28\n",
      "[ 16:06:51] Starting batch 29\n",
      "[ 16:06:54] Finish batch 29\n",
      "[ 16:06:54] Starting batch 30\n",
      "[ 16:06:57] Finish batch 30\n",
      "[ 16:06:57] Starting batch 31\n",
      "[ 16:07:00] Finish batch 31\n",
      "[ 16:07:00] Starting batch 32\n",
      "[ 16:07:03] Finish batch 32\n",
      "[ 16:07:03] Starting batch 33\n",
      "[ 16:07:06] Finish batch 33\n",
      "[ 16:07:06] Starting batch 34\n",
      "[ 16:07:09] Finish batch 34\n",
      "[ 16:07:09] Starting batch 35\n",
      "[ 16:07:12] Finish batch 35\n",
      "[ 16:07:12] Starting batch 36\n",
      "[ 16:07:14] Finish batch 36\n",
      "[ 16:07:14] Starting batch 37\n",
      "[ 16:07:17] Finish batch 37\n",
      "[ 16:07:17] Starting batch 38\n",
      "[ 16:07:20] Finish batch 38\n",
      "[ 16:07:20] Starting batch 39\n",
      "[ 16:07:23] Finish batch 39\n",
      "[ 16:07:23] Starting batch 40\n",
      "[ 16:07:26] Finish batch 40\n",
      "[ 16:07:26] Starting batch 41\n",
      "[ 16:07:29] Finish batch 41\n",
      "[ 16:07:29] Starting batch 42\n",
      "[ 16:07:32] Finish batch 42\n",
      "[ 16:07:32] Starting batch 43\n",
      "[ 16:07:35] Finish batch 43\n",
      "[ 16:07:35] Starting batch 44\n",
      "[ 16:07:38] Finish batch 44\n",
      "[ 16:07:38] Starting batch 45\n",
      "[ 16:07:41] Finish batch 45\n",
      "[ 16:07:41] Starting batch 46\n",
      "[ 16:07:44] Finish batch 46\n",
      "[ 16:07:44] Starting batch 47\n",
      "[ 16:07:46] Finish batch 47\n",
      "[ 16:07:46] Starting batch 48\n",
      "[ 16:07:49] Finish batch 48\n",
      "[ 16:07:49] Starting batch 49\n",
      "[ 16:07:52] Finish batch 49\n",
      "[ 16:07:52] Starting batch 50\n",
      "[ 16:07:55] Finish batch 50\n",
      "[ 16:07:55] Starting batch 51\n",
      "[ 16:07:59] Finish batch 51\n",
      "[ 16:07:59] Starting batch 52\n",
      "[ 16:08:02] Finish batch 52\n",
      "[ 16:08:02] Starting batch 53\n",
      "[ 16:08:05] Finish batch 53\n",
      "[ 16:08:05] Starting batch 54\n",
      "[ 16:08:09] Finish batch 54\n",
      "[ 16:08:09] Starting batch 55\n",
      "[ 16:08:12] Finish batch 55\n",
      "[ 16:08:12] Starting batch 56\n",
      "[ 16:08:16] Finish batch 56\n",
      "[ 16:08:16] Starting batch 57\n",
      "[ 16:08:19] Finish batch 57\n",
      "[ 16:08:19] Starting batch 58\n",
      "[ 16:08:22] Finish batch 58\n",
      "[ 16:08:22] Starting batch 59\n",
      "[ 16:08:25] Finish batch 59\n",
      "[ 16:08:25] Starting batch 60\n",
      "[ 16:08:28] Finish batch 60\n",
      "[ 16:08:28] Starting batch 61\n",
      "[ 16:08:31] Finish batch 61\n",
      "[ 16:08:31] Starting batch 62\n",
      "[ 16:08:34] Finish batch 62\n",
      "[ 16:08:34] Starting batch 63\n",
      "[ 16:08:37] Finish batch 63\n",
      "[ 16:08:37] Starting batch 64\n",
      "[ 16:08:40] Finish batch 64\n",
      "[ 16:08:40] Starting batch 65\n",
      "[ 16:08:43] Finish batch 65\n",
      "[ 16:08:43] Starting batch 66\n",
      "[ 16:08:45] Finish batch 66\n",
      "[ 16:08:45] Starting batch 67\n",
      "[ 16:08:48] Finish batch 67\n",
      "[ 16:08:48] Starting batch 68\n",
      "[ 16:08:51] Finish batch 68\n",
      "[ 16:08:51] Starting batch 69\n",
      "[ 16:08:54] Finish batch 69\n",
      "[ 16:08:54] Starting batch 70\n",
      "[ 16:08:57] Finish batch 70\n",
      "[ 16:08:57] Starting batch 71\n",
      "[ 16:09:00] Finish batch 71\n",
      "[ 16:09:00] Starting batch 72\n",
      "[ 16:09:02] Finish batch 72\n",
      "[ 16:09:02] Starting batch 73\n",
      "[ 16:09:05] Finish batch 73\n",
      "[ 16:09:05] Starting batch 74\n",
      "[ 16:09:08] Finish batch 74\n",
      "[ 16:09:08] Starting batch 75\n",
      "[ 16:09:11] Finish batch 75\n",
      "[ 16:09:11] Starting batch 76\n",
      "[ 16:09:14] Finish batch 76\n",
      "[ 16:09:14] Starting batch 77\n",
      "[ 16:09:17] Finish batch 77\n",
      "[ 16:09:17] Starting batch 78\n",
      "[ 16:09:19] Finish batch 78\n",
      "[ 16:09:19] Starting batch 79\n",
      "[ 16:09:22] Finish batch 79\n",
      "[ 16:09:22] Starting batch 80\n",
      "[ 16:09:25] Finish batch 80\n",
      "[ 16:09:25] Starting batch 81\n",
      "[ 16:09:28] Finish batch 81\n",
      "[ 16:09:28] Starting batch 82\n",
      "[ 16:09:31] Finish batch 82\n",
      "[ 16:09:31] Starting batch 83\n",
      "[ 16:09:33] Finish batch 83\n",
      "[ 16:09:33] Starting batch 84\n",
      "[ 16:09:36] Finish batch 84\n",
      "[ 16:09:36] Starting batch 85\n",
      "[ 16:09:39] Finish batch 85\n",
      "[ 16:09:39] Starting batch 86\n",
      "[ 16:09:42] Finish batch 86\n",
      "[ 16:09:42] Starting batch 87\n",
      "[ 16:09:45] Finish batch 87\n",
      "[ 16:09:45] Starting batch 88\n",
      "[ 16:09:48] Finish batch 88\n",
      "[ 16:09:48] Starting batch 89\n",
      "[ 16:09:50] Finish batch 89\n",
      "[ 16:09:50] Starting batch 90\n",
      "[ 16:09:53] Finish batch 90\n",
      "[ 16:09:53] Starting batch 91\n",
      "[ 16:09:56] Finish batch 91\n",
      "[ 16:09:56] Starting batch 92\n",
      "[ 16:09:59] Finish batch 92\n",
      "[ 16:09:59] Starting batch 93\n",
      "[ 16:10:02] Finish batch 93\n",
      "[ 16:10:02] Starting batch 94\n",
      "[ 16:10:05] Finish batch 94\n",
      "[ 16:10:05] Starting batch 95\n",
      "[ 16:10:07] Finish batch 95\n",
      "[ 16:10:07] Starting batch 96\n",
      "[ 16:10:10] Finish batch 96\n",
      "[ 16:10:10] Starting batch 97\n",
      "[ 16:10:13] Finish batch 97\n",
      "[ 16:10:13] Starting batch 98\n",
      "[ 16:10:16] Finish batch 98\n",
      "[ 16:10:16] Starting batch 99\n",
      "[ 16:10:19] Finish batch 99\n",
      "[ 16:10:19] Starting batch 100\n",
      "[ 16:10:22] Finish batch 100\n",
      "[ 16:10:22] Starting batch 101\n",
      "[ 16:10:22] Finish batch 101\n",
      "[ 16:10:22] Finish training\n"
     ]
    }
   ],
   "source": [
    "sthofts_order = 4\n",
    "nparts = 20\n",
    "\n",
    "\n",
    "forecast_clean = sthofts_forecast(train_clean_df[neighbor_stations_90], test_clean_df[neighbor_stations_90], target_station, sthofts_order, nparts)\n",
    "forecast_clean = denormalize(forecast_clean, min_clean, max_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 16:12:52] Start training\n",
      "[ 16:12:52] Starting batch 1\n",
      "[ 16:12:59] Finish batch 1\n",
      "[ 16:12:59] Starting batch 2\n",
      "[ 16:13:06] Finish batch 2\n",
      "[ 16:13:06] Starting batch 3\n",
      "[ 16:13:13] Finish batch 3\n",
      "[ 16:13:13] Starting batch 4\n",
      "[ 16:13:21] Finish batch 4\n",
      "[ 16:13:21] Starting batch 5\n",
      "[ 16:13:26] Finish batch 5\n",
      "[ 16:13:26] Starting batch 6\n",
      "[ 16:13:33] Finish batch 6\n",
      "[ 16:13:33] Starting batch 7\n",
      "[ 16:13:41] Finish batch 7\n",
      "[ 16:13:41] Starting batch 8\n",
      "[ 16:13:47] Finish batch 8\n",
      "[ 16:13:47] Starting batch 9\n",
      "[ 16:13:53] Finish batch 9\n",
      "[ 16:13:53] Starting batch 10\n",
      "[ 16:13:58] Finish batch 10\n",
      "[ 16:13:58] Starting batch 11\n",
      "[ 16:14:06] Finish batch 11\n",
      "[ 16:14:06] Starting batch 12\n",
      "[ 16:14:13] Finish batch 12\n",
      "[ 16:14:13] Starting batch 13\n",
      "[ 16:14:21] Finish batch 13\n",
      "[ 16:14:21] Starting batch 14\n",
      "[ 16:14:29] Finish batch 14\n",
      "[ 16:14:29] Starting batch 15\n",
      "[ 16:14:35] Finish batch 15\n",
      "[ 16:14:35] Starting batch 16\n",
      "[ 16:14:42] Finish batch 16\n",
      "[ 16:14:42] Starting batch 17\n",
      "[ 16:14:48] Finish batch 17\n",
      "[ 16:14:48] Starting batch 18\n",
      "[ 16:14:56] Finish batch 18\n",
      "[ 16:14:56] Starting batch 19\n",
      "[ 16:15:04] Finish batch 19\n",
      "[ 16:15:04] Starting batch 20\n",
      "[ 16:15:15] Finish batch 20\n",
      "[ 16:15:15] Starting batch 21\n",
      "[ 16:15:21] Finish batch 21\n",
      "[ 16:15:21] Starting batch 22\n",
      "[ 16:15:30] Finish batch 22\n",
      "[ 16:15:30] Starting batch 23\n",
      "[ 16:15:38] Finish batch 23\n",
      "[ 16:15:38] Starting batch 24\n",
      "[ 16:15:46] Finish batch 24\n",
      "[ 16:15:46] Starting batch 25\n",
      "[ 16:15:54] Finish batch 25\n",
      "[ 16:15:54] Starting batch 26\n",
      "[ 16:16:03] Finish batch 26\n",
      "[ 16:16:03] Starting batch 27\n",
      "[ 16:16:10] Finish batch 27\n",
      "[ 16:16:10] Starting batch 28\n",
      "[ 16:16:16] Finish batch 28\n",
      "[ 16:16:16] Starting batch 29\n",
      "[ 16:16:23] Finish batch 29\n",
      "[ 16:16:23] Starting batch 30\n",
      "[ 16:16:31] Finish batch 30\n",
      "[ 16:16:31] Starting batch 31\n",
      "[ 16:16:39] Finish batch 31\n",
      "[ 16:16:39] Starting batch 32\n",
      "[ 16:16:45] Finish batch 32\n",
      "[ 16:16:45] Starting batch 33\n",
      "[ 16:16:52] Finish batch 33\n",
      "[ 16:16:52] Starting batch 34\n",
      "[ 16:17:00] Finish batch 34\n",
      "[ 16:17:00] Starting batch 35\n",
      "[ 16:17:08] Finish batch 35\n",
      "[ 16:17:08] Starting batch 36\n",
      "[ 16:17:19] Finish batch 36\n",
      "[ 16:17:19] Starting batch 37\n",
      "[ 16:17:27] Finish batch 37\n",
      "[ 16:17:27] Starting batch 38\n",
      "[ 16:17:35] Finish batch 38\n",
      "[ 16:17:35] Starting batch 39\n",
      "[ 16:17:43] Finish batch 39\n",
      "[ 16:17:43] Starting batch 40\n",
      "[ 16:17:50] Finish batch 40\n",
      "[ 16:17:50] Starting batch 41\n",
      "[ 16:17:57] Finish batch 41\n",
      "[ 16:17:57] Starting batch 42\n",
      "[ 16:18:06] Finish batch 42\n",
      "[ 16:18:06] Starting batch 43\n",
      "[ 16:18:15] Finish batch 43\n",
      "[ 16:18:15] Starting batch 44\n",
      "[ 16:18:23] Finish batch 44\n",
      "[ 16:18:23] Starting batch 45\n",
      "[ 16:18:31] Finish batch 45\n",
      "[ 16:18:31] Starting batch 46\n",
      "[ 16:18:38] Finish batch 46\n",
      "[ 16:18:38] Starting batch 47\n",
      "[ 16:18:46] Finish batch 47\n",
      "[ 16:18:46] Starting batch 48\n",
      "[ 16:18:55] Finish batch 48\n",
      "[ 16:18:55] Starting batch 49\n",
      "[ 16:19:01] Finish batch 49\n",
      "[ 16:19:01] Starting batch 50\n",
      "[ 16:19:08] Finish batch 50\n",
      "[ 16:19:08] Starting batch 51\n",
      "[ 16:19:15] Finish batch 51\n",
      "[ 16:19:15] Starting batch 52\n",
      "[ 16:19:20] Finish batch 52\n",
      "[ 16:19:20] Starting batch 53\n",
      "[ 16:19:27] Finish batch 53\n",
      "[ 16:19:27] Starting batch 54\n",
      "[ 16:19:33] Finish batch 54\n",
      "[ 16:19:33] Starting batch 55\n",
      "[ 16:19:39] Finish batch 55\n",
      "[ 16:19:39] Starting batch 56\n",
      "[ 16:19:44] Finish batch 56\n",
      "[ 16:19:44] Starting batch 57\n",
      "[ 16:19:49] Finish batch 57\n",
      "[ 16:19:49] Starting batch 58\n",
      "[ 16:19:55] Finish batch 58\n",
      "[ 16:19:55] Starting batch 59\n",
      "[ 16:20:01] Finish batch 59\n",
      "[ 16:20:01] Starting batch 60\n",
      "[ 16:20:06] Finish batch 60\n",
      "[ 16:20:06] Starting batch 61\n",
      "[ 16:20:12] Finish batch 61\n",
      "[ 16:20:12] Starting batch 62\n",
      "[ 16:20:18] Finish batch 62\n",
      "[ 16:20:18] Starting batch 63\n",
      "[ 16:20:24] Finish batch 63\n",
      "[ 16:20:24] Starting batch 64\n",
      "[ 16:20:30] Finish batch 64\n",
      "[ 16:20:30] Starting batch 65\n",
      "[ 16:20:35] Finish batch 65\n",
      "[ 16:20:35] Starting batch 66\n",
      "[ 16:20:40] Finish batch 66\n",
      "[ 16:20:40] Starting batch 67\n",
      "[ 16:20:46] Finish batch 67\n",
      "[ 16:20:46] Starting batch 68\n",
      "[ 16:20:51] Finish batch 68\n",
      "[ 16:20:51] Starting batch 69\n",
      "[ 16:20:56] Finish batch 69\n",
      "[ 16:20:56] Starting batch 70\n",
      "[ 16:21:02] Finish batch 70\n",
      "[ 16:21:02] Starting batch 71\n",
      "[ 16:21:07] Finish batch 71\n",
      "[ 16:21:07] Starting batch 72\n",
      "[ 16:21:13] Finish batch 72\n",
      "[ 16:21:13] Starting batch 73\n",
      "[ 16:21:19] Finish batch 73\n",
      "[ 16:21:19] Starting batch 74\n",
      "[ 16:21:25] Finish batch 74\n",
      "[ 16:21:25] Starting batch 75\n",
      "[ 16:21:32] Finish batch 75\n",
      "[ 16:21:32] Starting batch 76\n",
      "[ 16:21:38] Finish batch 76\n",
      "[ 16:21:38] Starting batch 77\n",
      "[ 16:21:43] Finish batch 77\n",
      "[ 16:21:43] Starting batch 78\n",
      "[ 16:21:48] Finish batch 78\n",
      "[ 16:21:48] Starting batch 79\n",
      "[ 16:21:54] Finish batch 79\n",
      "[ 16:21:54] Starting batch 80\n",
      "[ 16:21:59] Finish batch 80\n",
      "[ 16:21:59] Starting batch 81\n",
      "[ 16:22:06] Finish batch 81\n",
      "[ 16:22:06] Starting batch 82\n",
      "[ 16:22:11] Finish batch 82\n",
      "[ 16:22:11] Starting batch 83\n",
      "[ 16:22:16] Finish batch 83\n",
      "[ 16:22:16] Starting batch 84\n",
      "[ 16:22:22] Finish batch 84\n",
      "[ 16:22:22] Starting batch 85\n",
      "[ 16:22:28] Finish batch 85\n",
      "[ 16:22:28] Starting batch 86\n",
      "[ 16:22:34] Finish batch 86\n",
      "[ 16:22:34] Starting batch 87\n",
      "[ 16:22:39] Finish batch 87\n",
      "[ 16:22:39] Starting batch 88\n",
      "[ 16:22:44] Finish batch 88\n",
      "[ 16:22:44] Starting batch 89\n",
      "[ 16:22:49] Finish batch 89\n",
      "[ 16:22:49] Starting batch 90\n",
      "[ 16:22:54] Finish batch 90\n",
      "[ 16:22:54] Starting batch 91\n",
      "[ 16:22:59] Finish batch 91\n",
      "[ 16:22:59] Starting batch 92\n",
      "[ 16:23:04] Finish batch 92\n",
      "[ 16:23:04] Starting batch 93\n",
      "[ 16:23:09] Finish batch 93\n",
      "[ 16:23:09] Starting batch 94\n",
      "[ 16:23:17] Finish batch 94\n",
      "[ 16:23:17] Starting batch 95\n",
      "[ 16:23:24] Finish batch 95\n",
      "[ 16:23:24] Starting batch 96\n",
      "[ 16:23:30] Finish batch 96\n",
      "[ 16:23:30] Starting batch 97\n",
      "[ 16:23:35] Finish batch 97\n",
      "[ 16:23:35] Starting batch 98\n",
      "[ 16:23:42] Finish batch 98\n",
      "[ 16:23:42] Starting batch 99\n",
      "[ 16:23:49] Finish batch 99\n",
      "[ 16:23:49] Starting batch 100\n",
      "[ 16:23:55] Finish batch 100\n",
      "[ 16:23:55] Starting batch 101\n",
      "[ 16:23:55] Finish batch 101\n",
      "[ 16:23:55] Finish training\n"
     ]
    }
   ],
   "source": [
    "forecast_residual = sthofts_forecast(train_residual_df[neighbor_stations_90], test_residual_df[neighbor_stations_90], target_station, sthofts_order, nparts)\n",
    "forecast_residual = denormalize(forecast_residual, min_residual, max_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 0.7407508188869937\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "final_forecast = reconstruct_ssa_series(forecast_clean, forecast_residual)\n",
    "rmse = calculate_rmse(test_df[target_station], final_forecast, sthofts_order, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'rmse': rmse, 'final': final_forecast, 'clean': forecast_clean, 'residual': forecast_residual}\n",
    "save_obj(result, name=\"wind_sthofts_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Variance FTS - CVFTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyFTS.models.nonstationary import cvfts\n",
    "from pyFTS.models.nonstationary import partitioners as nspartitioners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvfts_forecast(train, test, _partitioner,_partitions):\n",
    "    \n",
    "    fuzzy_sets =  nspartitioners.PolynomialNonStationaryPartitioner(data=train.values, part=_partitioner(data=train.values, npart=_partitions), degree=2)\n",
    "                    \n",
    "    model_cvfts = cvfts.ConditionalVarianceFTS()\n",
    "    model_cvfts.fit(train.values, parameters=1, partitioner=fuzzy_sets)\n",
    "\n",
    "    forecast = model_cvfts.predict(test.values)\n",
    "\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioner = Grid.GridPartitioner\n",
    "nparts = 90\n",
    "\n",
    "\n",
    "forecast_clean = cvfts_forecast(train_clean_df[target_station], test_clean_df[target_station], partitioner, nparts)\n",
    "forecast_clean = denormalize(forecast_clean, min_clean, max_clean)\n",
    "\n",
    "forecast_residual = cvfts_forecast(train_residual_df[target_station], test_residual_df[target_station], partitioner, nparts)\n",
    "forecast_residual = denormalize(forecast_residual, min_residual, max_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 0.5781277828227142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cseveriano/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "final_forecast = reconstruct_ssa_series(forecast_clean, forecast_residual)\n",
    "rmse = calculate_rmse(test_df[target_station], final_forecast, 1, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'rmse': rmse, 'final': final_forecast, 'clean': forecast_clean, 'residual': forecast_residual}\n",
    "save_obj(result, name=\"wind_cvfts_1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
