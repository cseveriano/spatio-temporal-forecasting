{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    mindf = df.min()\n",
    "    maxdf = df.max()\n",
    "    return (df-mindf)/(maxdf-mindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(norm, _min, _max):\n",
    "    return [(n * (_max-_min)) + _min for n in norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, interval):\n",
    "    sample_df = df.loc[interval]\n",
    "\n",
    "    week = (sample_df.index.day - 1) // 7 + 1\n",
    "\n",
    "    # PARA OS TESTES:\n",
    "    # 2 SEMANAS PARA TREINAMENTO\n",
    "    train_df = sample_df.loc[week <= 2]\n",
    "\n",
    "    # 1 SEMANA PARA VALIDACAO\n",
    "    validation_df = sample_df.loc[week == 3]\n",
    "\n",
    "    # 1 SEMANA PARA TESTES\n",
    "    test_df = sample_df.loc[week > 3]\n",
    "    \n",
    "    return (train_df, validation_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(test, forecast, order, step):\n",
    "    rmse = math.sqrt(mean_squared_error(test.iloc[(order):], forecast[:-step]))\n",
    "    print(\"RMSE : \"+str(rmse))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_ssa_series(clean, residual):\n",
    "    return [r + c for r, c in zip(residual,clean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('results/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('results/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(raw_df, interval=1):\n",
    "    df_diff = pd.DataFrame(columns=raw_df.columns, index=raw_df.index[1:])\n",
    "    \n",
    "    for col in raw_df.columns:\n",
    "        raw_array = raw_df[col]\n",
    "        diff = []\n",
    "        for i in range(interval, len(raw_array)):\n",
    "            value = raw_array[i] - raw_array[i - interval]\n",
    "            diff.append(value)\n",
    "        \n",
    "        df_diff[col] = diff\n",
    "    return df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_difference(raw_series, diff_series):\n",
    "    inverted = []\n",
    "    for i in range(len(diff_series)):\n",
    "        interval = len(raw_series)-i\n",
    "        value = diff_series[i] + raw_series[-interval]\n",
    "        inverted.append(value)\n",
    "        \n",
    "    return inverted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "Split the data into train, validation and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set target and input variables \n",
    "target_station = 'DHHL_3'\n",
    "\n",
    "#All neighbor stations with residual correlation greater than .90\n",
    "neighbor_stations_90 = ['DHHL_3',  'DHHL_4','DHHL_5','DHHL_10','DHHL_11','DHHL_9','DHHL_2', 'DHHL_6','DHHL_7','DHHL_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"df_oahu.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove columns with many corrupted or missing values\n",
    "df.drop(columns=['AP_1', 'AP_7'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize Data\n",
    "\n",
    "# Save Min-Max for Denorm\n",
    "min_raw = df[target_station].min()\n",
    "\n",
    "max_raw = df[target_station].max()\n",
    "\n",
    "# Perform Normalization\n",
    "norm_df = normalize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "interval = ((df.index >= '2010-06') & (df.index < '2011-06'))\n",
    "#interval = ((df.index >= '2010-11') & (df.index <= '2010-12'))\n",
    "\n",
    "(train_df, validation_df, test_df) = split_data(df, interval)\n",
    "(norm_train_df, norm_validation_df, norm_test_df) = split_data(norm_df, interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting with Raw Time Series\n",
    "\n",
    "For each dataset, all the time series were normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistence_forecast(train, test, step):\n",
    "    predictions = []\n",
    "    \n",
    "    for t in np.arange(0,len(test), step):\n",
    "        yhat = [test.iloc[t]]  * step\n",
    "        predictions.extend(yhat)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 116.58118645284732\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "persistence_order = 1\n",
    "\n",
    "forecast = persistence_forecast(norm_train_df[target_station], norm_test_df[target_station],step)\n",
    "forecast = denormalize(forecast, min_raw, max_raw)\n",
    "\n",
    "rmse = calculate_rmse(test_df[target_station], forecast, persistence_order, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'rmse': rmse, 'final': forecast}\n",
    "save_obj(result, name=\"oahu_raw_persistence_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cseveriano/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarima_forecast(train, test, arima_order, sarima_order, step):\n",
    "\n",
    "    predictions = []\n",
    "    window_size = sarima_order[3] * 5\n",
    "    \n",
    "    for date in train.index.to_period('M').unique():\n",
    "        \n",
    "        history = list(train[str(date)].iloc[-window_size:])\n",
    "        \n",
    "        model = SARIMAX(history, order=arima_order, seasonal_order=sarima_order,enforce_invertibility=False,enforce_stationarity=False)\n",
    "        model_fit = model.fit(disp=True,enforce_invertibility=False,  method='powell', maxiter=200)\n",
    "        \n",
    "        #save the state parameter\n",
    "        est_params = model_fit.params\n",
    "        est_state = model_fit.predicted_state[:, -1]\n",
    "        est_state_cov = model_fit.predicted_state_cov[:, :, -1]\n",
    "\n",
    "        print(\"Predicting : \"+str(date))\n",
    "        \n",
    "        st = 0\n",
    "        test_date = test[str(date)]\n",
    "        \n",
    "        for t in np.arange(1,len(test_date)+1,step):\n",
    "            obs = test_date.iloc[st:t].values\n",
    "            history.extend(obs)\n",
    "            history = history[-window_size:]\n",
    "            \n",
    "            mod_updated = SARIMAX(history, order=arima_order, seasonal_order=sarima_order,enforce_invertibility=False,enforce_stationarity=False)\n",
    "            mod_updated.initialize_known(est_state, est_state_cov)\n",
    "            mod_frcst = mod_updated.smooth(est_params)\n",
    "\n",
    "        \n",
    "            yhat = mod_frcst.forecast(step)   \n",
    "            predictions.extend(yhat)\n",
    "            \n",
    "            est_params = mod_frcst.params\n",
    "            est_state = mod_frcst.predicted_state[:, -1]\n",
    "            est_state_cov = mod_frcst.predicted_state_cov[:, :, -1]\n",
    "            \n",
    "            st = t\n",
    "                \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.619265\n",
      "         Iterations: 8\n",
      "         Function evaluations: 947\n",
      "Predicting : 2010-06\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 18.517468\n",
      "         Iterations: 1\n",
      "         Function evaluations: 88\n",
      "Predicting : 2010-07\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 18.934954\n",
      "         Iterations: 1\n",
      "         Function evaluations: 91\n",
      "Predicting : 2010-08\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 19.126512\n",
      "         Iterations: 4\n",
      "         Function evaluations: 415\n",
      "Predicting : 2010-09\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.728150\n",
      "         Iterations: 23\n",
      "         Function evaluations: 2698\n",
      "Predicting : 2010-10\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.790575\n",
      "         Iterations: 8\n",
      "         Function evaluations: 964\n",
      "Predicting : 2010-11\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.765307\n",
      "         Iterations: 7\n",
      "         Function evaluations: 896\n",
      "Predicting : 2010-12\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 19.201844\n",
      "         Iterations: 1\n",
      "         Function evaluations: 93\n",
      "Predicting : 2011-01\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.588610\n",
      "         Iterations: 11\n",
      "         Function evaluations: 1292\n",
      "Predicting : 2011-02\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.647769\n",
      "         Iterations: 14\n",
      "         Function evaluations: 1530\n",
      "Predicting : 2011-03\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.485479\n",
      "         Iterations: 9\n",
      "         Function evaluations: 1045\n",
      "Predicting : 2011-04\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.639458\n",
      "         Iterations: 5\n",
      "         Function evaluations: 561\n",
      "Predicting : 2011-05\n",
      "RMSE : 12729.503449265369\n"
     ]
    }
   ],
   "source": [
    "order = 1\n",
    "step = 1\n",
    "arima_order = (2, 1, 2)\n",
    "sarima_order = (1, 1, 1, 61)\n",
    "forecast = sarima_forecast(norm_train_df[target_station], norm_test_df[target_station], arima_order, sarima_order, step)\n",
    "forecast = denormalize(forecast, min_raw, max_raw)\n",
    "rmse = calculate_rmse(test_df[target_station], forecast, order, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 178.31337062654006\n"
     ]
    }
   ],
   "source": [
    "result = {'rmse': rmse, 'final': forecast}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(result, name=\"oahu_raw_sarima_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Autoregressive - VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR, DynamicVAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_forecast(train, test, target, order, step):\n",
    "    model = VAR(train.values)\n",
    "    results = model.fit(maxlags=order)\n",
    "    lag_order = results.k_ar\n",
    "    print(\"Lag order:\" + str(lag_order))\n",
    "    forecast = []\n",
    "\n",
    "    for i in np.arange(0,len(test)-lag_order+1,step) :\n",
    "        forecast.extend(results.forecast(test.values[i:i+lag_order],step))\n",
    "\n",
    "    forecast_df = pd.DataFrame(columns=test.columns, data=forecast)\n",
    "    return forecast_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag order:4\n"
     ]
    }
   ],
   "source": [
    "var_order = 4\n",
    "step = 1\n",
    "\n",
    "forecast = var_forecast(norm_train_df[neighbor_stations_90], norm_test_df[neighbor_stations_90], target_station, var_order, step)\n",
    "forecast = denormalize(forecast, min_raw, max_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 109.28142781608405\n"
     ]
    }
   ],
   "source": [
    "rmse = calculate_rmse(test_df[target_station], forecast, var_order, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'rmse': rmse, 'final': forecast}\n",
    "save_obj(result, name=\"oahu_raw_var_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_multi_forecast(train_df, test_df, _order, _steps, _neurons, _epochs):\n",
    "\n",
    "    \n",
    "    nfeat = len(train_df.columns)\n",
    "    nlags = _order\n",
    "    nsteps = _steps\n",
    "    nobs = nlags * nfeat\n",
    "    \n",
    "    train_reshaped_df = series_to_supervised(train_df, n_in=nlags, n_out=nsteps)\n",
    "    train_X, train_Y = train_reshaped_df.iloc[:,:nobs].values, train_reshaped_df.iloc[:,-nfeat].values\n",
    "    train_X = train_X.reshape((train_X.shape[0], nlags, nfeat))\n",
    "    \n",
    "    test_reshaped_df = series_to_supervised(test_df, n_in=nlags, n_out=nsteps)\n",
    "    test_X, test_Y = test_reshaped_df.iloc[:,:nobs].values, test_reshaped_df.iloc[:,-nfeat].values\n",
    "    test_X = test_X.reshape((test_X.shape[0], nlags, nfeat))\n",
    "    \n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(_neurons, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    \n",
    "    # fit network\n",
    "    model.fit(train_X, train_Y, epochs=_epochs, batch_size=72, verbose=False, shuffle=False)\n",
    "    \n",
    "    forecast = model.predict(test_X)\n",
    "        \n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 50\n",
    "lstm_order = 2\n",
    "epochs = 100\n",
    "steps = 1\n",
    "\n",
    "forecast = lstm_multi_forecast(norm_train_df[neighbor_stations_90], norm_test_df[neighbor_stations_90], lstm_order, steps, neurons, epochs)\n",
    "forecast = denormalize(forecast, min_raw, max_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.append(0) ## para manter o mesmo tamanho dos demais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 110.81399734978343\n"
     ]
    }
   ],
   "source": [
    "rmse = calculate_rmse(test_df[target_station], forecast, lstm_order, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'rmse': rmse, 'final': forecast}\n",
    "save_obj(result, name=\"oahu_raw_lstm_multi_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 50\n",
    "lstm_order = 2\n",
    "epochs = 100\n",
    "steps = 1\n",
    "\n",
    "forecast = lstm_multi_forecast(norm_train_df[[target_station]], norm_test_df[[target_station]], lstm_order, steps, neurons, epochs)\n",
    "forecast = denormalize(forecast, min_raw, max_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.append(0) ## para manter o mesmo tamanho dos demais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 113.89297538695396\n"
     ]
    }
   ],
   "source": [
    "rmse = calculate_rmse(test_df[target_station], forecast, lstm_order, step)\n",
    "result = {'rmse': rmse, 'final': forecast}\n",
    "save_obj(result, name=\"oahu_raw_lstm_multi_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron - MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_forecast(train_df, test_df, _order, _steps, _neurons, _epochs):\n",
    "\n",
    "    \n",
    "    nfeat = len(train_df.columns)\n",
    "    nlags = _order\n",
    "    nsteps = _steps\n",
    "    nobs = nlags * nfeat\n",
    "    \n",
    "    train_reshaped_df = series_to_supervised(train_df, n_in=nlags, n_out=nsteps)\n",
    "    train_X, train_Y = train_reshaped_df.iloc[:,:nobs].values, train_reshaped_df.iloc[:,-nfeat].values\n",
    "    \n",
    "    test_reshaped_df = series_to_supervised(test_df, n_in=nlags, n_out=nsteps)\n",
    "    test_X, test_Y = test_reshaped_df.iloc[:,:nobs].values, test_reshaped_df.iloc[:,-nfeat].values\n",
    "    \n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_dim=train_X.shape[1]))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit(train_X, train_Y, epochs=_epochs, batch_size=72, verbose=False, shuffle=False)   \n",
    "\n",
    "    forecast = model.predict(test_X)\n",
    "        \n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 50\n",
    "mlp_order = 2\n",
    "epochs = 500\n",
    "steps = 1\n",
    "\n",
    "forecast = mlp_forecast(norm_train_df[neighbor_stations_90], norm_test_df[neighbor_stations_90], mlp_order, steps, neurons, epochs)\n",
    "forecast = denormalize(forecast, min_raw, max_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.append(0) ## para manter o mesmo tamanho dos demais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 109.39872643629796\n"
     ]
    }
   ],
   "source": [
    "rmse = calculate_rmse(test_df[target_station], forecast, mlp_order, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'rmse': rmse, 'final': forecast}\n",
    "save_obj(result, name=\"oahu_raw_mlp_multi_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 109.46746359249855\n"
     ]
    }
   ],
   "source": [
    "neurons = 50\n",
    "mlp_order = 4\n",
    "epochs = 500\n",
    "steps = 1\n",
    "\n",
    "forecast = mlp_forecast(norm_train_df[[target_station]], norm_test_df[[target_station]], mlp_order, steps, neurons, epochs)\n",
    "forecast = denormalize(forecast, min_raw, max_raw)\n",
    "\n",
    "forecast.append(0) ## para manter o mesmo tamanho dos demais\n",
    "\n",
    "rmse = calculate_rmse(test_df[target_station], forecast, mlp_order, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'rmse': rmse, 'final': forecast}\n",
    "save_obj(result, name=\"oahu_raw_mlp_uni_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Order FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyFTS.partitioners import Grid, Entropy, Util as pUtil\n",
    "from pyFTS.models import hofts\n",
    "from pyFTS.common import Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hofts_forecast(train_df, test_df, _order, _partitioner, _npartitions):\n",
    "    \n",
    "    fuzzy_sets = _partitioner(data=train_df.values, npart=_npartitions)\n",
    "    model_simple_hofts = hofts.HighOrderFTS()\n",
    "    \n",
    "\n",
    "    model_simple_hofts.fit(train_df.values, order=_order, partitioner=fuzzy_sets)\n",
    "\n",
    "    \n",
    "    forecast = model_simple_hofts.predict(test_df.values)\n",
    "\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hofts_order = 2\n",
    "#partitioner = Entropy.EntropyPartitioner\n",
    "partitioner = Grid.GridPartitioner\n",
    "nparts = 90\n",
    "\n",
    "\n",
    "forecast = hofts_forecast(norm_train_df[target_station], norm_test_df[target_station], hofts_order, partitioner, nparts)\n",
    "forecast = denormalize(forecast, min_raw, max_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 120.11789595500608\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "rmse = calculate_rmse(test_df[target_station], forecast, hofts_order, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'rmse': rmse, 'final': forecast}\n",
    "save_obj(result, name=\"oahu_raw_hofts_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import KMeansPartitioner\n",
    "from models import sthofts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.sthofts' from '/Users/cseveriano/Google Drive/Doutorado/Codes/spatio-temporal-forecasting/src/models/sthofts.py'>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(sthofts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sthofts_forecast(train_df, test_df, target, _order, npartitions):\n",
    "    \n",
    "    _partitioner = KMeansPartitioner.KMeansPartitioner(data=train_df.values, npart=npartitions, batch_size=1000, init_size=npartitions*3)\n",
    "    model_sthofts = sthofts.SpatioTemporalHighOrderFTS()\n",
    "    \n",
    "    model_sthofts.fit(train_df.values, num_batches=100, order=_order, partitioner=_partitioner)\n",
    "    forecast = model_sthofts.predict(test_df.values)\n",
    "    forecast_df = pd.DataFrame(data=forecast, columns=test_df.columns)\n",
    "    return forecast_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sthofts_order = 2\n",
    "nparts = 20\n",
    "\n",
    "\n",
    "forecast = sthofts_forecast(norm_train_df[neighbor_stations_90], norm_test_df[neighbor_stations_90], target_station, sthofts_order, nparts)\n",
    "forecast = denormalize(forecast, min_raw, max_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 169.33198777653578\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "rmse = calculate_rmse(test_df[target_station], forecast, sthofts_order, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'rmse': rmse, 'final': forecast}\n",
    "save_obj(result, name=\"oahu_raw_sthofts_1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
