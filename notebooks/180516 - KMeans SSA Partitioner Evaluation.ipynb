{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from models import KMeansPartitioner\n",
    "from sklearn import preprocessing\n",
    "from pyFTS.partitioners import Grid, Util as pUtil\n",
    "from pyFTS.models import hofts\n",
    "\n",
    "from models import sthofts\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_rmse(targets, forecasts):\n",
    "    if isinstance(targets, list):\n",
    "        targets = np.array(targets)\n",
    "    if isinstance(forecasts, list):\n",
    "        forecasts = np.array(forecasts)\n",
    "    return ((np.sqrt(np.nanmean((targets - forecasts) ** 2))) / np.nanmean(targets) ) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    mindf = df.min()\n",
    "    maxdf = df.max()\n",
    "    return (df-mindf)/(maxdf-mindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(norm, _min, _max):\n",
    "    return [(n * (_max-_min)) + _min for n in norm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de Dados\n",
    "Montagem de casos de treinamento, validação e testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"df_oahu.pkl\")\n",
    "df_ssa_clean = pd.read_pickle(\"df_ssa_clean.pkl\")\n",
    "df_ssa_residual = pd.read_pickle(\"df_ssa_residual.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = '2010-11'\n",
    "\n",
    "sample_df = df.loc[interval]\n",
    "residual_sample_df = df_ssa_residual.loc[interval]\n",
    "clean_sample_df = df_ssa_clean.loc[interval]\n",
    "\n",
    "norm_residual_sample_df = normalize(residual_sample_df)\n",
    "norm_clean_sample_df = normalize(clean_sample_df)\n",
    "\n",
    "\n",
    "week = (sample_df.index.day - 1) // 7 + 1\n",
    "\n",
    "# PARA OS TESTES:\n",
    "# 2 SEMANAS PARA TREINAMENTO\n",
    "train_df = sample_df.loc[week <= 2]\n",
    "train_residual_df = norm_residual_sample_df.loc[week <= 2]\n",
    "train_clean_df = norm_clean_sample_df.loc[week <= 2]\n",
    "\n",
    "# 1 SEMANA PARA VALIDACAO\n",
    "validation_df = sample_df.loc[week == 3]\n",
    "validation_residual_df = norm_residual_sample_df.loc[week == 3]\n",
    "validation_clean_df = norm_clean_sample_df.loc[week == 3]\n",
    "\n",
    "# 1 SEMANA PARA TESTES\n",
    "test_df = sample_df.loc[week > 3]\n",
    "test_residual_df = norm_residual_sample_df.loc[week > 3]\n",
    "test_clean_df = norm_clean_sample_df.loc[week > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Spatio-temporal High Order FTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sthofts_forecast(_order, npartitions, col, train, validation):\n",
    "    \n",
    "    fuzzy_sets = KMeansPartitioner.KMeansPartitioner(data=train, npart=npartitions, batch_size=1000, init_size=npartitions*3)\n",
    "    model_sthofts = sthofts.SpatioTemporalHighOrderFTS(\"FTS\", nlags=_order, partitioner=fuzzy_sets)\n",
    "    \n",
    "    model_sthofts.fit(np.array(train.values), dump = 'time', num_batches=100)\n",
    "    forecast_sthofts = model_hofts.predict(np.array(validation.values))\n",
    "    forecast_hofts_df = pd.DataFrame(data=forecast_hofts, columns=validation.columns)\n",
    "    return forecast_hofts_df[col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    eval_order_list = np.arange(1,3)\n",
    "    partitions_list = np.arange(10,100,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = sthofts_forecast(_order, npartitions, col, train_df, validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_residual_forecast = sthofts_forecast(_order, npartitions, col, train_residual_df, validation_residual_df)\n",
    "norm_clean_forecast = sthofts_forecast(_order, npartitions, col, train_clean_df, validation_clean_df)\n",
    "\n",
    "residual_forecast = denormalize(norm_residual_forecast, residual_sample_df[col].min(), residual_sample_df[col].max())\n",
    "clean_forecast = denormalize(norm_clean_forecast, clean_sample_df[col].min(), clean_sample_df[col].max())\n",
    "\n",
    "forecast = [r + c for r, c in zip(residual_forecast,clean_forecast)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = forecast\n",
    "y_obs = validation_df['AP_1'].values\n",
    "\n",
    "#_nrmse = normalized_rmse(y_obs[(_order-1):], y_est)\n",
    "_nrmse = normalized_rmse(y_obs[_order:], y_est[:-1])\n",
    "print(\"nRMSE: \", _nrmse, \"\\n\")\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot( y_obs[_order:])\n",
    "plt.plot(y_est[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train_df.values)\n",
    "validation = np.array(validation_df.values)\n",
    "\n",
    "k = 20\n",
    "\n",
    "fuzzy_sets = KMeansPartitioner.KMeansPartitioner(data=train, npart=k, batch_size=1000, init_size=k*3)\n",
    "\n",
    "_order = 6\n",
    "\n",
    "model_hofts = sthofts.SpatioTemporalHighOrderFTS(\"FTS\", nlags=_order, partitioner=fuzzy_sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hofts.fit(train, dump = 'time', num_batches=100)\n",
    "#model_hofts.fit(train, dump = 'time', num_batches=100, distributed=True, nodes=['192.168.1.3','192.168.1.8'])\n",
    "#model_hofts.fit(train, dump = 'time', num_batches=100, distributed=True, nodes=['192.168.1.3'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_hofts = model_hofts.predict(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_frcst_df = pd.DataFrame(data=forecast_hofts, columns=df.columns)\n",
    "denormalized_frcst_df = (norm_frcst_df * (sample_df.max()-sample_df.min())) + sample_df.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_hofts_df = pd.DataFrame(data=forecast_hofts, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(forecast_hofts_df[col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs = validation_df[col].values\n",
    "y_est = forecast_hofts_df[col].values\n",
    "\n",
    "#_nrmse = normalized_rmse(y_obs[(_order-1):], y_est)\n",
    "_nrmse = normalized_rmse(y_obs[_order:], y_est[:-1])\n",
    "print(\"nRMSE: \", _nrmse, \"\\n\")\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot( y_obs[_order:])\n",
    "plt.plot(y_est[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_nrmse = normalized_rmse(y_obs[(_order - 1):], y_est)\n",
    "print(\"nRMSE: \", _nrmse, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple HOFTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyFTS.models.multivariate import common, variable, mvfts\n",
    "from pyFTS.models.seasonal import partitioner as seasonal\n",
    "from pyFTS.models.seasonal.common import DateTime\n",
    "from pyFTS.partitioners import Grid, Util as pUtil\n",
    "from pyFTS.models.multivariate import common, variable, mvfts\n",
    "from pyFTS.models import hofts\n",
    "from pyFTS.common import Transformations\n",
    "tdiff = Transformations.Differential(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hofts_forecast(_order, npartitions, col, train, validation):\n",
    "    \n",
    "    fuzzy_sets = Grid.GridPartitioner(data=train[col].values, npart=npartitions)\n",
    "    model_simple_hofts = hofts.HighOrderFTS(\"FTS\", partitioner=fuzzy_sets)\n",
    "    \n",
    "    #model_simple_hofts.append_transformation(Transformations.Differential(1))\n",
    "    model_simple_hofts.fit(train[col].values, order=_order)\n",
    "    \n",
    "    return model_simple_hofts.predict(validation[col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_order = 6\n",
    "npartitions = 70\n",
    "col = 'AP_1'\n",
    "\n",
    "\n",
    "norm_residual_forecast = hofts_forecast(_order, npartitions, col, train_residual_df, validation_residual_df)\n",
    "norm_clean_forecast = hofts_forecast(_order, npartitions, col, train_clean_df, validation_clean_df)\n",
    "\n",
    "residual_forecast = denormalize(norm_residual_forecast, residual_sample_df[col].min(), residual_sample_df[col].max())\n",
    "clean_forecast = denormalize(norm_clean_forecast, clean_sample_df[col].min(), clean_sample_df[col].max())\n",
    "\n",
    "forecast = [r + c for r, c in zip(residual_forecast,clean_forecast)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast = hofts_forecast(_order, npartitions, col, train_df, validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = forecast\n",
    "y_obs = validation_df['AP_1'].values\n",
    "\n",
    "#_nrmse = normalized_rmse(y_obs[(_order-1):], y_est)\n",
    "_nrmse = normalized_rmse(y_obs[_order:], y_est[:-1])\n",
    "print(\"nRMSE: \", _nrmse, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot( y_obs[_order:])\n",
    "plt.plot(y_est[:-1])\n",
    "\n",
    "#plt.plot( y_obs[_order-1:])\n",
    "#plt.plot(y_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistence_forecast(data, data_clean, order):\n",
    "    l = len(data)\n",
    "    d = list(data)\n",
    "    dc = list(data_clean)\n",
    "    fcst = []\n",
    "    for k in np.arange(order, l):\n",
    "        irr = d[k-1]\n",
    "        irr_clean = dc[k-1]\n",
    "        irr_clean_nxt = dc[k]\n",
    "        \n",
    "        irr_nxt = (irr/irr_clean) * irr_clean_nxt\n",
    "        fcst.append(irr_nxt)\n",
    "    return fcst\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(validation_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fc = persistence_forecast(validation_df[col], validation_clean_df[col], _order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_nrmse = normalized_rmse(y_obs[(_order):], fc)\n",
    "print(\"nRMSE: \", _nrmse, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot( y_obs[_order:])\n",
    "plt.plot(fc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date(df):\n",
    "    df_mv = df.copy()\n",
    "    df_mv['date'] = df.index\n",
    "    return df_mv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mv = add_date(train_df)\n",
    "train_residual_mv = add_date(train_residual_df)\n",
    "train_clean_mv = add_date(train_clean_df)\n",
    "\n",
    "validation_mv = add_date(validation_df)\n",
    "validation_residual_mv = add_date(validation_residual_df)\n",
    "validation_clean_mv = add_date(validation_clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyFTS.models.multivariate import common, variable, mvfts\n",
    "from pyFTS.models.seasonal import partitioner as seasonal\n",
    "from pyFTS.models.seasonal.common import DateTime\n",
    "\n",
    "#fig, axes = plt.subplots(nrows=2, ncols=1,figsize=[15,10])\n",
    "\n",
    "sp = {'seasonality': DateTime.minute_of_day}\n",
    "\n",
    "vhour = variable.Variable(\"Hour\", data_label=\"date\", partitioner=seasonal.TimeGridPartitioner, npart=24, \n",
    "                          data=train_mv, partitioner_specific=sp)\n",
    "\n",
    "vavg = variable.Variable(\"Irradiance\", data_label=col, partitioner=Grid.GridPartitioner, npart=50, \n",
    "                         data=train_mv) \n",
    "#vhour.partitioner.plot(axes[1])\n",
    "\n",
    "#plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyFTS.models.multivariate import common, variable, mvfts\n",
    "\n",
    "model1 = mvfts.MVFTS(\"\")\n",
    "\n",
    "model1.append_variable(vhour)\n",
    "\n",
    "model1.append_variable(vavg)\n",
    "\n",
    "model1.target_variable = vavg\n",
    "\n",
    "model1.fit(train_mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = model1.predict(validation_mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = forecast\n",
    "y_obs = validation_df['AP_1'].values\n",
    "\n",
    "#_nrmse = normalized_rmse(y_obs[(_order-1):], y_est)\n",
    "_nrmse = normalized_rmse(y_obs[1:], y_est[:-1])\n",
    "print(\"nRMSE: \", _nrmse, \"\\n\")\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot( y_obs[1:])\n",
    "plt.plot(y_est[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from itertools import product\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial approximation of parameters\n",
    "ps = range(0, 2)\n",
    "ds = range(0, 2)\n",
    "qs = range(0, 2)\n",
    "Ps = range(0, 2)\n",
    "Ds = range(0, 2)\n",
    "Qs = range(0, 2)\n",
    "D=1\n",
    "d=1\n",
    "parameters = product(ps, ds, qs, Ps, Ds, Qs)\n",
    "parameters_list = list(parameters)\n",
    "\n",
    "print(\"Num combinations: \", len(parameters_list))\n",
    "# Model Selection\n",
    "results = []\n",
    "best_aic = float(\"inf\")\n",
    "warnings.filterwarnings('ignore')\n",
    "for param in parameters_list:\n",
    "    print(\" Testing combination: \", param)\n",
    "    try:\n",
    "        model = SARIMAX(train_df[col].values, order=(param[0], param[1], param[2]), seasonal_order=(param[3], param[4], param[5], 96)).fit(disp=-1)\n",
    "    except ValueError:\n",
    "        print('bad parameter combination:', param)\n",
    "        continue\n",
    "    aic = model.aic\n",
    "    if aic < best_aic:\n",
    "        best_model = model\n",
    "        best_aic = aic\n",
    "        best_param = param\n",
    "    results.append([param, model.aic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SARIMAX(train_df[col].values, order=(3, 1, 1), seasonal_order=(1, 1, 1, 24)).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_mod = SARIMAX(train_df[col].values, order=(6, 1, 1), seasonal_order=(1, 1, 1, 96))\n",
    "training_res = training_mod.fit()\n",
    "\n",
    "whole_data = train_df.append(validation_df)\n",
    "test_data = validation_df\n",
    "\n",
    "mod = SARIMAX(whole_data[col].values, order=(6, 1, 1), seasonal_order=(1, 1, 1, 96))\n",
    "res = mod.filter(training_res.params)\n",
    "\n",
    "insample = res.predict()\n",
    "T = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlen = len(whole_data)\n",
    "tlen = len(test_data)\n",
    "\n",
    "forecast = insample[wlen-tlen:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = forecast\n",
    "y_obs = validation_df['AP_1'].values\n",
    "\n",
    "_nrmse = normalized_rmse(y_obs, y_est)\n",
    "#_nrmse = normalized_rmse(y_obs[1:], y_est[:-1])\n",
    "print(\"nRMSE: \", _nrmse, \"\\n\")\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot( y_obs)\n",
    "plt.plot(y_est)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
