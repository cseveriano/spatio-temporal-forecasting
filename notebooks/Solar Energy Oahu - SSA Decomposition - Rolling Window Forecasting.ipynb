{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "from pyFTS.benchmarks import Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    mindf = df.min()\n",
    "    maxdf = df.max()\n",
    "    return (df-mindf)/(maxdf-mindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(norm, _min, _max):\n",
    "    return [(n * (_max-_min)) + _min for n in norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('results/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('results/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set target and input variables \n",
    "target_station = 'DHHL_3'\n",
    "\n",
    "#All neighbor stations with residual correlation greater than .90\n",
    "neighbor_stations_90 = ['DHHL_3',  'DHHL_4','DHHL_5','DHHL_10','DHHL_11','DHHL_9','DHHL_2', 'DHHL_6','DHHL_7','DHHL_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"https://github.com/cseveriano/spatio-temporal-forecasting/raw/master/notebooks/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('df_ssa_residual', <http.client.HTTPMessage at 0x10ca1f9b0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request.urlretrieve(dataset_path+\"df_oahu.pkl\", \"df_oahu\")\n",
    "request.urlretrieve(dataset_path+\"df_ssa_clean.pkl\", \"df_ssa_clean\")\n",
    "request.urlretrieve(dataset_path+\"df_ssa_residual.pkl\", \"df_ssa_residual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"df_oahu\")\n",
    "df_ssa_clean = pd.read_pickle(\"df_ssa_clean\")\n",
    "df_ssa_residual = pd.read_pickle(\"df_ssa_residual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove columns with many corrupted or missing values\n",
    "df.drop(columns=['AP_1', 'AP_7'], inplace=True)\n",
    "df_ssa_clean.drop(columns=['AP_1', 'AP_7'], inplace=True)\n",
    "df_ssa_residual.drop(columns=['AP_1', 'AP_7'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data form the interval of interest\n",
    "interval = ((df.index >= '2010-06') & (df.index < '2010-08'))\n",
    "df = df.loc[interval]\n",
    "df_ssa_clean = df_ssa_clean.loc[interval]\n",
    "df_ssa_residual = df_ssa_residual.loc[interval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize Data\n",
    "\n",
    "# Save Min-Max for Denorm\n",
    "min_raw = df[target_station].min()\n",
    "min_clean = df_ssa_clean[target_station].min()\n",
    "min_residual = df_ssa_residual[target_station].min()\n",
    "\n",
    "max_raw = df[target_station].max()\n",
    "max_clean = df_ssa_clean[target_station].max()\n",
    "max_residual = df_ssa_residual[target_station].max()\n",
    "\n",
    "# Perform Normalization\n",
    "norm_df_ssa_clean = normalize(df_ssa_clean)\n",
    "norm_df_ssa_residual = normalize(df_ssa_residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling window Validation\n",
    "\n",
    "Training: 4 weeks\n",
    "\n",
    "Validation: 1 week\n",
    "\n",
    "Test: 1 week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRollingWindow(index):\n",
    "    pivot = index\n",
    "    train_start = pivot.strftime('%Y-%m-%d')\n",
    "    pivot = pivot + datetime.timedelta(days=27)\n",
    "    train_end = pivot.strftime('%Y-%m-%d')\n",
    "\n",
    "    pivot = pivot + datetime.timedelta(days=1)\n",
    "    validation_start = pivot.strftime('%Y-%m-%d')\n",
    "    pivot = pivot + datetime.timedelta(days=6)\n",
    "    validation_end = pivot.strftime('%Y-%m-%d')\n",
    "\n",
    "    pivot = pivot + datetime.timedelta(days=1)\n",
    "    test_start = pivot.strftime('%Y-%m-%d')\n",
    "    pivot = pivot + datetime.timedelta(days=6)\n",
    "    test_end = pivot.strftime('%Y-%m-%d')\n",
    "    \n",
    "    return train_start, train_end, validation_start, validation_end, test_start, test_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_error(cv_name, df, forecasts, order_list):\n",
    "    cv_results = pd.DataFrame(columns=['Split', 'RMSE', 'SMAPE', 'U'])\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    for i in np.arange(len(forecasts)):\n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        test = df[test_start : test_end]\n",
    "    \n",
    "        yhat = forecasts[i]\n",
    "        order = order_list[i]\n",
    "        \n",
    "        rmse = Measures.rmse(test[target_station].iloc[order:], yhat[:-1])\n",
    "        \n",
    "        smape = Measures.smape(test[target_station].iloc[order:], yhat[:-1])\n",
    "        \n",
    "        u = Measures.UStatistic(test[target_station].iloc[order:], yhat[:-1])\n",
    "       \n",
    "        res = {'Split' : index.strftime('%Y-%m-%d') ,'RMSE' : rmse, 'SMAPE' : smape, 'U' : u}\n",
    "        cv_results = cv_results.append(res, ignore_index=True)\n",
    "        cv_results.to_csv(cv_name+\".csv\")        \n",
    "\n",
    "        index = index + datetime.timedelta(days=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_ssa_series(clean, residual):\n",
    "    return [r + c for r, c in zip(residual,clean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual):\n",
    "    \n",
    "    forecasts_final = []\n",
    "    order_list = []\n",
    "    \n",
    "    for i in np.arange(len(forecasts_clean)):\n",
    "        f_clean = denormalize(forecasts_clean[i], min_clean, max_clean)\n",
    "        f_residual = denormalize(forecasts_residual[i], min_residual, max_residual)\n",
    "\n",
    "        o_clean = order_list_clean[i]\n",
    "        o_residual = order_list_residual[i]\n",
    "\n",
    "        max_order = max(o_clean, o_residual)\n",
    "\n",
    "        f_final = reconstruct_ssa_series(f_clean[max_order-o_clean:], f_residual[max_order-o_residual:])\n",
    "        \n",
    "        forecasts_final.append(f_final)\n",
    "        order_list.append(max_order)\n",
    "        \n",
    "    return forecasts_final, order_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistence_forecast(train, test, step):\n",
    "    predictions = []\n",
    "    \n",
    "    for t in np.arange(0,len(test), step):\n",
    "        yhat = [test.iloc[t]]  * step\n",
    "        predictions.extend(yhat)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_persistence(df, step):\n",
    "\n",
    "    forecasts = []\n",
    "    lags_list = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "    \n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "        yhat = persistence_forecast(train[target_station], test[target_station], step)        \n",
    "        \n",
    "        lags_list.append(1)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_persistence(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_persistence(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_persistence\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from itertools import product\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_SARIMA_models(test_name, train, validation, parameters_list, period_length):\n",
    "\n",
    "    sarima_results = pd.DataFrame(columns=['Order','RMSE'])\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "\n",
    "    for param in parameters_list:\n",
    "        arima_order = (param[0],param[1],param[2])\n",
    "        sarima_order = (param[3],param[4],param[5],period_length)\n",
    "        print('Testing SARIMA%s %s ' % (str(arima_order),str(sarima_order)))\n",
    "        try:\n",
    "            fcst = sarima_forecast(train, validation, arima_order, sarima_order)\n",
    "            rmse = Measures.rmse(validation.values, fcst)\n",
    "            \n",
    "            if rmse < best_score:\n",
    "                best_score, best_cfg = rmse, (arima_order, sarima_order)\n",
    "\n",
    "            res = {'Parameters' : str(param) ,'RMSE' : rmse}\n",
    "            print('SARIMA%s %s RMSE=%.3f' % (str(arima_order),str(sarima_order),rmse))\n",
    "            sarima_results = sarima_results.append(res, ignore_index=True)\n",
    "            sarima_results.to_csv(test_name+\".csv\")\n",
    "        except:\n",
    "            print(sys.exc_info())\n",
    "            print('Invalid model%s %s ' % (str(arima_order),str(sarima_order)))\n",
    "            continue\n",
    "    \n",
    "    print('Best SARIMA(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarima_forecast(train, test, arima_order, sarima_order):\n",
    "\n",
    "    predictions = []\n",
    "    window_size = sarima_order[3] * 5\n",
    "    step = 5\n",
    "    \n",
    "    history = list(train.iloc[-window_size:])\n",
    "\n",
    "    print(\"Fitting model at:\", datetime.datetime.now())\n",
    "    model = SARIMAX(history, order=arima_order, seasonal_order=sarima_order,enforce_invertibility=False,enforce_stationarity=False)\n",
    "    model_fit = model.fit(disp=True,enforce_invertibility=False,enforce_stationarity=False, maxiter=100)\n",
    "\n",
    "    #save the state parameter\n",
    "    est_params = model_fit.params\n",
    "    est_state = model_fit.predicted_state[:, -1]\n",
    "    est_state_cov = model_fit.predicted_state_cov[:, :, -1]\n",
    "\n",
    "    st = 0\n",
    "        \n",
    "    print(\"Forecasting at:\", datetime.datetime.now())\n",
    "    for t in np.arange(1,len(test)+1,step):\n",
    "        obs = test.iloc[st:t].values\n",
    "        history.extend(obs)\n",
    "        history = history[-window_size:]\n",
    "        \n",
    "        mod_updated = SARIMAX(history, order=arima_order, seasonal_order=sarima_order,enforce_invertibility=False,enforce_stationarity=False)\n",
    "        mod_updated.initialize_known(est_state, est_state_cov)\n",
    "        mod_frcst = mod_updated.smooth(est_params)\n",
    "        \n",
    "        yhat = mod_frcst.forecast(step)   \n",
    "        predictions.extend(yhat)\n",
    "            \n",
    "        est_params = mod_frcst.params\n",
    "        est_state = mod_frcst.predicted_state[:, -1]\n",
    "        est_state_cov = mod_frcst.predicted_state_cov[:, :, -1]\n",
    "            \n",
    "        st = t\n",
    "    print(\"Forecasting complete at:\", datetime.datetime.now())\n",
    "                \n",
    "    return predictions[:len(test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_SARIMA(df, step):\n",
    "\n",
    "    p_values = [0,1,2]\n",
    "    d_values = [0,1]\n",
    "    q_values = [0,1,2]\n",
    "    P_values = [0,1]\n",
    "    D_Values = [0,1]\n",
    "    Q_Values = [0,1]\n",
    "\n",
    "    parameters = product(p_values, d_values, q_values, P_values, D_Values, Q_Values)\n",
    "    parameters_list = list(parameters)\n",
    "    period_length = 61 #de 5:00 as 20:00\n",
    "\n",
    "    \n",
    "    forecasts = []\n",
    "    lags_list = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "    \n",
    "        # Perform grid search\n",
    "        #(arima_params, sarima_params) = evaluate_SARIMA_models(\"nested_test_sarima_oahu\", train[target_station], validation[target_station], parameters_list, period_length)\n",
    "        arima_params = (2, 1, 2)\n",
    "        sarima_params = (1, 1, 1, 61)\n",
    "        \n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "        yhat = sarima_forecast(train[target_station], test[target_station], arima_params, sarima_params)        \n",
    "        \n",
    "        lags_list.append(1)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_SARIMA(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_SARIMA(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_sarima\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Autoregressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR, DynamicVAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_VAR_models(test_name, train, validation,target, maxlags_list):\n",
    "    var_results = pd.DataFrame(columns=['Order','RMSE'])\n",
    "    best_score, best_cfg, best_model = float(\"inf\"), None, None\n",
    "    \n",
    "    for lgs in maxlags_list:\n",
    "        model = VAR(train)\n",
    "        results = model.fit(maxlags=lgs, ic='aic')\n",
    "        \n",
    "        order = results.k_ar\n",
    "        forecast = []\n",
    "\n",
    "        for i in range(len(validation)-order) :\n",
    "            forecast.extend(results.forecast(validation.values[i:i+order],1))\n",
    "\n",
    "        forecast_df = pd.DataFrame(columns=validation.columns, data=forecast)\n",
    "        rmse = Measures.rmse(validation[target].iloc[order:], forecast_df[target].values)\n",
    "\n",
    "        if rmse < best_score:\n",
    "            best_score, best_cfg, best_model = rmse, order, results\n",
    "\n",
    "        res = {'Order' : str(order) ,'RMSE' : rmse}\n",
    "        print('VAR (%s)  RMSE=%.3f' % (str(order),rmse))\n",
    "        var_results = var_results.append(res, ignore_index=True)\n",
    "        var_results.to_csv(test_name+\".csv\")\n",
    "        \n",
    "    print('Best VAR(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_forecast(train, test, target, order, step):\n",
    "    model = VAR(train.values)\n",
    "    results = model.fit(maxlags=order)\n",
    "    lag_order = results.k_ar\n",
    "    print(\"Lag order:\" + str(lag_order))\n",
    "    forecast = []\n",
    "\n",
    "    for i in np.arange(0,len(test)-lag_order+1,step) :\n",
    "        forecast.extend(results.forecast(test.values[i:i+lag_order],step))\n",
    "\n",
    "    forecast_df = pd.DataFrame(columns=test.columns, data=forecast)\n",
    "    return forecast_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv_var(df, step):\n",
    "    maxlags_list = [1,2,4,6,8,10,20,40]\n",
    "    forecasts = []\n",
    "    order_list = []\n",
    "    \n",
    "    for i in np.arange(n_folds):\n",
    "        train = getNestedData(df, train_inds[i])\n",
    "        validation = getNestedData(df, val_inds[i])\n",
    "        test = getNestedData(df, test_inds[i])\n",
    "    \n",
    "        # Perform grid search\n",
    "        best_model = evaluate_VAR_models(\"nested_test_var_oahu\", train[neighbor_stations_90], validation[neighbor_stations_90],target_station, maxlags_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "        order = best_model.k_ar\n",
    "        yhat = var_forecast(train[neighbor_stations_90], test[neighbor_stations_90], target_station, order, step)\n",
    "        \n",
    "        order_list.append(order)\n",
    "        forecasts.append(yhat)\n",
    "    \n",
    "    return forecasts, order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_var(df, step):\n",
    "    maxlags_list = [1,2,4,6,8,10,20,40]\n",
    "    forecasts = []\n",
    "    order_list = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "    \n",
    "        # Perform grid search\n",
    "        best_model = evaluate_VAR_models(\"nested_test_var_oahu\", train[neighbor_stations_90], validation[neighbor_stations_90],target_station, maxlags_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "        order = best_model.k_ar\n",
    "        yhat = var_forecast(train[neighbor_stations_90], test[neighbor_stations_90], target_station, order, step)\n",
    "        \n",
    "        order_list.append(order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nested_error(cv_name, test_df, forecasts, order_list):\n",
    "    cv_results = pd.DataFrame(columns=['Split', 'RMSE', 'MAPE', 'SMAPE', 'U'])\n",
    "\n",
    "    for i in np.arange(n_folds):\n",
    "        test = getNestedData(test_df, test_inds[i])\n",
    "        yhat = forecasts[i]\n",
    "        order = order_list[i]\n",
    "        \n",
    "        rmse = Measures.rmse(test[target_station].iloc[order:], yhat[:-1])\n",
    "        \n",
    "        mape = Measures.mape(test[target_station].iloc[order:], yhat[:-1])\n",
    "\n",
    "        smape = Measures.smape(test[target_station].iloc[order:], yhat[:-1])\n",
    "        \n",
    "        u = Measures.UStatistic(test[target_station].iloc[order:], yhat[:-1])\n",
    "       \n",
    "        res = {'Split' : i ,'RMSE' : rmse, 'MAPE' : mape, 'SMAPE' : smape, 'U' : u}\n",
    "        cv_results = cv_results.append(res, ignore_index=True)\n",
    "        cv_results.to_csv(cv_name+\".csv\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_var(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_var(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_var\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Order FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyFTS.partitioners import Grid, Entropy, Util as pUtil\n",
    "from pyFTS.models import hofts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hofts_models(test_name, train, validation, partitioners_list, order_list, partitions_list):\n",
    "    \n",
    "    hofts_results = pd.DataFrame(columns=['Partitioner','Partitions','Order','RMSE'])\n",
    "    best_score, best_cfg, best_model = float(\"inf\"), None, None\n",
    "\n",
    "    for _partitioner in partitioners_list:\n",
    "        for _order in order_list:\n",
    "            for npartitions in partitions_list:\n",
    "                fuzzy_sets = _partitioner(data=train.values, npart=npartitions)\n",
    "                model_simple_hofts = hofts.HighOrderFTS(order=_order)\n",
    "\n",
    "                model_simple_hofts.fit(train.values, order=_order, partitioner=fuzzy_sets)\n",
    "                \n",
    "                forecast = model_simple_hofts.predict(validation.values)\n",
    "                rmse = Measures.rmse(validation.iloc[_order:], forecast[:-1])\n",
    "\n",
    "                if rmse < best_score:\n",
    "                    best_score, best_cfg = rmse, (_order,npartitions,_partitioner)\n",
    "                    best_model = model_simple_hofts\n",
    "\n",
    "                res = {'Partitioner':str(_partitioner), 'Partitions':npartitions, 'Order' : str(_order) ,'RMSE' : rmse}\n",
    "                print('HOFTS %s - %s - %s  RMSE=%.3f' % (str(_partitioner), npartitions, str(_order),rmse))\n",
    "                hofts_results = hofts_results.append(res, ignore_index=True)\n",
    "                hofts_results.to_csv(test_name+\".csv\")\n",
    "\n",
    "    print('Best HOFTS(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    \n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hofts_forecast(train_df, test_df, _order, _partitioner, _npartitions):\n",
    "    \n",
    "    fuzzy_sets = _partitioner(data=train_df.values, npart=_npartitions)\n",
    "    model_simple_hofts = hofts.HighOrderFTS()\n",
    "    \n",
    "\n",
    "    model_simple_hofts.fit(train_df.values, order=_order, partitioner=fuzzy_sets)\n",
    "\n",
    "    \n",
    "    forecast = model_simple_hofts.predict(test_df.values)\n",
    "\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_hofts(df, step):\n",
    "    \n",
    "    partitioners_list = [Grid.GridPartitioner, Entropy.EntropyPartitioner]\n",
    "    eval_order_list = np.arange(1,3)\n",
    "    partitions_list = np.arange(10,100,10)\n",
    "    \n",
    "    forecasts = []\n",
    "    order_list = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=1)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (order,nparts,partitioner) = evaluate_hofts_models(\"nested_eval_hofts_oahu\", train[target_station], validation[target_station], partitioners_list, eval_order_list, partitions_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = hofts_forecast(train[target_station], test[target_station], order, partitioner, nparts)\n",
    "        \n",
    "        order_list.append(order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_hofts(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_hofts(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_hofts\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonstationary FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyFTS.models.nonstationary import cvfts\n",
    "from pyFTS.models.nonstationary import partitioners as nspartitioners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cvfts_models(test_name, train, validation, partitions_list):\n",
    "    \n",
    "    cvfts_results = pd.DataFrame(columns=['Partitions','RMSE'])\n",
    "    best_score, best_cfg, best_model = float(\"inf\"), None, None\n",
    "\n",
    "    for npartitions in partitions_list:\n",
    "                \n",
    "        fuzzy_sets =  nspartitioners.PolynomialNonStationaryPartitioner(data=train.values, part=Grid.GridPartitioner(data=train.values, npart=npartitions), degree=2)\n",
    "                \n",
    "        model_cvfts = cvfts.ConditionalVarianceFTS()\n",
    "        model_cvfts.fit(train.values, parameters=1, partitioner=fuzzy_sets, num_batches=1000)\n",
    "                                \n",
    "        forecast = model_cvfts.predict(validation.values)\n",
    "        rmse = Measures.rmse(validation.iloc[1:], forecast[:-1])\n",
    "\n",
    "        if rmse < best_score:\n",
    "            best_score, best_cfg = rmse, npartitions\n",
    "            best_model = model_cvfts\n",
    "\n",
    "        res = {'Partitions':npartitions, 'RMSE' : rmse}\n",
    "        print('CVFTS %s -  RMSE=%.3f' % (npartitions, rmse))\n",
    "        cvfts_results = cvfts_results.append(res, ignore_index=True)\n",
    "        cvfts_results.to_csv(test_name+\".csv\")\n",
    "\n",
    "    print('Best CVFTS(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    \n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvfts_forecast(train, test, _partitions):\n",
    "    \n",
    "    fuzzy_sets =  nspartitioners.PolynomialNonStationaryPartitioner(data=train.values, part=Grid.GridPartitioner(data=train.values, npart=_partitions), degree=2)\n",
    "                    \n",
    "    model_cvfts = cvfts.ConditionalVarianceFTS()\n",
    "    model_cvfts.fit(train.values, parameters=1, partitioner=fuzzy_sets, num_batches=1000)\n",
    "\n",
    "    forecast = model_cvfts.predict(test.values)\n",
    "\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_cvfts(df, step):\n",
    "    \n",
    "    partitions_list = np.arange(50,120,10)\n",
    "    \n",
    "    forecasts = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        nparts = evaluate_cvfts_models(\"nested_eval_cvfts_oahu\", train[target_station], validation[target_station], partitions_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = cvfts_forecast(train[target_station], test[target_station],nparts)\n",
    "        \n",
    "        order_list.append(1)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_cvfts(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_cvfts(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_cvfts\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered Multivariate FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U git+https://github.com/cseveriano/spatio-temporal-forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import KMeansPartitioner\n",
    "from models import sthofts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmvfts_forecast(train_df, test_df, target, _order, npartitions):\n",
    "    \n",
    "    print(\"KMean Partition at:\", datetime.datetime.now())\n",
    "\n",
    "    _partitioner = KMeansPartitioner.KMeansPartitioner(data=train_df.values, npart=npartitions, batch_size=1000, init_size=npartitions*3)\n",
    "\n",
    "    model_sthofts = sthofts.SpatioTemporalHighOrderFTS()\n",
    "    \n",
    "    print(\"CMVFTS fit at:\", datetime.datetime.now())\n",
    "    model_sthofts.fit(train_df.values, order=_order, partitioner=_partitioner)\n",
    "    \n",
    "    print(\"CMVFTS prediction at:\", datetime.datetime.now())\n",
    "    forecast = model_sthofts.predict(test_df.values)\n",
    "    forecast_df = pd.DataFrame(data=forecast, columns=test_df.columns)\n",
    "    return forecast_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cmvfts_models(test_name, train, validation, order_list, partitions_list):\n",
    "    \n",
    "    cmvfts_results = pd.DataFrame(columns=['Partitions','Order','RMSE'])\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "\n",
    "    for _order in order_list:\n",
    "        for npartitions in partitions_list:\n",
    "            \n",
    "            forecast = cmvfts_forecast(train, validation, target_station, _order, npartitions)\n",
    "            rmse = Measures.rmse(validation[target_station].iloc[_order:], forecast[:-1])\n",
    "\n",
    "            if rmse < best_score:\n",
    "                best_score, best_cfg = rmse, (_order,npartitions)\n",
    "\n",
    "            res = {'Partitions':npartitions, 'Order' : str(_order) ,'RMSE' : rmse}\n",
    "            print('CMVFTS %s - %s  RMSE=%.3f' % (npartitions, str(_order),rmse))\n",
    "            cmvfts_results = cmvfts_results.append(res, ignore_index=True)\n",
    "            cmvfts_results.to_csv(test_name+\".csv\")\n",
    "\n",
    "    print('Best CMVFTS(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    \n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_cmvfts(df, step):\n",
    "    \n",
    "    eval_order_list = np.arange(1,3)\n",
    "    partitions_list = np.arange(80,150,10)\n",
    "    \n",
    "    forecasts = []\n",
    "    order_list = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (order,nparts) = evaluate_cmvfts_models(\"nested_eval_cmvfts_oahu\", train[neighbor_stations_90], validation[neighbor_stations_90], eval_order_list, partitions_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = cmvfts_forecast(train[neighbor_stations_90], test[neighbor_stations_90],target_station, order, nparts)\n",
    "        \n",
    "        order_list.append(order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  2010-06-01\n",
      "KMean Partition at: 2018-08-17 10:27:38.086219\n",
      "CMVFTS fit at: 2018-08-17 10:27:38.335553\n",
      "CMVFTS prediction at: 2018-08-17 10:34:38.660364\n",
      "New forecast with membership\n",
      "CMVFTS 80 - 2  RMSE=0.018\n",
      "Best CMVFTS((2, 80)) RMSE=0.018\n",
      "KMean Partition at: 2018-08-17 10:36:22.035763\n",
      "CMVFTS fit at: 2018-08-17 10:36:22.158439\n",
      "CMVFTS prediction at: 2018-08-17 10:46:23.265118\n",
      "New forecast with membership\n",
      "Index:  2010-06-08\n",
      "KMean Partition at: 2018-08-17 10:47:31.254263\n",
      "CMVFTS fit at: 2018-08-17 10:47:31.361915\n",
      "CMVFTS prediction at: 2018-08-17 10:54:26.039490\n",
      "New forecast with membership\n",
      "CMVFTS 80 - 2  RMSE=0.017\n",
      "Best CMVFTS((2, 80)) RMSE=0.017\n",
      "KMean Partition at: 2018-08-17 10:55:34.475596\n",
      "CMVFTS fit at: 2018-08-17 10:55:34.626190\n",
      "CMVFTS prediction at: 2018-08-17 11:03:54.567306\n",
      "New forecast with membership\n",
      "Index:  2010-06-15\n",
      "KMean Partition at: 2018-08-17 11:05:01.862509\n",
      "CMVFTS fit at: 2018-08-17 11:05:02.001321\n"
     ]
    }
   ],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_cmvfts(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_cmvfts(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)\n",
    "calculate_rolling_error(\"rolling_cv_oahu_ssa_cmvfts\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_multi_forecast(train_df, test_df, _order, _steps, _neurons, _epochs):\n",
    "\n",
    "    \n",
    "    nfeat = len(train_df.columns)\n",
    "    nlags = _order\n",
    "    nsteps = _steps\n",
    "    nobs = nlags * nfeat\n",
    "    \n",
    "    train_reshaped_df = series_to_supervised(train_df, n_in=nlags, n_out=nsteps)\n",
    "    train_X, train_Y = train_reshaped_df.iloc[:,:nobs].values, train_reshaped_df.iloc[:,-nfeat].values\n",
    "    train_X = train_X.reshape((train_X.shape[0], nlags, nfeat))\n",
    "\n",
    "    test_reshaped_df = series_to_supervised(test_df, n_in=nlags, n_out=nsteps)\n",
    "    test_X, test_Y = test_reshaped_df.iloc[:,:nobs].values, test_reshaped_df.iloc[:,-nfeat].values\n",
    "    test_X = test_X.reshape((test_X.shape[0], nlags, nfeat))\n",
    "    \n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(_neurons, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "    # design network\n",
    "#    model = Sequential()\n",
    "#    model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "#    model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "#    model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "#    model.add(LSTM(_neurons, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "#    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "#    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "#    model = Sequential()\n",
    "#    _dropout = 0.1\n",
    "#    model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', dropout=_dropout, kernel_constraint=maxnorm(3)))\n",
    "#    model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', dropout=_dropout, kernel_constraint=maxnorm(3)))\n",
    "#    model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', dropout=_dropout, kernel_constraint=maxnorm(3)))\n",
    "#    model.add(LSTM(_neurons, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', dropout=_dropout, kernel_constraint=maxnorm(3)))\n",
    "#    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "#    model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "    # fit network\n",
    "    model.fit(train_X, train_Y, epochs=_epochs, batch_size=1000, verbose=False, shuffle=False)\n",
    "    \n",
    "    forecast = model.predict(test_X)\n",
    "    \n",
    "    fcst = [f[0] for f in forecast]\n",
    "\n",
    "    return fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLD_evaluate_lstm_models(test_name, train, validation, order_list, neurons_list, epochs_list):\n",
    "    \n",
    "    lstm_results = pd.DataFrame(columns=['Order','Neurons','Epochs','RMSE'])\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "\n",
    "    for _order in order_list:\n",
    "        for _neurons in neurons_list:\n",
    "            for _epochs in epochs_list:\n",
    "                forecast = lstm_multi_forecast(train, validation, _order, 1, _neurons, _epochs)\n",
    "\n",
    "                obs = validation[target_station].values\n",
    "                rmse = Measures.rmse(obs[_order:], forecast)\n",
    "\n",
    "                if rmse < best_score:\n",
    "                    best_score, best_cfg = rmse, (_order,_neurons,_epochs)\n",
    "\n",
    "                res = {'Order' : str(_order) ,'Neurons' : str(_neurons) ,'Epochs' : str(_epochs) ,'RMSE' : rmse}\n",
    "                print('LSTM %s - %s - %s  RMSE=%.3f' % (str(_order), _neurons, str(_epochs),rmse))\n",
    "                lstm_results = lstm_results.append(res, ignore_index=True)\n",
    "                lstm_results.to_csv(test_name+\".csv\")\n",
    "\n",
    "    print('Best LSTM(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    \n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multivariate_lstm_models(test_name, train_df, validation_df, neurons_list, order_list, epochs_list):\n",
    "    \n",
    "    lstm_results = pd.DataFrame(columns=['Neurons','Order','Epochs','RMSE'])\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    \n",
    "    nfeat = len(train_df.columns)\n",
    "    nsteps = 1\n",
    "    \n",
    "    for _neurons in neurons_list:\n",
    "        for _order in order_list:\n",
    "            for epochs in epochs_list:\n",
    "                    \n",
    "                    nobs = nfeat * _order\n",
    "                    \n",
    "                    train_reshaped_df = series_to_supervised(train_df, n_in=_order, n_out=nsteps)\n",
    "                    train_X, train_Y = train_reshaped_df.iloc[:,:nobs].values, train_reshaped_df.iloc[:,-nfeat].values\n",
    "                    train_X = train_X.reshape((train_X.shape[0], _order, nfeat))                    \n",
    "                    \n",
    "                    val_reshaped_df = series_to_supervised(validation_df, n_in=_order, n_out=nsteps)\n",
    "                    validation_X, validation_Y = val_reshaped_df.iloc[:,:nobs].values, val_reshaped_df.iloc[:,-nfeat].values\n",
    "                    validation_X = validation_X.reshape((validation_X.shape[0], _order, nfeat))\n",
    "                    \n",
    "                    # design network\n",
    "                    model = Sequential()\n",
    "                    model.add(LSTM(_neurons, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "                    model.add(Dense(1))\n",
    "                    model.compile(loss='mae', optimizer='adam')\n",
    " \n",
    "                    # design network\n",
    "                    #model = Sequential()\n",
    "                    #model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "                    #model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "                    #model.add(LSTM(_neurons, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "                    #model.add(LSTM(_neurons, input_shape=(train_X.shape[1], train_X.shape[2]), kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "                    #model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "                    #model.compile(loss='mae', optimizer='adam')\n",
    "                    \n",
    "\n",
    "                    # fit network\n",
    "                    history = model.fit(train_X, train_Y, epochs=epochs, batch_size=1000, verbose=False, shuffle=False)\n",
    "                    forecast = model.predict(validation_X)\n",
    "                    fcst = [f[0] for f in forecast]\n",
    "                    \n",
    "                    \n",
    "                    rmse = Measures.rmse(validation_Y, fcst)\n",
    "                    #rmse = math.sqrt(mean_squared_error(validation_Y, forecast))\n",
    "                    \n",
    "                    params = (_neurons, _order,epochs)\n",
    "                    if rmse < best_score:\n",
    "                        best_score, best_cfg = rmse, params\n",
    "\n",
    "                    res = {'Neurons':_neurons, 'Order':_order, 'Epochs' : epochs ,'RMSE' : rmse}\n",
    "                    print('LSTM %s  RMSE=%.3f' % (params,rmse))\n",
    "                    lstm_results = lstm_results.append(res, ignore_index=True)\n",
    "                    lstm_results.to_csv(test_name+\".csv\")\n",
    "\n",
    "    print('Best LSTM(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_lstm_multi(df, step):\n",
    "    \n",
    "    neurons_list = np.arange(50,110,10)\n",
    "    order_list = np.arange(2,4)\n",
    "    epochs_list = [50, 100, 150]\n",
    "\n",
    "    lags_list = []\n",
    "    forecasts = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (_neurons, _order,epochs) = evaluate_multivariate_lstm_models(\"nested_eval_lstm_multi_oahu\", train[neighbor_stations_90], validation[neighbor_stations_90], neurons_list, order_list, epochs_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = lstm_multi_forecast(train[neighbor_stations_90], test[neighbor_stations_90], _order, 1, _neurons,epochs)\n",
    "        \n",
    "        yhat.append(0) #para manter o formato do vetor de metricas\n",
    "        \n",
    "        lags_list.append(_order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_lstm_multi(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_lstm_multi(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_lstm_multi\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_lstm_uni(df, step):\n",
    "    \n",
    "    neurons_list = np.arange(50,110,10)\n",
    "    order_list = np.arange(2,4)\n",
    "    epochs_list = [50, 100, 150]\n",
    "\n",
    "    lags_list = []\n",
    "    forecasts = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (_neurons, _order,epochs) = evaluate_multivariate_lstm_models(\"nested_eval_lstm_multi_oahu\", train[[target_station]], validation[[target_station]], neurons_list, order_list, epochs_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = lstm_multi_forecast(train[[target_station]], test[[target_station]], _order, 1, _neurons,epochs)\n",
    "        \n",
    "        yhat.append(0) #para manter o formato do vetor de metricas\n",
    "        \n",
    "        lags_list.append(_order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_lstm_uni(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_lstm_uni(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_rolling_error(\"rolling_cv_oahu_lstm_uni\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_multi_forecast(train_df, test_df, _order, _steps, _neurons, _epochs):\n",
    "\n",
    "    \n",
    "    nfeat = len(train_df.columns)\n",
    "    nlags = _order\n",
    "    nsteps = _steps\n",
    "    nobs = nlags * nfeat\n",
    "    \n",
    "    train_reshaped_df = series_to_supervised(train_df, n_in=nlags, n_out=nsteps)\n",
    "    train_X, train_Y = train_reshaped_df.iloc[:,:nobs].values, train_reshaped_df.iloc[:,-nfeat].values\n",
    "    \n",
    "    test_reshaped_df = series_to_supervised(test_df, n_in=nlags, n_out=nsteps)\n",
    "    test_X, test_Y = test_reshaped_df.iloc[:,:nobs].values, test_reshaped_df.iloc[:,-nfeat].values\n",
    "    \n",
    "    # design network\n",
    "    model = designMLPNetwork(_neurons,train_X.shape[1])\n",
    "    \n",
    "    # fit network\n",
    "    model.fit(train_X, train_Y, epochs=_epochs, batch_size=1000, verbose=False, shuffle=False)\n",
    "    \n",
    "    forecast = model.predict(test_X)\n",
    "    \n",
    "    fcst = [f[0] for f in forecast]\n",
    "\n",
    "    return fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multivariate_mlp_models(test_name, train_df, validation_df, neurons_list, order_list, epochs_list):\n",
    "    \n",
    "    lstm_results = pd.DataFrame(columns=['Neurons','Order','Epochs','RMSE'])\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    \n",
    "    nfeat = len(train_df.columns)\n",
    "    nsteps = 1\n",
    "    \n",
    "    for _neurons in neurons_list:\n",
    "        for _order in order_list:\n",
    "            for epochs in epochs_list:\n",
    "                    \n",
    "                    nobs = nfeat * _order\n",
    "                    \n",
    "                    train_reshaped_df = series_to_supervised(train_df, n_in=_order, n_out=nsteps)\n",
    "                    train_X, train_Y = train_reshaped_df.iloc[:,:nobs].values, train_reshaped_df.iloc[:,-nfeat].values\n",
    "                    \n",
    "                    val_reshaped_df = series_to_supervised(validation_df, n_in=_order, n_out=nsteps)\n",
    "                    validation_X, validation_Y = val_reshaped_df.iloc[:,:nobs].values, val_reshaped_df.iloc[:,-nfeat].values\n",
    "                   \n",
    "                    model = designMLPNetwork(_neurons,train_X.shape[1])\n",
    "                                        \n",
    "                    # fit network\n",
    "                    history = model.fit(train_X, train_Y, epochs=epochs, batch_size=1000, verbose=False, shuffle=False)\n",
    "                    forecast = model.predict(validation_X)\n",
    "                    fcst = [f[0] for f in forecast]\n",
    "                    \n",
    "                    \n",
    "                    rmse = Measures.rmse(validation_Y, fcst)\n",
    "                    #rmse = math.sqrt(mean_squared_error(validation_Y, forecast))\n",
    "                    \n",
    "                    params = (_neurons, _order,epochs)\n",
    "                    if rmse < best_score:\n",
    "                        best_score, best_cfg = rmse, params\n",
    "\n",
    "                    res = {'Neurons':_neurons, 'Order':_order, 'Epochs' : epochs ,'RMSE' : rmse}\n",
    "                    print('LSTM %s  RMSE=%.3f' % (params,rmse))\n",
    "                    lstm_results = lstm_results.append(res, ignore_index=True)\n",
    "                    lstm_results.to_csv(test_name+\".csv\")\n",
    "\n",
    "    print('Best MLP(%s) RMSE=%.3f' % (best_cfg, best_score))\n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def designMLPNetwork(neurons, shape):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_dim=shape))\n",
    "    model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dense(neurons, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_mlp_multi(df, step):\n",
    "    \n",
    "    neurons_list = np.arange(50,110,10)\n",
    "    order_list = np.arange(2,4)\n",
    "    epochs_list = [50, 100, 150]\n",
    "\n",
    "    lags_list = []\n",
    "    forecasts = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (_neurons, _order,epochs) = evaluate_multivariate_mlp_models(\"nested_eval_mlp_multi_oahu\", train[neighbor_stations_90], validation[neighbor_stations_90], neurons_list, order_list, epochs_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = mlp_multi_forecast(train[neighbor_stations_90], test[neighbor_stations_90], _order, 1, _neurons,epochs)\n",
    "        \n",
    "        yhat.append(0) #para manter o formato do vetor de metricas\n",
    "        \n",
    "        lags_list.append(_order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_mlp_multi(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_mlp_multi(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)\n",
    "calculate_rolling_error(\"rolling_cv_oahu_mlp_multi\", df, forecasts_final, order_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_cv_mlp_uni(df, step):\n",
    "    \n",
    "    neurons_list = np.arange(50,110,10)\n",
    "    order_list = np.arange(2,4)\n",
    "    epochs_list = [50, 100, 150]\n",
    "\n",
    "\n",
    "    lags_list = []\n",
    "    forecasts = []\n",
    "\n",
    "    limit = df.index[-1].strftime('%Y-%m-%d')\n",
    "\n",
    "    test_end = \"\"\n",
    "    index = df.index[0]\n",
    "\n",
    "    while test_end < limit :\n",
    "        print(\"Index: \", index.strftime('%Y-%m-%d'))  \n",
    "\n",
    "        train_start, train_end, validation_start, validation_end, test_start, test_end = getRollingWindow(index)\n",
    "        index = index + datetime.timedelta(days=7)\n",
    "        \n",
    "        train = df[train_start : train_end]\n",
    "        validation = df[validation_start : validation_end]\n",
    "        test = df[test_start : test_end]\n",
    "\n",
    "        # Perform grid search\n",
    "        (_neurons, _order,epochs) = evaluate_multivariate_mlp_models(\"nested_eval_mlp_uni_oahu\", train[[target_station]], validation[[target_station]], neurons_list, order_list, epochs_list)\n",
    "\n",
    "        # Concat train & validation for test\n",
    "        train = train.append(validation)\n",
    "\n",
    "        # Perform forecast\n",
    "        yhat = mlp_multi_forecast(train[[target_station]], test[[target_station]], _order, 1, _neurons,epochs)\n",
    "        \n",
    "        yhat.append(0) #para manter o formato do vetor de metricas\n",
    "        \n",
    "        lags_list.append(_order)\n",
    "        forecasts.append(yhat)\n",
    "\n",
    "    return forecasts, lags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_clean, order_list_clean = rolling_cv_mlp_uni(norm_df_ssa_clean, 1)\n",
    "forecasts_residual, order_list_residual = rolling_cv_mlp_uni(norm_df_ssa_residual, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_final, order_list = get_final_forecast(forecasts_clean, forecasts_residual, order_list_clean, order_list_residual)\n",
    "calculate_rolling_error(\"rolling_cv_oahu_mlp_uni\", df, forecasts_final, order_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
